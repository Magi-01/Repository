id,title,abstract,year,concepts,abstract_text
https://openalex.org/W2122410182,Artificial intelligence: a modern approach,"{'The': [0], 'long-anticipated': [1], 'revision': [2], 'of': [3, 13, 22], 'this': [4], '#1': [5], 'selling': [6], 'book': [7], 'offers': [8], 'the': [9, 14, 18], 'most': [10], 'comprehensive,': [11], 'state': [12], 'art': [15], 'introduction': [16], 'to': [17], 'theory': [19], 'and': [20, 59, 96], 'practice': [21], 'artificial': [23, 101], 'intelligence': [24], 'for': [25], 'modern': [26], 'applications.': [27], 'Intelligent': [28], 'Agents.': [29], 'Solving': [30], 'Problems': [31], 'by': [32], 'Searching.': [33], 'Informed': [34], 'Search': [35], 'Methods.': [36], 'Game': [37], 'Playing.': [38], 'Agents': [39, 83], 'that': [40, 84], 'Reason': [41], 'Logically.': [42], 'First-order': [43], 'Logic.': [44, 52], 'Building': [45], 'a': [46], 'Knowledge': [47, 80], 'Base.': [48], 'Inference': [49], 'in': [50, 81, 88, 100], 'First-Order': [51], 'Logical': [53], 'Reasoning': [54, 63], 'Systems.': [55, 64], 'Practical': [56, 86], 'Planning.': [57], 'Planning': [58], 'Acting.': [60], 'Uncertainty.': [61], 'Probabilistic': [62], 'Making': [65, 68], 'Simple': [66], 'Decisions.': [67, 70], 'Complex': [69], 'Learning': [71, 74], 'from': [72], 'Observations.': [73], 'with': [75], 'Neural': [76], 'Networks.': [77], 'Reinforcement': [78], 'Learning.': [79, 82], 'Communicate.': [85], 'Communication': [87], 'English.': [89], 'Perception.': [90], 'Robotics.': [91], 'For': [92], 'computer': [93], 'professionals,': [94], 'linguists,': [95], 'cognitive': [97], 'scientists': [98], 'interested': [99], 'intelligence.': [102]}",1995,"['Artificial intelligence', 'Computer science', 'Inference', 'Artificial intelligence, situated approach', 'Probabilistic logic', 'Cognitive robotics', 'Symbolic artificial intelligence', 'Reinforcement learning', 'Cognitive science', 'Robot', 'Psychology']","The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence."
https://openalex.org/W2623293810,Artificial intelligence: A modern approach,,1996,"['Computer science', 'Artificial intelligence', 'Cognitive science', 'Psychology']",
https://openalex.org/W26772505,Lecture Notes in Artificial Intelligence,,1999,"['Artificial intelligence', 'Computer science']",
https://openalex.org/W2152475379,Principles of Artificial Intelligence,,1982,"['Computer science', 'Artificial intelligence']",
https://openalex.org/W2531563875,Proceedings of the 19th International Joint Conference on Artificial Intelligence,,2005,"['Joint (building)', 'Artificial intelligence', 'Computer science', 'Engineering', 'Architectural engineering']",
https://openalex.org/W2908201961,High-performance medicine: the convergence of human and artificial intelligence,,2018,"['Workflow', 'Cloud computing', 'Transparency (behavior)', 'Computer science', 'Big data', 'Productivity', 'Data science', 'Deep learning', 'Artificial intelligence', 'Process (computing)', 'Precision medicine', 'Risk analysis (engineering)', 'Medicine', 'Computer security', 'Data mining', 'Pathology', 'Macroeconomics', 'Database', 'Economics', 'Operating system']",
https://openalex.org/W2084792706,Future paths for integer programming and links to artificial intelligence,,1986,"['Integer programming', 'Tabu search', 'Computer science', 'Key (lock)', 'Decomposition', 'Integer (computer science)', 'Artificial intelligence', 'Heuristic', 'Mathematical optimization', 'Operations research', 'Theoretical computer science', 'Machine learning', 'Mathematics', 'Algorithm', 'Programming language', 'Ecology', 'Computer security', 'Biology']",
https://openalex.org/W2803760365,Artificial intelligence in radiology,,2018,"['Pace', 'Computer science', 'Convolutional neural network', 'Deep learning', 'Artificial intelligence', 'Field (mathematics)', 'Medical imaging', 'Focus (optics)', 'Perspective (graphical)', 'Domain (mathematical analysis)', 'Data science', 'Clinical Practice', 'Medical physics', 'Radiology', 'Medicine', 'Mathematical analysis', 'Physics', 'Mathematics', 'Geodesy', 'Family medicine', 'Pure mathematics', 'Optics', 'Geography']",
https://openalex.org/W1564534945,Multiagent Systems : A Modern Approach to Distributed Artificial Intelligence,"{'From': [0], 'the': [1, 4, 54, 64, 68], 'Publisher:\r\nThis': [2], 'is': [3, 32, 50], 'first': [5], 'comprehensive': [6], 'introduction': [7], 'to': [8, 52], 'multiagent': [9], 'systems': [10], 'and': [11, 31, 48, 59, 81, 87, 89], 'contemporary': [12], 'distributed': [13], 'artificial': [14], 'intelligence.': [15], 'The': [16, 37], 'book': [17, 38], 'provides': [18], 'detailed': [19], 'coverage': [20], 'of': [21, 56, 63, 67], 'basic': [22], 'topics': [23], 'as': [24, 26, 34, 44, 46, 92], 'well': [25, 45], 'several': [27], 'closely': [28], 'related': [29], 'ones': [30], 'suitable': [33], 'a': [35, 73], 'textbook.': [36], 'can': [39], 'be': [40, 72], 'used': [41], 'for': [42, 78, 84], 'teaching': [43], 'self-study,': [47], 'it': [49, 70], 'designed': [51], 'meet': [53], 'needs': [55], 'both': [57], 'researchers': [58], 'practitioners.': [60], 'In': [61], 'view': [62], 'interdisciplinary': [65], 'nature': [66], 'field,': [69], 'will': [71], 'useful': [74], 'reference': [75], 'not': [76], 'only': [77], 'computer': [79], 'scientists': [80, 86, 91], 'engineers,': [82], 'but': [83], 'social': [85], 'management': [88], 'organization': [90], 'well.': [93]}",2000,"['Field (mathematics)', 'Computer science', 'Artificial intelligence', 'Marketing and artificial intelligence', 'Management science', 'Data science', 'Engineering ethics', 'Engineering', 'Intelligent decision support system', 'Mathematics', 'Pure mathematics']","From the Publisher:
This is the first comprehensive introduction to multiagent systems and contemporary distributed artificial intelligence. The book provides detailed coverage of basic topics as well as several closely related ones and is suitable as a textbook. The book can be used for teaching as well as self-study, and it is designed to meet the needs of both researchers and practitioners. In view of the interdisciplinary nature of the field, it will be a useful reference not only for computer scientists and engineers, but for social scientists and management and organization scientists as well."
https://openalex.org/W2664267452,"Artificial intelligence in healthcare: past, present and future","{'Artificial': [0], 'intelligence': [1], '(AI)': [2], 'aims': [3], 'to': [4, 15, 48], 'mimic': [5], 'human': [6], 'cognitive': [7], 'functions.': [8], 'It': [9], 'is': [10], 'bringing': [11], 'a': [12], 'paradigm': [13], 'shift': [14], 'healthcare,': [16], 'powered': [17], 'by': [18], 'increasing': [19], 'availability': [20], 'of': [21, 27, 35, 51, 119, 150], 'healthcare': [22, 39, 52], 'data': [23, 53], 'and': [24, 40, 55, 74, 77, 101, 122, 130, 145], 'rapid': [25], 'progress': [26], 'analytics': [28], 'techniques.': [29], 'We': [30, 103, 133], 'survey': [31], 'the': [32, 69, 78, 109, 115], 'current': [33], 'status': [34], 'AI': [36, 44, 58, 96, 110, 139], 'applications': [37, 111], 'in': [38, 106, 112, 114], 'discuss': [41], 'its': [42], 'future.': [43], 'can': [45], 'be': [46], 'applied': [47], 'various': [49], 'types': [50], '(structured': [54], 'unstructured).': [56], 'Popular': [57], 'techniques': [59], 'include': [60, 98], 'machine': [61, 73], 'learning': [62], 'methods': [63], 'for': [64, 88, 147], 'structured': [65], 'data,': [66], 'such': [67, 141], 'as': [68, 82, 84, 125, 127, 142], 'classical': [70], 'support': [71], 'vector': [72], 'neural': [75], 'network,': [76], 'modern': [79], 'deep': [80], 'learning,': [81], 'well': [83, 126], 'natural': [85], 'language': [86], 'processing': [87], 'unstructured': [89], 'data.': [90], 'Major': [91], 'disease': [92], 'areas': [93, 118], 'that': [94], 'use': [95], 'tools': [97], 'cancer,': [99], 'neurology': [100], 'cardiology.': [102], 'then': [104], 'review': [105], 'more': [107], 'details': [108], 'stroke,': [113], 'three': [116], 'major': [117], 'early': [120], 'detection': [121], 'diagnosis,': [123], 'treatment,': [124], 'outcome': [128], 'prediction': [129], 'prognosis': [131], 'evaluation.': [132], 'conclude': [134], 'with': [135], 'discussion': [136], 'about': [137], 'pioneer': [138], 'systems,': [140], 'IBM': [143], 'Watson,': [144], 'hurdles': [146], 'real-life': [148], 'deployment': [149], 'AI.': [151]}",2017,"['Cognitive computing', 'Health care', 'Data science', 'Artificial intelligence', 'Computer science', 'Software deployment', 'Unstructured data', 'Watson', 'Applications of artificial intelligence', 'Big data', 'Analytics', 'IBM', 'Deep learning', 'Predictive analytics', 'Machine learning', 'Cognition', 'Medicine', 'Software engineering', 'Data mining', 'Materials science', 'Psychiatry', 'Economics', 'Nanotechnology', 'Economic growth']","Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI."
https://openalex.org/W2163605009,ImageNet Classification with Deep Convolutional Neural Networks,"{'We': [0, 120], 'trained': [1], 'a': [2, 77, 90, 108, 123, 134], 'large,': [3], 'deep': [4], 'convolutional': [5, 62], 'neural': [6, 49], 'network': [7], 'to': [8, 116, 143], 'classify': [9], 'the': [10, 16, 21, 26, 45, 96, 103, 129, 147], '1.2': [11], 'million': [12, 54], 'high-resolution': [13], 'images': [14], 'in': [15, 102, 128], 'ImageNet': [17], 'LSVRC-2010': [18], 'contest': [19], 'into': [20], '1000': [22], 'different': [23], 'classes.': [24], 'On': [25], 'test': [27, 137], 'data,': [28], 'we': [29, 85, 106], 'achieved': [30, 133, 145], 'top-1': [31], 'and': [32, 38, 56, 72, 89, 132], 'top-5': [33, 136], 'error': [34, 138], 'rates': [35], 'of': [36, 60, 65, 95, 125, 140], '37.5%': [37], '17.0%': [39], 'which': [40, 51, 66], 'is': [41], 'considerably': [42], 'better': [43], 'than': [44], 'previous': [46], 'state-of-the-art.': [47], 'The': [48], 'network,': [50], 'has': [52], '60': [53], 'parameters': [55], '650,000': [57], 'neurons,': [58], 'consists': [59], 'five': [61], 'layers,': [63, 71], 'some': [64], 'are': [67], 'followed': [68], 'by': [69, 146], 'max-pooling': [70], 'three': [73], 'fully-connected': [74, 104], 'layers': [75, 105], 'with': [76], 'final': [78], '1000-way': [79], 'softmax.': [80], 'To': [81, 99], 'make': [82], 'training': [83], 'faster,': [84], 'used': [86], 'non-saturating': [87], 'neurons': [88], 'very': [91, 118], 'efficient': [92], 'GPU': [93], 'implementation': [94], 'convolution': [97], 'operation.': [98], 'reduce': [100], 'overriding': [101], 'employed': [107], 'recently-developed': [109], 'regularization': [110], 'method': [111], 'called': [112], 'dropout': [113], 'that': [114], 'proved': [115], 'be': [117], 'effective.': [119], 'also': [121], 'entered': [122], 'variant': [124], 'this': [126], 'model': [127], 'ILSVRC-2012': [130], 'competition': [131], 'winning': [135], 'rate': [139], '15.3%,': [141], 'compared': [142], '26.2%': [144], 'second-best': [148], 'entry.': [149]}",2012,"['Softmax function', 'Convolutional neural network', 'Computer science', 'Pooling', 'Dropout (neural networks)', 'Artificial intelligence', 'Convolution (computer science)', 'Regularization (linguistics)', 'Pattern recognition (psychology)', 'Deep neural networks', 'Word error rate', 'Normalization (sociology)', 'Artificial neural network', 'Machine learning', 'Sociology', 'Anthropology']","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called dropout that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
https://openalex.org/W2124776405,Neural Networks: A Comprehensive Foundation,"{'From': [0], 'the': [1, 5, 27, 35, 72], 'Publisher:\r\nThis': [2], 'book': [3, 77], 'represents': [4], 'most': [6], 'comprehensive': [7], 'treatment': [8], 'available': [9], 'of': [10, 30, 54], 'neural': [11, 55], 'networks': [12], 'from': [13], 'an': [14], 'engineering': [15, 67], 'perspective.': [16], 'Thorough,': [17], 'well-organized,': [18], 'and': [19, 49, 51, 61, 83, 98], 'completely': [20], 'up': [21], 'to': [22, 70], 'date,': [23], 'it': [24], 'examines': [25], 'all': [26], 'important': [28], 'aspects': [29], 'this': [31, 76, 87], 'emerging': [32], 'technology,': [33], 'including': [34], 'learning': [36], 'process,': [37], 'back-propagation': [38], 'learning,': [39], 'radial-basis': [40], 'function': [41], 'networks,': [42, 46], 'self-organizing': [43], 'systems,': [44], 'modular': [45], 'temporal': [47], 'processing': [48], 'neurodynamics,': [50], 'VLSI': [52], 'implementation': [53], 'networks.': [56], 'Written': [57], 'in': [58], 'a': [59, 65, 95], 'concise': [60], 'fluid': [62], 'manner,': [63], 'by': [64], 'foremost': [66], 'textbook': [68], 'author,': [69], 'make': [71], 'material': [73], 'more': [74], 'accessible,': [75], 'is': [78], 'ideal': [79], 'for': [80], 'professional': [81], 'engineers': [82], 'graduate': [84], 'students': [85], 'entering': [86], 'exciting': [88], 'field.': [89], 'Computer': [90], 'experiments,': [91], 'problems,': [92], 'worked': [93], 'examples,': [94], 'bibliography,': [96], 'photographs,': [97], 'illustrations': [99], 'reinforce': [100], 'key': [101], 'concepts.': [102]}",1998,"['Modular design', 'Computer science', 'Field (mathematics)', 'Artificial neural network', 'Process (computing)', 'Function (biology)', 'Ideal (ethics)', 'Key (lock)', 'Perspective (graphical)', 'Artificial intelligence', 'Foundation (evidence)', 'Engineering', 'Geography', 'Epistemology', 'Mathematics', 'Philosophy', 'Computer security', 'Archaeology', 'Evolutionary biology', 'Pure mathematics', 'Biology', 'Operating system']","From the Publisher:
This book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts."
https://openalex.org/W2618530766,ImageNet classification with deep convolutional neural networks,"{'We': [0, 124], 'trained': [1], 'a': [2, 79, 92, 111, 127, 138], 'large,': [3], 'deep': [4], 'convolutional': [5, 63], 'neural': [6, 50], 'network': [7], 'to': [8, 120, 147], 'classify': [9], 'the': [10, 16, 21, 26, 46, 98, 105, 133, 151], '1.2': [11], 'million': [12, 55], 'high-resolution': [13], 'images': [14], 'in': [15, 104, 132], 'ImageNet': [17], 'LSVRC-2010': [18], 'contest': [19], 'into': [20], '1000': [22], 'different': [23], 'classes.': [24], 'On': [25], 'test': [27, 141], 'data,': [28], 'we': [29, 87, 109], 'achieved': [30, 137, 149], 'top-1': [31], 'and': [32, 38, 57, 73, 91, 136], 'top-5': [33, 140], 'error': [34, 142], 'rates': [35], 'of': [36, 61, 66, 97, 129, 144], '37.5%': [37], '17.0%,': [39], 'respectively,': [40], 'which': [41, 52, 67], 'is': [42], 'considerably': [43], 'better': [44], 'than': [45], 'previous': [47], 'state-of-the-art.': [48], 'The': [49], 'network,': [51], 'has': [53], '60': [54], 'parameters': [56], '650,000': [58], 'neurons,': [59], 'consists': [60], 'five': [62], 'layers,': [64, 72], 'some': [65], 'are': [68], 'followed': [69], 'by': [70, 150], 'max-pooling': [71], 'three': [74], 'fully': [75, 106], 'connected': [76, 107], 'layers': [77, 108], 'with': [78], 'final': [80], '1000-way': [81], 'softmax.': [82], 'To': [83, 101], 'make': [84], 'training': [85], 'faster,': [86], 'used': [88], 'non-saturating': [89], 'neurons': [90], 'very': [93, 122], 'efficient': [94], 'GPU': [95], 'implementation': [96], 'convolution': [99], 'operation.': [100], 'reduce': [102], 'overfitting': [103], 'employed': [110], 'recently': [112], 'developed': [113], 'regularization': [114], 'method': [115], 'called': [116], '""dropout""': [117], 'that': [118], 'proved': [119], 'be': [121], 'effective.': [123], 'also': [125], 'entered': [126], 'variant': [128], 'this': [130], 'model': [131], 'ILSVRC-2012': [134], 'competition': [135], 'winning': [139], 'rate': [143], '15.3%,': [145], 'compared': [146], '26.2%': [148], 'second-best': [152], 'entry.': [153]}",2017,"['Softmax function', 'Computer science', 'Overfitting', 'Convolutional neural network', 'Pooling', 'Dropout (neural networks)', 'Artificial intelligence', 'Pattern recognition (psychology)', 'Regularization (linguistics)', 'Convolution (computer science)', 'Deep neural networks', 'Word error rate', 'Deep learning', 'Artificial neural network', 'Machine learning']","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
https://openalex.org/W2095705004,Dropout: a simple way to prevent neural networks from overfitting,"{'Deep': [0], 'neural': [1, 46, 73, 146], 'nets': [2, 47], 'with': [3, 36, 68], 'a': [4, 18, 53, 118], 'large': [5, 45], 'number': [6, 91], 'of': [7, 42, 92, 106, 110, 145], 'parameters': [8], 'are': [9, 26], 'very': [10], 'powerful': [11], 'machine': [12], 'learning': [13, 150], 'systems.': [14], 'However,': [15], 'overfitting': [16, 37, 129], 'is': [17, 52, 62, 100], 'serious': [19], 'problem': [20], 'in': [21, 152], 'such': [22], 'networks.': [23, 95], 'Large': [24], 'networks': [25, 114, 147], 'also': [27], 'slow': [28], 'to': [29, 34, 63, 102], 'use,': [30], 'making': [31], 'it': [32, 99], 'difficult': [33], 'deal': [35], 'by': [38, 115], 'combining': [39], 'the': [40, 72, 104, 108, 143], 'predictions': [41, 109], 'many': [43, 165], 'different': [44, 93], 'at': [48], 'test': [49, 97], 'time.': [50], 'Dropout': [51], 'technique': [54], 'for': [55], 'addressing': [56], 'this': [57], 'problem.': [58], 'The': [59], 'key': [60], 'idea': [61], 'randomly': [64], 'drop': [65], 'units': [66, 79], '(along': [67], 'their': [69], 'connections)': [70], 'from': [71, 80, 88], 'network': [74, 121], 'during': [75], 'training.': [76], 'This': [77, 126], 'prevents': [78], 'co-adapting': [81], 'too': [82], 'much.': [83], 'During': [84], 'training,': [85], 'dropout': [86, 141], 'samples': [87], 'an': [89], 'exponential': [90], 'thinned': [113], 'At': [96], 'time,': [98], 'easy': [101], 'approximate': [103], 'effect': [105], 'averaging': [107], 'all': [111], 'these': [112], 'simply': [116], 'using': [117], 'single': [119], 'unthinned': [120], 'that': [122, 140], 'has': [123], 'smaller': [124], 'weights.': [125], 'significantly': [127], 'reduces': [128], 'and': [130, 158], 'gives': [131], 'major': [132], 'improvements': [133], 'over': [134], 'other': [135], 'regularization': [136], 'methods.': [137], 'We': [138], 'show': [139], 'improves': [142], 'performance': [144], 'on': [148, 164], 'supervised': [149], 'tasks': [151], 'vision,': [153], 'speech': [154], 'recognition,': [155], 'document': [156], 'classification': [157], 'computational': [159], 'biology,': [160], 'obtaining': [161], 'state-of-the-art': [162], 'results': [163], 'benchmark': [166], 'data': [167], 'sets.': [168]}",2014,"['Overfitting', 'Dropout (neural networks)', 'Computer science', 'Artificial intelligence', 'Artificial neural network', 'Machine learning', 'Benchmark (surveying)', 'Deep neural networks', 'Regularization (linguistics)', 'Geodesy', 'Geography']","Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
https://openalex.org/W1554663460,Neural networks for pattern recognition,"{'From': [0], 'the': [1, 4, 13, 21, 24, 34, 39, 91], 'Publisher:\r\nThis': [2], 'is': [3], 'first': [5], 'comprehensive': [6], 'treatment': [7], 'of': [8, 15, 38, 53, 93], 'feed-forward': [9], 'neural': [10, 66, 94], 'networks': [11], 'from': [12], 'perspective': [14], 'statistical': [16], 'pattern': [17, 97], 'recognition.': [18, 98], 'After': [19], 'introducing': [20], 'basic': [22], 'concepts,': [23], 'book': [25], 'examines': [26], 'techniques': [27, 70], 'for': [28, 58], 'modelling': [29], 'probability': [30], 'density': [31], 'functions': [32], 'and': [33, 36, 42, 63, 68, 71, 96], 'properties': [35], 'merits': [37], 'multi-layer': [40], 'perceptron': [41], 'radial': [43], 'basis': [44], 'function': [45, 60], 'network': [46], 'models.': [47], 'Also': [48], 'covered': [49], 'are': [50], 'various': [51], 'forms': [52], 'error': [54, 59], 'functions,': [55], 'principal': [56], 'algorithms': [57], 'minimalization,': [61], 'learning': [62], 'generalization': [64], 'in': [65, 90], 'networks,': [67], 'Bayesian': [69], 'their': [72], 'applications.': [73], 'Designed': [74], 'as': [75], 'a': [76], 'text,': [77], 'with': [78], 'over': [79], '100': [80], 'exercises,': [81], 'this': [82], 'fully': [83], 'up-to-date': [84], 'work': [85], 'will': [86], 'benefit': [87], 'anyone': [88], 'involved': [89], 'fields': [92], 'computation': [95]}",1994,"['Computer science', 'Pattern recognition (psychology)', 'Artificial intelligence']","From the Publisher:
This is the first comprehensive treatment of feed-forward neural networks from the perspective of statistical pattern recognition. After introducing the basic concepts, the book examines techniques for modelling probability density functions and the properties and merits of the multi-layer perceptron and radial basis function network models. Also covered are various forms of error functions, principal algorithms for error function minimalization, learning and generalization in neural networks, and Bayesian techniques and their applications. Designed as a text, with over 100 exercises, this fully up-to-date work will benefit anyone involved in the fields of neural computation and pattern recognition."
https://openalex.org/W2128084896,Neural networks and physical systems with emergent collective computational abilities.,"{'Computational': [0], 'properties': [1, 17, 71, 112, 129], 'of': [2, 4, 11, 18, 24, 33, 44, 47, 52, 61, 72, 88, 97, 100, 136, 142], 'use': [3], 'biological': [5], 'organisms': [6], 'or': [7, 139], 'to': [8, 66, 134], 'the': [9, 45, 94, 98, 101, 137, 140], 'construction': [10], 'computers': [12], 'can': [13], 'emerge': [14], 'as': [15], 'collective': [16, 70, 111, 128], 'systems': [19], 'having': [20], 'a': [21, 48, 54, 76], 'large': [22], 'number': [23], 'simple': [25], 'equivalent': [26], 'components': [27], '(or': [28], 'neurons).': [29], 'The': [30, 69, 91, 127], 'physical': [31], 'meaning': [32], 'content-addressable': [34, 77], 'memory': [35, 78, 84], 'is': [36, 56, 103], 'described': [37], 'by': [38], 'an': [39, 82], 'appropriate': [40], 'phase': [41], 'space': [42], 'flow': [43], 'state': [46, 99], 'system.': [49], 'A': [50], 'model': [51, 74], 'such': [53], 'system': [55, 102], 'given,': [57], 'based': [58, 104], 'on': [59, 105], 'aspects': [60], 'neurobiology': [62], 'but': [63], 'readily': [64], 'adapted': [65], 'integrated': [67], 'circuits.': [68], 'this': [73], 'produce': [75], 'which': [79], 'correctly': [80], 'yields': [81], 'entire': [83], 'from': [85], 'any': [86], 'subpart': [87], 'sufficient': [89], 'size.': [90], 'algorithm': [92], 'for': [93, 116], 'time': [95, 124], 'evolution': [96], 'asynchronous': [106], 'parallel': [107], 'processing.': [108], 'Additional': [109], 'emergent': [110], 'include': [113], 'some': [114], 'capacity': [115], 'generalization,': [117], 'familiarity': [118], 'recognition,': [119], 'categorization,': [120], 'error': [121], 'correction,': [122], 'and': [123], 'sequence': [125], 'retention.': [126], 'are': [130], 'only': [131], 'weakly': [132], 'sensitive': [133], 'details': [135], 'modeling': [138], 'failure': [141], 'individual': [143], 'devices.': [144]}",1982,"['Computer science', 'Asynchronous communication', 'Collective behavior', 'Generalization', 'Simple (philosophy)', 'Categorization', 'Content-addressable memory', 'Neural system', 'Artificial neural network', 'Physical system', 'State space', 'Meaning (existential)', 'Theoretical computer science', 'Artificial intelligence', 'Cognitive science', 'Neuroscience', 'Psychology', 'Sociology', 'Anthropology', 'Psychotherapist', 'Physics', 'Quantum mechanics', 'Mathematical analysis', 'Computer network', 'Philosophy', 'Statistics', 'Mathematics', 'Epistemology']","Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices."
https://openalex.org/W2100495367,Reducing the Dimensionality of Data with Neural Networks,"{'High-dimensional': [0], 'data': [1], 'can': [2, 26], 'be': [3, 27], 'converted': [4], 'to': [5, 19, 48, 66, 81], 'low-dimensional': [6, 68], 'codes': [7, 69], 'by': [8], 'training': [9], 'a': [10, 15, 49, 79], 'multilayer': [11], 'neural': [12], 'network': [13], 'with': [14], 'small': [16], 'central': [17], 'layer': [18], 'reconstruct': [20], 'high-dimensional': [21], 'input': [22], 'vectors.': [23], 'Gradient': [24], 'descent': [25], 'used': [28], 'for': [29], 'fine-tuning': [30], 'the': [31, 43, 59, 83], 'weights': [32, 45, 60], 'in': [33], 'such': [34], '“autoencoder”': [35], 'networks,': [36], 'but': [37], 'this': [38], 'works': [39], 'well': [40], 'only': [41], 'if': [42], 'initial': [44], 'are': [46], 'close': [47], 'good': [50], 'solution.': [51], 'We': [52], 'describe': [53], 'an': [54], 'effective': [55], 'way': [56], 'of': [57, 85], 'initializing': [58], 'that': [61, 70], 'allows': [62], 'deep': [63], 'autoencoder': [64], 'networks': [65], 'learn': [67], 'work': [71], 'much': [72], 'better': [73], 'than': [74], 'principal': [75], 'components': [76], 'analysis': [77], 'as': [78], 'tool': [80], 'reduce': [82], 'dimensionality': [84], 'data.': [86]}",2006,"['Autoencoder', 'Curse of dimensionality', 'Initialization', 'Gradient descent', 'Artificial neural network', 'Computer science', 'Principal component analysis', 'Artificial intelligence', 'Pattern recognition (psychology)', 'Layer (electronics)', 'High dimensional', 'Principal (computer security)', 'Algorithm', 'Materials science', 'Nanotechnology', 'Programming language', 'Operating system']","High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data."
https://openalex.org/W2076063813,Deep learning in neural networks: An overview,,2014,"['Artificial intelligence', 'Deep learning', 'Computer science', 'Artificial neural network', 'Backpropagation', 'Reinforcement learning', 'Machine learning', 'Deep neural networks', 'Unsupervised learning', 'Recurrent neural network', 'Encoding (memory)', 'Evolutionary computation', 'Supervised learning']",
https://openalex.org/W1821462560,Distilling the Knowledge in a Neural Network,"{'A': [0], 'very': [1], 'simple': [2], 'way': [3], 'to': [4, 15, 26, 47, 50, 75, 90, 161], 'improve': [5, 117], 'the': [6, 21, 58, 77, 118, 129, 166], 'performance': [7], 'of': [8, 37, 54, 121, 134, 146, 149, 173], 'almost': [9], 'any': [10], 'machine': [11], 'learning': [12], 'algorithm': [13], 'is': [14, 39, 73, 87], 'train': [16], 'many': [17, 156], 'different': [18, 100], 'models': [19, 38, 60, 135, 154, 158, 168, 177], 'on': [20, 108], 'same': [22], 'data': [23], 'and': [24, 41, 66, 92, 110, 155, 182], 'then': [25], 'average': [27], 'their': [28], 'predictions.': [29], 'Unfortunately,': [30], 'making': [31], 'predictions': [32], 'using': [33, 98], 'a': [34, 51, 83, 99, 122, 137, 143, 171], 'whole': [35], 'ensemble': [36, 81, 133, 147], 'cumbersome': [40], 'may': [42], 'be': [43, 179], 'too': [44], 'computationally': [45], 'expensive': [46], 'allow': [48], 'deployment': [49], 'large': [52, 62], 'number': [53], 'users,': [55], 'especially': [56], 'if': [57], 'individual': [59], 'are': [61], 'neural': [63], 'nets.': [64], 'Caruana': [65], 'his': [67], 'collaborators': [68], 'have': [69], 'shown': [70], 'that': [71, 113, 165], 'it': [72], 'possible': [74], 'compress': [76], 'knowledge': [78, 130], 'in': [79, 131, 183], 'an': [80, 132], 'into': [82, 136], 'single': [84, 138], 'model': [85, 120], 'which': [86, 159], 'much': [88], 'easier': [89], 'deploy': [91], 'we': [93, 111, 114], 'develop': [94], 'this': [95], 'approach': [96], 'further': [97], 'compression': [101], 'technique.': [102], 'We': [103, 140], 'achieve': [104], 'some': [105], 'surprising': [106], 'results': [107], 'MNIST': [109], 'show': [112], 'can': [115, 178], 'significantly': [116], 'acoustic': [119], 'heavily': [123], 'used': [124], 'commercial': [125], 'system': [126], 'by': [127], 'distilling': [128], 'model.': [139], 'also': [141], 'introduce': [142], 'new': [144], 'type': [145], 'composed': [148], 'one': [150], 'or': [151], 'more': [152], 'full': [153, 167], 'specialist': [157, 176], 'learn': [160], 'distinguish': [162], 'fine-grained': [163], 'classes': [164], 'confuse.': [169], 'Unlike': [170], 'mixture': [172], 'experts,': [174], 'these': [175], 'trained': [180], 'rapidly': [181], 'parallel.': [184]}",2015,"['Artificial neural network', 'Computer science', 'Artificial intelligence']","A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel."
https://openalex.org/W2130942839,Sequence to Sequence Learning with Neural Networks,"{'Deep': [0], 'Neural': [1], 'Networks': [2], '(DNNs)': [3], 'are': [4, 25, 202, 208], 'powerful': [5], 'models': [6], 'that': [7, 48, 94, 201, 221], 'have': [8, 138], 'achieved': [9], 'excellent': [10], 'performance': [11, 239], 'on': [12, 52, 95, 118, 130, 140, 155, 189], 'difficult': [13], 'learning': [14, 47], 'tasks.': [15], 'Although': [16], 'DNNs': [17], 'work': [18], 'well': [19], 'whenever': [20], 'large': [21], 'labeled': [22], 'training': [23], 'sets': [24], 'available,': [26], 'they': [27], 'cannot': [28], 'be': [29], 'used': [30, 161], 'to': [31, 34, 45, 65, 70, 82, 98, 164, 179, 184, 204, 211], 'map': [32, 66], 'sequences': [33], 'sequences.': [35], 'In': [36], 'this': [37, 190], 'paper,': [38], 'we': [39, 160, 219], 'present': [40], 'a': [41, 59, 71, 74, 113, 145, 150], 'general': [42], 'end-to-end': [43], 'approach': [44], 'sequence': [46, 54, 69, 86], 'makes': [49], 'minimal': [50], 'assumptions': [51], 'the': [53, 67, 84, 88, 103, 106, 110, 119, 124, 134, 156, 162, 166, 171, 185, 212, 215, 223, 226, 237, 250, 253, 258], 'structure.': [55], 'Our': [56, 90], 'method': [57], 'uses': [58], 'multilayered': [60], 'Long': [61], 'Short-Term': [62], 'Memory': [63], '(LSTM)': [64], 'input': [68], 'vector': [72], 'of': [73, 116, 153, 225], 'fixed': [75], 'dimensionality,': [76], 'and': [77, 198, 207, 214, 252], 'then': [78], 'another': [79], 'deep': [80], 'LSTM': [81, 111, 135, 163, 193], 'decode': [83], 'target': [85, 234, 254], 'from': [87, 102], 'vector.': [89], 'main': [91], 'result': [92, 188], 'is': [93, 182], 'an': [96], 'English': [97], 'French': [99], 'translation': [100], 'task': [101], ""WMT'14"": [104], 'dataset,': [105], 'translations': [107], 'produced': [108, 169], 'by': [109, 170], 'achieve': [112], 'BLEU': [114, 126, 151, 176], 'score': [115, 127, 152, 177], '34.8': [117], 'entire': [120], 'test': [121], 'set,': [122], 'where': [123], ""LSTM's"": [125, 238], 'was': [128], 'penalized': [129], 'out-of-vocabulary': [131], 'words.': [132], 'Additionally,': [133], 'did': [136], 'not': [137, 233], 'difficulty': [139], 'long': [141], 'sentences.': [142], 'For': [143], 'comparison,': [144], 'phrase-based': [146], 'SMT': [147, 173], 'system': [148], 'achieves': [149], '33.3': [154], 'same': [157], 'dataset.': [158], 'When': [159], 'rerank': [165], '1000': [167], 'hypotheses': [168], 'aforementioned': [172], 'system,': [174], 'its': [175], 'increases': [178], '36.5,': [180], 'which': [181, 256], 'close': [183], 'previous': [186], 'best': [187], 'task.': [191], 'The': [192], 'also': [194], 'learned': [195], 'sensible': [196], 'phrase': [197], 'sentence': [199, 255], 'representations': [200], 'sensitive': [203], 'word': [205], 'order': [206, 224], 'relatively': [209], 'invariant': [210], 'active': [213], 'passive': [216], 'voice.': [217], 'Finally,': [218], 'found': [220], 'reversing': [222], 'words': [227], 'in': [228], 'all': [229], 'source': [230, 251], 'sentences': [231], '(but': [232], 'sentences)': [235], 'improved': [236], 'markedly,': [240], 'because': [241], 'doing': [242], 'so': [243], 'introduced': [244], 'many': [245], 'short': [246], 'term': [247], 'dependencies': [248], 'between': [249], 'made': [257], 'optimization': [259], 'problem': [260], 'easier.': [261]}",2014,"['Computer science', 'Artificial intelligence', 'Sentence', 'Phrase', 'Sequence (biology)', 'Natural language processing', 'Word (group theory)', 'Task (project management)', 'Speech recognition', 'Recurrent neural network', 'Artificial neural network', 'Machine translation', 'Vocabulary', 'Deep learning', 'Mathematics', 'Genetics', 'Biology', 'Linguistics', 'Philosophy', 'Geometry', 'Management', 'Economics']","Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier."
https://openalex.org/W1479807131,Semi-Supervised Learning,"{'A': [0], 'comprehensive': [1, 95], 'review': [2], 'of': [3, 6, 14, 24, 38, 78, 97, 104, 140, 146, 157, 194, 214], 'an': [4, 155], 'area': [5], 'machine': [7, 39], 'learning': [8, 42, 50, 60, 219], 'that': [9, 164, 174], 'deals': [10], 'with': [11, 211], 'the': [12, 25, 36, 45, 105, 121, 127, 141, 144, 160, 166, 192, 199, 215], 'use': [13], 'unlabeled': [15, 83], 'data': [16, 65, 84], 'in': [17, 69, 73, 81], 'classification': [18], 'problems:': [19], 'state-of-the-art': [20, 100], 'algorithms,': [21, 101], 'a': [22, 102, 212], 'taxonomy': [23, 103], 'field,': [26, 106], 'applications,': [27, 108], 'benchmark': [28, 109, 196], 'experiments,': [29, 110], 'and': [30, 58, 91, 111, 115, 124, 136, 172, 184, 220], 'directions': [31, 204], 'for': [32, 187, 205], 'future': [33, 116], 'research.': [34, 207], 'In': [35], 'field': [37], 'learning,': [40], 'semi-supervised': [41, 218], '(SSL)': [43], 'occupies': [44], 'middle': [46], 'ground,': [47], 'between': [48, 217], 'supervised': [49], '(in': [51, 61], 'which': [52, 62, 82], 'all': [53], 'training': [54], 'examples': [55], 'are': [56, 66, 85], 'labeled)': [57], 'unsupervised': [59], 'no': [63], 'label': [64], 'given).': [67], 'Interest': [68], 'SSL': [70, 98, 147, 182, 188, 206], 'has': [71], 'increased': [72], 'recent': [74], 'years,': [75], 'particularly': [76], 'because': [77], 'application': [79], 'domains': [80], 'plentiful,': [86], 'such': [87], 'as': [88], 'images,': [89], 'text,': [90], 'bioinformatics.': [92], 'This': [93], 'first': [94, 119], 'overview': [96], 'presents': [99, 120], 'selected': [107], 'perspectives': [112], 'on': [113], 'ongoing': [114], 'research.Semi-Supervised': [117], 'Learning': [118], 'key': [122], 'assumptions': [123], 'ideas': [125], 'underlying': [126], 'field:': [128], 'smoothness,': [129], 'cluster': [130], 'or': [131], 'low-density': [132, 167], 'separation,': [133], 'manifold': [134], 'structure,': [135], 'transduction.': [137, 221], 'The': [138, 178, 208], 'core': [139], 'book': [142, 161, 179, 200, 209], 'is': [143], 'presentation': [145], 'methods,': [148, 171], 'organized': [149], 'according': [150], 'to': [151], 'algorithmic': [152], 'strategies.': [153], 'After': [154], 'examination': [156], 'generative': [158], 'models,': [159], 'describes': [162], 'algorithms': [163, 173], 'implement': [165], 'separation': [168], 'assumption,': [169], 'graph-based': [170], 'perform': [175], 'two-step': [176], 'learning.': [177], 'then': [180], 'discusses': [181], 'applications': [183], 'offers': [185], 'guidelines': [186], 'practitioners': [189], 'by': [190], 'analyzing': [191], 'results': [193], 'extensive': [195], 'experiments.': [197], 'Finally,': [198], 'looks': [201], 'at': [202], 'interesting': [203], 'closes': [210], 'discussion': [213], 'relationship': [216]}",2006,"['Computer science', 'Artificial intelligence', 'Machine learning', 'Benchmark (surveying)', 'Field (mathematics)', 'Unsupervised learning', 'Semi-supervised learning', 'Graph', 'Taxonomy (biology)', 'Theoretical computer science', 'Mathematics', 'Botany', 'Geodesy', 'Pure mathematics', 'Biology', 'Geography']","A comprehensive review of an area of machine learning that deals with the use of unlabeled data in classification problems: state-of-the-art algorithms, a taxonomy of the field, applications, benchmark experiments, and directions for future research. In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research.Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction."
https://openalex.org/W2136504847,Semi-Supervised Learning Literature Survey,,2005,"['Computer science', 'Artificial intelligence']",
https://openalex.org/W2051812123,A scaled conjugate gradient algorithm for fast supervised learning,,1993,"['Broyden–Fletcher–Goldfarb–Shanno algorithm', 'Conjugate gradient method', 'Algorithm', 'Nonlinear conjugate gradient method', 'Computer science', 'Conjugate', 'Line (geometry)', 'Line search', 'Gradient descent', 'Backpropagation', 'Gradient method', 'Artificial intelligence', 'Mathematics', 'Artificial neural network', 'Computer network', 'Mathematical analysis', 'Geometry', 'Asynchronous communication', 'Computer security', 'RADIUS']",
https://openalex.org/W2129018774,An empirical comparison of supervised learning algorithms,"{'A': [0], 'number': [1], 'of': [2, 19, 83, 89, 92], 'supervised': [3, 20, 38], 'learning': [4, 21, 39, 98], 'methods': [5], 'have': [6], 'been': [7], 'introduced': [8], 'in': [9, 26], 'the': [10, 14, 23, 27, 64, 68, 87, 97], 'last': [11, 15], 'decade.': [12], 'Unfortunately,': [13], 'comprehensive': [16], 'empirical': [17, 34], 'evaluation': [18], 'was': [22], 'Statlog': [24], 'Project': [25], 'early': [28], ""90's."": [29], 'We': [30, 61], 'present': [31], 'a': [32, 90], 'large-scale': [33], 'comparison': [35], 'between': [36], 'ten': [37], 'methods:': [40], 'SVMs,': [41], 'neural': [42], 'nets,': [43], 'logistic': [44], 'regression,': [45], 'naive': [46], 'bayes,': [47], 'memory-based': [48], 'learning,': [49], 'random': [50], 'forests,': [51], 'decision': [52], 'trees,': [53, 55, 57], 'bagged': [54], 'boosted': [56, 59], 'and': [58, 73], 'stumps.': [60], 'also': [62], 'examine': [63], 'effect': [65], 'that': [66], 'calibrating': [67], 'models': [69], 'via': [70], 'Platt': [71], 'Scaling': [72], 'Isotonic': [74], 'Regression': [75], 'has': [76], 'on': [77], 'their': [78], 'performance.': [79], 'An': [80], 'important': [81], 'aspect': [82], 'our': [84], 'study': [85], 'is': [86], 'use': [88], 'variety': [91], 'performance': [93], 'criteria': [94], 'to': [95], 'evaluate': [96], 'methods.': [99]}",2006,"['Machine learning', 'Artificial intelligence', 'Computer science', 'Decision tree', 'Isotonic regression', 'Random forest', 'Supervised learning', 'Naive Bayes classifier', 'Support vector machine', 'Logistic regression', 'Empirical research', 'Regression', 'Artificial neural network', 'Mathematics', 'Statistics', 'Estimator']","A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods."
https://openalex.org/W2139823104,Semi-supervised learning using Gaussian fields and harmonic functions,"{'An': [0], 'approach': [1], 'to': [2, 94, 123], 'semi-supervised': [3], 'learning': [4, 37, 77, 114], 'is': [5, 8, 39, 58, 66], 'proposed': [6], 'that': [7], 'based': [9], 'on': [10, 49], 'a': [11, 25, 45, 110], 'Gaussian': [12, 46], 'random': [13, 47, 83], 'field': [14, 48, 57], 'model.': [15], 'Labeled': [16], 'and': [17, 65, 87, 98, 118, 137], 'unlabeled': [18], 'data': [19], 'are': [20, 130], 'represented': [21], 'as': [22], 'vertices': [23], 'in': [24, 42, 60], 'weighted': [26], 'graph,': [27, 51], 'with': [28, 82], 'edge': [29], 'weights': [30], 'encoding': [31], 'the': [32, 53, 56, 99, 120], 'similarity': [33], 'between': [34], 'instances.': [35], 'The': [36, 75], 'problem': [38], 'then': [40], 'formulated': [41], 'terms': [43, 61], 'of': [44, 55, 62, 101, 112], 'this': [50], 'where': [52], 'mean': [54], 'characterized': [59], 'harmonic': [63], 'functions,': [64], 'efficiently': [67], 'obtained': [68, 103], 'using': [69], 'matrix': [70], 'methods': [71, 93], 'or': [72], 'belief': [73], 'propagation.': [74], 'resulting': [76], 'algorithms': [78], 'have': [79], 'intimate': [80], 'connections': [81], 'walks,': [84], 'electric': [85], 'networks,': [86], 'spectral': [88], 'graph': [89], 'theory.': [90], 'We': [91, 107], 'discuss': [92], 'incorporate': [95], 'class': [96], 'priors': [97], 'predictions': [100], 'classifiers': [102], 'by': [104, 115], 'supervised': [105], 'learning.': [106], 'also': [108], 'propose': [109], 'method': [111], 'parameter': [113], 'entropy': [116], 'minimization,': [117], 'show': [119], ""algorithm's"": [121], 'ability': [122], 'perform': [124], 'feature': [125], 'selection.': [126], 'Promising': [127], 'experimental': [128], 'results': [129], 'presented': [131], 'for': [132], 'synthetic': [133], 'data,': [134], 'digit': [135], 'classification,': [136], 'text': [138], 'classification': [139], 'tasks.': [140]}",2003,"['Artificial intelligence', 'Gaussian', 'Semi-supervised learning', 'Pattern recognition (psychology)', 'Belief propagation', 'Supervised learning', 'Entropy (arrow of time)', 'Graph', 'Computer science', 'Mathematics', 'Machine learning', 'Algorithm', 'Artificial neural network', 'Theoretical computer science', 'Decoding methods', 'Physics', 'Quantum mechanics']","An approach to semi-supervised learning is proposed that is based on a Gaussian random field model. Labeled and unlabeled data are represented as vertices in a weighted graph, with edge weights encoding the similarity between instances. The learning problem is then formulated in terms of a Gaussian random field on this graph, where the mean of the field is characterized in terms of harmonic functions, and is efficiently obtained using matrix methods or belief propagation. The resulting learning algorithms have intimate connections with random walks, electric networks, and spectral graph theory. We discuss methods to incorporate class priors and the predictions of classifiers obtained by supervised learning. We also propose a method of parameter learning by entropy minimization, and show the algorithm's ability to perform feature selection. Promising experimental results are presented for synthetic data, digit classification, and text classification tasks."
https://openalex.org/W2996428491,ALBERT: A Lite BERT for Self-supervised Learning of Language Representations,"{'Increasing': [0], 'model': [1, 21, 101], 'size': [2], 'when': [3], 'pretraining': [4], 'natural': [5], 'language': [6], 'representations': [7], 'often': [8], 'results': [9, 105], 'in': [10], 'improved': [11], 'performance': [12], 'on': [13, 82, 106], 'downstream': [14, 91], 'tasks.': [15], 'However,': [16], 'at': [17, 128], 'some': [18], 'point': [19], 'further': [20], 'increases': [22], 'become': [23], 'harder': [24], 'due': [25], 'to': [26, 43, 63, 70, 118], 'GPU/TPU': [27], 'memory': [28, 45], 'limitations': [29], 'and': [30, 47, 86, 110, 122], 'longer': [31], 'training': [32, 50], 'times.': [33], 'To': [34], 'address': [35], 'these': [36], 'problems,': [37], 'we': [38], 'present': [39], 'two': [40], 'parameter-reduction': [41], 'techniques': [42], 'lower': [44], 'consumption': [46], 'increase': [48], 'the': [49, 71, 107, 123], 'speed': [51], 'of': [52], 'BERT.': [53, 73], 'Comprehensive': [54], 'empirical': [55], 'evidence': [56], 'shows': [57], 'that': [58, 65, 80], 'our': [59, 99], 'proposed': [60], 'methods': [61], 'lead': [62], 'models': [64, 125], 'scale': [66], 'much': [67], 'better': [68], 'compared': [69, 117], 'original': [72], 'We': [74], 'also': [75], 'use': [76], 'a': [77, 97], 'self-supervised': [78], 'loss': [79], 'focuses': [81], 'modeling': [83], 'inter-sentence': [84], 'coherence,': [85], 'show': [87], 'it': [88], 'consistently': [89], 'helps': [90], 'tasks': [92], 'with': [93], 'multi-sentence': [94], 'inputs.': [95], 'As': [96], 'result,': [98], 'best': [100], 'establishes': [102], 'new': [103], 'state-of-the-art': [104], 'GLUE,': [108], 'RACE,': [109], '\\squad': [111], 'benchmarks': [112], 'while': [113], 'having': [114], 'fewer': [115], 'parameters': [116], 'BERT-large.': [119], 'The': [120], 'code': [121], 'pretrained': [124], 'are': [126], 'available': [127], 'https://github.com/google-research/ALBERT.': [129]}",2019,"['Computer science', 'Sentence', 'Language model', 'Artificial intelligence', 'Code (set theory)', 'Natural language processing', 'Point (geometry)', 'Coherence (philosophical gambling strategy)', 'Machine learning', 'Programming language', 'Set (abstract data type)', 'Physics', 'Geometry', 'Mathematics', 'Quantum mechanics']","Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at https://github.com/google-research/ALBERT."
https://openalex.org/W2530395818,Equality of Opportunity in Supervised Learning,"{'We': [0, 116, 142], 'propose': [1], 'a': [2, 7, 147], 'criterion': [3], 'for': [4], 'discrimination': [5, 53], 'against': [6], 'specified': [8], 'sensitive': [9], 'attribute': [10], 'in': [11, 34], 'supervised': [12], 'learning,': [13], 'where': [14], 'the': [15, 29, 35, 65, 74, 82, 98, 102, 104, 107, 118], 'goal': [16], 'is': [17, 92], 'to': [18, 43, 51, 55, 73], 'predict': [19], 'some': [20], 'target': [21, 105], 'based': [22, 126], 'on': [23, 97, 112, 127], 'available': [24], 'features.': [25], 'Assuming': [26], 'data': [27], 'about': [28], 'predictor,': [30, 103], 'target,': [31], 'and': [32, 106, 123, 134], 'membership': [33], 'protected': [36, 108], 'group': [37], 'are': [38], 'available,': [39], 'we': [40], 'show': [41], 'how': [42], 'optimally': [44], 'adjust': [45], 'any': [46], 'learned': [47], 'predictor': [48], 'so': [49], 'as': [50], 'remove': [52], 'according': [54], 'our': [56, 90, 144], 'definition.': [57], 'Our': [58], 'framework': [59], 'also': [60], 'improves': [61], 'incentives': [62], 'by': [63, 80], 'shifting': [64], 'cost': [66], 'of': [67, 101, 114, 121, 150], 'poor': [68], 'classification': [69, 83], 'from': [70, 138], 'disadvantaged': [71], 'groups': [72], 'decision': [75], 'maker,': [76], 'who': [77], 'can': [78, 133], 'respond': [79], 'improving': [81], 'accuracy.': [84], 'In': [85], 'line': [86], 'with': [87], 'other': [88], 'studies,': [89], 'notion': [91, 145], 'oblivious:': [93], 'it': [94], 'depends': [95], 'only': [96], 'joint': [99], 'statistics': [100], 'attribute,': [109], 'but': [110], 'not': [111], 'interpretation': [113], 'individualfeatures.': [115], 'study': [117, 149], 'inherent': [119], 'limits': [120], 'defining': [122], 'identifying': [124], 'biases': [125], 'such': [128], 'oblivious': [129, 140], 'measures,': [130], 'outlining': [131], 'what': [132], 'cannot': [135], 'be': [136], 'inferred': [137], 'different': [139], 'tests.': [141], 'illustrate': [143], 'using': [146], 'case': [148], 'FICO': [151], 'credit': [152], 'scores.': [153]}",2016,"['Computer science', 'Interpretation (philosophy)', 'Disadvantaged', 'Machine learning', 'Artificial intelligence', 'Incentive', 'Decision maker', 'Line (geometry)', 'Mathematics', 'Microeconomics', 'Operations research', 'Economics', 'Geometry', 'Programming language', 'Economic growth']","We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores."
https://openalex.org/W1990334093,Introduction to Semi-Supervised Learning,"{'Semi-supervised': [0], 'learning': [1, 4], 'is': [2], 'a': [3], 'paradigm': [5], 'concerned': [6], 'with': [7], 'the': [8, 21], 'study': [9], 'of': [10, 23], 'how': [11], 'computers': [12], 'and': [13, 26], 'natural': [14], 'systems': [15], 'such': [16], 'as': [17], 'humans': [18], 'learn': [19], 'in': [20], 'presence': [22], 'both': [24], 'labeled': [25], 'unlabeled': [27], 'data.': [28], 'Traditionally,': [29], 'le': [30]}",2009,"['Semi-supervised learning', 'Computer science', 'Artificial intelligence', 'Supervised learning', 'Natural (archaeology)', 'Machine learning', 'Geography', 'Artificial neural network', 'Archaeology']","Semi-supervised learning is a learning paradigm concerned with the study of how computers and natural systems such as humans learn in the presence of both labeled and unlabeled data. Traditionally, le"
https://openalex.org/W2984353870,A survey on semi-supervised learning,"{'Abstract': [0], 'Semi-supervised': [1], 'learning': [2, 8, 21, 134, 157, 244], 'is': [3], 'the': [4, 33, 62, 83, 114, 152, 171, 184, 191, 199, 220, 231, 238, 257], 'branch': [5], 'of': [6, 36, 50, 97, 116, 132, 155, 183, 212], 'machine': [7, 67], 'concerned': [9], 'with': [10, 46, 69, 179, 195], 'using': [11], 'labelled': [12, 51], 'as': [13, 15, 139, 141, 173, 175], 'well': [14, 140, 174], 'unlabelled': [16, 37, 228], 'data': [17, 38, 229], 'to': [18, 107, 121, 164, 170, 249, 256], 'perform': [19], 'certain': [20], 'tasks.': [22], 'Conceptually': [23], 'situated': [24], 'between': [25], 'supervised': [26], 'and': [27, 77, 90, 100, 109, 118, 167, 187, 202, 223, 252], 'unsupervised': [28], 'learning,': [29, 68], 'it': [30], 'permits': [31], 'harnessing': [32], 'large': [34, 153], 'amounts': [35], 'available': [39], 'in': [40, 44, 57, 66, 88], 'many': [41], 'use': [42], 'cases': [43], 'combination': [45], 'typically': [47], 'smaller': [48], 'sets': [49], 'data.': [52], 'In': [53], 'recent': [54, 104, 143], 'years,': [55], 'research': [56, 158], 'this': [58, 111, 125], 'area': [59], 'has': [60, 85], 'followed': [61], 'general': [63], 'trends': [64], 'observed': [65], 'much': [70], 'attention': [71], 'directed': [72], 'at': [73], 'neural': [74], 'network-based': [75], 'models': [76], 'generative': [78], 'learning.': [79], 'The': [80], 'literature': [81], 'on': [82, 148, 198, 219], 'topic': [84], 'also': [86], 'expanded': [87], 'volume': [89], 'scope,': [91], 'now': [92], 'encompassing': [93], 'a': [94, 180, 209], 'broad': [95], 'spectrum': [96], 'theory,': [98], 'algorithms': [99, 188, 245], 'applications.': [101], 'However,': [102], 'no': [103], 'surveys': [105], 'exist': [106], 'collect': [108], 'organize': [110], 'knowledge,': [112], 'impeding': [113], 'ability': [115], 'researchers': [117, 166], 'engineers': [119], 'alike': [120], 'utilize': [122], 'it.': [123], 'Filling': [124], 'void,': [126], 'we': [127, 207, 235], 'present': [128], 'an': [129, 196], 'up-to-date': [130], 'overview': [131], 'semi-supervised': [133, 149, 156, 213, 243, 259], 'methods,': [135], 'covering': [136], 'earlier': [137], 'work': [138], 'more': [142, 176], 'advances.': [144], 'We': [145], 'focus': [146], 'primarily': [147], 'classification,': [150], 'where': [151], 'majority': [154], 'takes': [159], 'place.': [160], 'Our': [161], 'survey': [162], 'aims': [163], 'provide': [165], 'practitioners': [168], 'new': [169, 210], 'field': [172], 'advanced': [177], 'readers': [178], 'solid': [181], 'understanding': [182], 'main': [185], 'approaches': [186, 225], 'developed': [189], 'over': [190], 'past': [192], 'two': [193], 'decades,': [194], 'emphasis': [197], 'most': [200, 242], 'prominent': [201], 'currently': [203], 'relevant': [204], 'work.': [205], 'Furthermore,': [206], 'propose': [208], 'taxonomy': [211], 'classification': [214], 'algorithms,': [215], 'which': [216], 'sheds': [217], 'light': [218], 'different': [221], 'conceptual': [222], 'methodological': [224], 'for': [226], 'incorporating': [227], 'into': [230], 'training': [232], 'process.': [233], 'Lastly,': [234], 'show': [236], 'how': [237, 253], 'fundamental': [239], 'assumptions': [240], 'underlying': [241], 'are': [246], 'closely': [247], 'connected': [248], 'each': [250], 'other,': [251], 'they': [254], 'relate': [255], 'well-known': [258], 'clustering': [260], 'assumption.': [261]}",2019,"['Artificial intelligence', 'Machine learning', 'Computer science', 'Supervised learning', 'Unsupervised learning', 'Scope (computer science)', 'Semi-supervised learning', 'Artificial neural network', 'Field (mathematics)', 'Data science', 'Mathematics', 'Pure mathematics', 'Programming language']","Abstract Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption."
https://openalex.org/W2964159205,Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning,"{'We': [0], 'propose': [1], 'a': [2, 11, 133], 'new': [3, 12], 'regularization': [4], 'method': [5, 49, 80], 'based': [6, 139], 'on': [7, 128, 140, 154], 'virtual': [8, 81, 100], 'adversarial': [9, 25, 46, 52, 82, 101], 'loss:': [10], 'measure': [13], 'of': [14, 17, 32, 88, 99, 112, 136], 'local': [15, 43], 'smoothness': [16], 'the': [18, 30, 33, 51, 65, 71, 96, 137, 141], 'conditional': [19, 34], 'label': [20, 35, 55], 'distribution': [21, 36], 'given': [22], 'input.': [23], 'Virtual': [24], 'loss': [26, 102], 'is': [27, 58, 90], 'defined': [28], 'as': [29], 'robustness': [31], 'around': [37], 'each': [38], 'input': [39], 'data': [40], 'point': [41], 'against': [42], 'perturbation.': [44], 'Unlike': [45], 'training,': [47], 'our': [48, 79, 117, 145], 'defines': [50], 'direction': [53], 'without': [54], 'information': [56], 'and': [57, 114, 124, 156], 'hence': [59], 'applicable': [60], 'to': [61, 122], 'semi-supervised': [62, 125, 151], 'learning.': [63], 'Because': [64], 'directions': [66], 'in': [67], 'which': [68], 'we': [69, 77, 119], 'smooth': [70], 'model': [72], 'are': [73], 'only': [74], '""virtually""': [75], 'adversarial,': [76], 'call': [78], 'training': [83], '(VAT).': [84], 'The': [85], 'computational': [86], 'cost': [87], 'VAT': [89, 121, 146], 'relatively': [91], 'low.': [92], 'For': [93], 'neural': [94], 'networks,': [95], 'approximated': [97], 'gradient': [98], 'can': [103], 'be': [104], 'computed': [105], 'with': [106], 'no': [107], 'more': [108], 'than': [109], 'two': [110], 'pairs': [111], 'forward-': [113], 'back-propagations.': [115], 'In': [116], 'experiments,': [118], 'applied': [120], 'supervised': [123], 'learning': [126, 152], 'tasks': [127, 153], 'multiple': [129], 'benchmark': [130], 'datasets.': [131], 'With': [132], 'simple': [134], 'enhancement': [135], 'algorithm': [138], 'entropy': [142], 'minimization': [143], 'principle,': [144], 'achieves': [147], 'state-of-the-art': [148], 'performance': [149], 'for': [150], 'SVHN': [155], 'CIFAR-10.': [157]}",2018,"['Adversarial system', 'Computer science', 'Artificial intelligence', 'Machine learning', 'Regularization (linguistics)', 'Entropy (arrow of time)', 'Minification', 'Supervised learning', 'Semi-supervised learning', 'Cross entropy', 'Artificial neural network', 'Pattern recognition (psychology)', 'Physics', 'Quantum mechanics', 'Programming language']","We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only ""virtually"" adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10."
https://openalex.org/W2015245929,Unsupervised learning of finite mixture models,"{'This': [0], 'paper': [1], 'proposes': [2], 'an': [3, 133], 'unsupervised': [4], 'algorithm': [5], 'for': [6, 59, 126, 150], 'learning': [7], 'a': [8, 67, 88, 96, 111], 'finite': [9], 'mixture': [10, 60, 124], 'model': [11, 89, 108, 125], 'from': [12], 'multivariate': [13], 'data.': [14], 'The': [15, 50, 77], 'adjective': [16], '""unsupervised""': [17], 'is': [18, 28, 82, 129], 'justified': [19], 'by': [20], 'two': [21], 'properties': [22], 'of': [23, 30, 34, 57, 64, 73, 79, 98, 122, 154], 'the': [24, 32, 39, 62, 71, 74, 151], 'algorithm:': [25], '1)': [26], 'it': [27, 44, 128, 141], 'capable': [29], 'selecting': [31], 'number': [33], 'components': [35], 'and': [36, 107], '2)': [37], 'unlike': [38], 'standard': [40], 'expectation-maximization': [41], '(EM)': [42], 'algorithm,': [43], 'does': [45], 'not': [46, 86], 'require': [47], 'careful': [48], 'initialization.': [49], 'proposed': [51], 'method': [52], 'also': [53], 'avoids': [54], 'another': [55], 'drawback': [56], 'EM': [58, 134], 'fitting:': [61], 'possibility': [63], 'convergence': [65], 'toward': [66], 'singular': [68], 'estimate': [69], 'at': [70], 'boundary': [72], 'parameter': [75], 'space.': [76], 'novelty': [78], 'our': [80, 155], 'approach': [81], 'that': [83], 'we': [84, 103, 139], 'do': [85], 'use': [87], 'selection': [90, 109], 'criterion': [91], 'to': [92, 119, 131], 'choose': [93], 'one': [94], 'among': [95], 'set': [97], 'preestimated': [99], 'candidate': [100], 'models;': [101], 'instead,': [102], 'seamlessly': [104], 'integrate': [105], 'estimation': [106], 'in': [110, 136], 'single': [112], 'algorithm.': [113], 'Our': [114], 'technique': [115], 'can': [116], 'be': [117], 'applied': [118], 'any': [120], 'type': [121], 'parametric': [123], 'which': [127], 'possible': [130], 'write': [132], 'algorithm;': [135], 'this': [137], 'paper,': [138], 'illustrate': [140], 'with': [142], 'experiments': [143, 148], 'involving': [144], 'Gaussian': [145], 'mixtures.': [146], 'These': [147], 'testify': [149], 'good': [152], 'performance': [153], 'approach.': [156]}",2002,"['Mixture model', 'Expectation–maximization algorithm', 'Initialization', 'Computer science', 'Model selection', 'Unsupervised learning', 'Artificial intelligence', 'Convergence (economics)', 'Gaussian process', 'Parametric statistics', 'Algorithm', 'Pattern recognition (psychology)', 'Gaussian', 'Mathematics', 'Maximum likelihood', 'Statistics', 'Physics', 'Quantum mechanics', 'Economics', 'Programming language', 'Economic growth']","This paper proposes an unsupervised algorithm for learning a finite mixture model from multivariate data. The adjective ""unsupervised"" is justified by two properties of the algorithm: 1) it is capable of selecting the number of components and 2) unlike the standard expectation-maximization (EM) algorithm, it does not require careful initialization. The proposed method also avoids another drawback of EM for mixture fitting: the possibility of convergence toward a singular estimate at the boundary of the parameter space. The novelty of our approach is that we do not use a model selection criterion to choose one among a set of preestimated candidate models; instead, we seamlessly integrate estimation and model selection in a single algorithm. Our technique can be applied to any type of parametric mixture model for which it is possible to write an EM algorithm; in this paper, we illustrate it with experiments involving Gaussian mixtures. These experiments testify for the good performance of our approach."
https://openalex.org/W2321533354,Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles,,2016,"['Pascal (unit)', 'Jigsaw', 'Computer science', 'Artificial intelligence', 'Convolutional neural network', 'Transfer of learning', 'Unsupervised learning', 'Annotation', 'Pattern recognition (psychology)', 'Machine learning', 'Context (archaeology)', 'Artificial neural network', 'Object detection', 'Set (abstract data type)', 'Psychology', 'Paleontology', 'Pedagogy', 'Biology', 'Programming language']",
https://openalex.org/W4240035482,Unsupervised Learning,"{'What': [0], 'use': [1], 'can': [2, 97], 'the': [3, 7, 26, 39, 50, 61, 84, 88, 150, 181, 186], 'brain': [4], 'make': [5], 'of': [6, 10, 28, 31, 45, 63, 80, 94, 102, 107, 129, 149, 161, 180], 'massive': [8], 'flow': [9], 'sensory': [11, 64, 81, 108], 'information': [12], 'that': [13, 52, 76], 'occurs': [14], 'without': [15], 'any': [16], 'associated': [17], 'rewards': [18], 'or': [19, 90], 'punishments?': [20], 'This': [21], 'question': [22], 'is': [23, 54, 74, 157], 'reviewed': [24], 'in': [25, 87, 126], 'light': [27], 'connectionist': [29], 'models': [30, 44], 'unsupervised': [32], 'learning': [33], 'and': [34, 42, 47, 49, 68, 105, 110, 164], 'some': [35], 'older': [36], 'ideas,': [37], 'namely': [38], 'cognitive': [40], 'maps': [41, 89], 'working': [43], 'Tolman': [46], 'Craik,': [48], 'idea': [51], 'redundancy': [53, 79], 'important': [55], 'for': [56], 'understanding': [57], 'perception': [58], '(Attneave': [59], '1954),': [60], 'physiology': [62], 'pathways': [65], '(Barlow': [66], '1959),': [67], 'pattern': [69], 'recognition': [70], '(Watanabe': [71], '1960).': [72], 'It': [73], 'argued': [75], '(1)': [77], 'The': [78], 'messages': [82, 136], 'provides': [83], 'knowledge': [85, 96, 122], 'incorporated': [86, 125, 152], 'models.': [91], '(2)': [92], 'Some': [93], 'this': [95], 'be': [98, 124, 144], 'obtained': [99], 'by': [100, 113], 'observations': [101], 'mean,': [103], 'variance,': [104], 'covariance': [106], 'messages,': [109], 'perhaps': [111], 'also': [112], 'a': [114, 127, 155, 158, 165], 'method': [115], 'called': [116], '“minimum': [117], 'entropy': [118], 'coding.”': [119], '(3)': [120], 'Such': [121], 'may': [123], 'model': [128], '“what': [130], 'usually': [131], 'happens”': [132], 'with': [133, 177, 185], 'which': [134], 'incoming': [135], 'are': [137, 169], 'automatically': [138], 'compared,': [139], 'enabling': [140], 'unexpected': [141], 'discrepancies': [142], 'to': [143, 174], 'immediately': [145], 'identified.': [146], '(4)': [147], 'Knowledge': [148], 'sort': [151], 'into': [153], 'such': [154], 'filter': [156], 'necessary': [159], 'prerequisite': [160], 'ordinary': [162], 'learning,': [163], 'representation': [166], 'whose': [167], 'elements': [168, 187], 'independent': [170], 'makes': [171], 'it': [172], 'possible': [173], 'form': [175], 'associations': [176], 'logical': [178], 'functions': [179], 'elements,': [182], 'not': [183], 'just': [184], 'themselves.': [188]}",1989,"['Connectionism', 'Computer science', 'sort', 'Perception', 'Sensory system', 'Artificial intelligence', 'Unsupervised learning', 'Coding (social sciences)', 'Cognition', 'Machine learning', 'Artificial neural network', 'Entropy (arrow of time)', 'Cognitive science', 'Psychology', 'Cognitive psychology', 'Mathematics', 'Neuroscience', 'Statistics', 'Physics', 'Quantum mechanics', 'Information retrieval']","What use can the brain make of the massive flow of sensory information that occurs without any associated rewards or punishments? This question is reviewed in the light of connectionist models of unsupervised learning and some older ideas, namely the cognitive maps and working models of Tolman and Craik, and the idea that redundancy is important for understanding perception (Attneave 1954), the physiology of sensory pathways (Barlow 1959), and pattern recognition (Watanabe 1960). It is argued that (1) The redundancy of sensory messages provides the knowledge incorporated in the maps or models. (2) Some of this knowledge can be obtained by observations of mean, variance, and covariance of sensory messages, and perhaps also by a method called “minimum entropy coding.” (3) Such knowledge may be incorporated in a model of “what usually happens” with which incoming messages are automatically compared, enabling unexpected discrepancies to be immediately identified. (4) Knowledge of the sort incorporated into such a filter is a necessary prerequisite of ordinary learning, and a representation whose elements are independent makes it possible to form associations with logical functions of the elements, not just with the elements themselves."
https://openalex.org/W2912889105,Unsupervised learning,,1999,"['Computer science', 'Psychology', 'Artificial intelligence']",
https://openalex.org/W2609883120,Unsupervised Learning of Depth and Ego-Motion from Video,"{'We': [0], 'present': [1], 'an': [2, 30], 'unsupervised': [3], 'learning': [4, 32], 'framework': [5], 'for': [6, 57, 130], 'the': [7, 38, 44, 77, 80, 91, 106, 110], 'task': [8], 'of': [9, 112], 'monocular': [10, 54, 116], 'depth': [11, 63, 82, 117, 129], 'and': [12, 64, 83, 132], 'camera': [13], 'motion': [14], 'estimation': [15, 135], 'from': [16], 'unstructured': [17], 'video': [18, 55], 'sequences.': [19], 'In': [20, 41], 'common': [21], 'with': [22, 34, 68, 120], 'recent': [23], 'work': [24], '[10,': [25], '14,': [26], '16],': [27], 'we': [28], 'use': [29, 124], 'end-to-end': [31], 'approach': [33], 'view': [35], 'synthesis': [36], 'as': [37], 'supervisory': [39], 'signal.': [40], 'contrast': [42], 'to': [43, 76, 139], 'previous': [45], 'work,': [46], 'our': [47, 113], 'method': [48, 60], 'is': [49], 'completely': [50], 'unsupervised,': [51], 'requiring': [52], 'only': [53], 'sequences': [56], 'training.': [58], 'Our': [59], 'uses': [61], 'single-view': [62], 'multiview': [65], 'pose': [66, 127, 134], 'networks,': [67], 'a': [69], 'loss': [70, 92], 'based': [71], 'on': [72, 105], 'warping': [73], 'nearby': [74], 'views': [75], 'target': [78], 'using': [79], 'computed': [81], 'pose.': [84], 'The': [85], 'networks': [86], 'are': [87], 'thus': [88], 'coupled': [89], 'by': [90], 'during': [93], 'training,': [94, 131], 'but': [95], 'can': [96], 'be': [97], 'applied': [98], 'independently': [99], 'at': [100], 'test': [101], 'time.': [102], 'Empirical': [103], 'evaluation': [104], 'KITTI': [107], 'dataset': [108], 'demonstrates': [109], 'effectiveness': [111], 'approach:': [114], '1)': [115], 'performs': [118, 136], 'comparably': [119], 'supervised': [121], 'methods': [122], 'that': [123], 'either': [125], 'ground-truth': [126], 'or': [128], '2)': [133], 'favorably': [137], 'compared': [138], 'established': [140], 'SLAM': [141], 'systems': [142], 'under': [143], 'comparable': [144], 'input': [145], 'settings.': [146]}",2017,"['Artificial intelligence', 'Computer science', 'Monocular', 'Image warping', 'Computer vision', 'Ground truth', 'Dynamic time warping', 'Unsupervised learning', 'Motion (physics)', 'Pose', 'Supervised learning', 'Task (project management)', 'Artificial neural network', 'Engineering', 'Systems engineering']","We present an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences. In common with recent work [10, 14, 16], we use an end-to-end learning approach with view synthesis as the supervisory signal. In contrast to the previous work, our method is completely unsupervised, requiring only monocular video sequences for training. Our method uses single-view depth and multiview pose networks, with a loss based on warping nearby views to the target using the computed depth and pose. The networks are thus coupled by the loss during training, but can be applied independently at test time. Empirical evaluation on the KITTI dataset demonstrates the effectiveness of our approach: 1) monocular depth performs comparably with supervised methods that use either ground-truth pose or depth for training, and 2) pose estimation performs favorably compared to established SLAM systems under comparable input settings."
https://openalex.org/W2130325614,Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,"{'There': [0], 'has': [1], 'been': [2], 'much': [3], 'interest': [4], 'in': [5, 78], 'unsupervised': [6], 'learning': [7], 'of': [8, 75, 101], 'hierarchical': [9, 40, 122], 'generative': [10, 41], 'models': [11, 19], 'such': [12, 18, 94], 'as': [13, 95], 'deep': [14, 36], 'belief': [15, 37], 'networks.': [16], 'Scaling': [17], 'to': [20, 45, 62], 'full-sized,': [21], 'high-dimensional': [22], 'images': [23, 100], 'remains': [24], 'a': [25, 39, 68, 79], 'difficult': [26], 'problem.': [27], 'To': [28], 'address': [29], 'this': [30], 'problem,': [31], 'we': [32], 'present': [33], 'the': [34, 73, 87], 'convolutional': [35], 'network,': [38], 'model': [42, 50, 119], 'which': [43, 71], 'scales': [44], 'realistic': [46], 'image': [47], 'sizes.': [48], 'This': [49], 'is': [51, 65], 'translation-invariant': [52], 'and': [53, 57, 103, 115, 124], 'supports': [54], 'efficient': [55], 'bottom-up': [56], 'top-down': [58], 'probabilistic': [59, 66], 'inference.': [60], 'Key': [61], 'our': [63, 118], 'approach': [64], 'max-pooling,': [67], 'novel': [69], 'technique': [70], 'shrinks': [72], 'representations': [74], 'higher': [76], 'layers': [77], 'probabilistically': [80], 'sound': [81], 'way.': [82], 'Our': [83], 'experiments': [84], 'show': [85, 116], 'that': [86, 117], 'algorithm': [88], 'learns': [89], 'useful': [90], 'high-level': [91], 'visual': [92, 112], 'features,': [93], 'object': [96], 'parts,': [97], 'from': [98], 'unlabeled': [99], 'objects': [102], 'natural': [104], 'scenes.': [105], 'We': [106], 'demonstrate': [107], 'excellent': [108], 'performance': [109], 'on': [110], 'several': [111], 'recognition': [113], 'tasks': [114], 'can': [120], 'perform': [121], '(bottom-up': [123], 'top-down)': [125], 'inference': [126], 'over': [127], 'full-sized': [128], 'images.': [129]}",2009,"['Computer science', 'Artificial intelligence', 'Pooling', 'Inference', 'Deep belief network', 'Generative model', 'Deep learning', 'Probabilistic logic', 'Convolutional neural network', 'Pattern recognition (psychology)', 'Scalability', 'Unsupervised learning', 'Machine learning', 'Generative grammar', 'Database']","There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images."
https://openalex.org/W2131329059,Optimal unsupervised learning in a single-layer linear feedforward neural network,,1989,"['Computer science', 'Unsupervised learning', 'Artificial neural network', 'Feedforward neural network', 'Hebbian theory', 'Competitive learning', 'Artificial intelligence', 'Backpropagation', 'Probabilistic neural network', 'Wake-sleep algorithm', 'Pattern recognition (psychology)', 'Types of artificial neural networks', 'Feed forward', 'Time delay neural network', 'Learning vector quantization', 'Supervised learning', 'Algorithm', 'Control engineering', 'Engineering', 'Generalization error']",
https://openalex.org/W2883725317,Deep Clustering for Unsupervised Learning of Visual Features,,2018,"['Computer science', 'Cluster analysis', 'Margin (machine learning)', 'Artificial intelligence', 'Unsupervised learning', 'Pattern recognition (psychology)', 'Convolutional neural network', 'Artificial neural network', 'Machine learning', 'Class (philosophy)']",
https://openalex.org/W2116435618,Unsupervised Learning of Video Representations using LSTMs,"{'We': [0, 55, 81, 98, 125, 148], 'use': [1], 'Long': [2], 'Short': [3], 'Term': [4], 'Memory': [5], '(LSTM)': [6], 'networks': [7], 'to': [8, 20, 40, 106], 'learn': [9], 'representations': [10, 70, 129, 152], 'of': [11, 60, 65, 72, 102, 173], 'video': [12, 73, 116], 'sequences.': [13], 'Our': [14], 'model': [15, 104, 111], 'uses': [16], 'an': [17, 22], 'encoder': [18], 'LSTM': [19], 'map': [21], 'input': [23, 48, 61], 'sequence': [24], 'into': [25, 118, 122], 'a': [26, 77, 134], 'fixed': [27], 'length': [28], 'representation.': [29], 'This': [30], 'representation': [31, 117], 'is': [32], 'decoded': [33], 'using': [34, 76], 'single': [35], 'or': [36, 50], 'multiple': [37], 'decoder': [38, 90], 'LSTMs': [39, 91], 'perform': [41], 'different': [42, 83], 'tasks,': [43], 'such': [44, 86], 'as': [45, 87], 'reconstructing': [46], 'the': [47, 52, 89, 95, 100, 103, 110, 114, 119, 123, 128, 143, 151], 'sequence,': [49], 'predicting': [51], 'future': [53, 120], 'sequence.': [54], 'experiment': [56], 'with': [57], 'two': [58], 'kinds': [59], 'sequences': [62], '-': [63, 138], 'patches': [64], 'image': [66], 'pixels': [67], 'and': [68, 121, 145], 'high-level': [69], '(percepts)': [71], 'frames': [74], 'extracted': [75], 'pretrained': [78, 167], 'convolutional': [79], 'net.': [80], 'explore': [82], 'design': [84], 'choices': [85], 'whether': [88], 'should': [92], 'condition': [93], 'on': [94, 142, 168], 'generated': [96], 'output.': [97], 'analyze': [99], 'outputs': [101], 'qualitatively': [105], 'see': [107], 'how': [108], 'well': [109], 'can': [112, 176], 'extrapolate': [113], 'learned': [115], 'past.': [124], 'further': [126], 'evaluate': [127], 'by': [130], 'finetuning': [131], 'them': [132], 'for': [133], 'supervised': [135], 'learning': [136], 'problem': [137], 'human': [139], 'action': [140, 178], 'recognition': [141, 179], 'UCF-101': [144], 'HMDB-51': [146], 'datasets.': [147], 'show': [149], 'that': [150], 'help': [153, 177], 'improve': [154], 'classification': [155], 'accuracy,': [156], 'especially': [157], 'when': [158], 'there': [159], 'are': [160], 'only': [161], 'few': [162], 'training': [163], 'examples.': [164], 'Even': [165], 'models': [166], 'unrelated': [169], 'datasets': [170], '(300': [171], 'hours': [172], 'YouTube': [174], 'videos)': [175], 'performance.': [180]}",2015,"['Computer science', 'Encoder', 'Artificial intelligence', 'Representation (politics)', 'Sequence (biology)', 'Pattern recognition (psychology)', 'Feature learning', 'Convolutional neural network', 'Deep learning', 'Sequence labeling', 'Machine learning', 'Task (project management)', 'Management', 'Politics', 'Biology', 'Political science', 'Law', 'Economics', 'Genetics', 'Operating system']","We use Long Short Term Memory (LSTM) networks to learn representations of video sequences. Our model uses an encoder LSTM to map an input sequence into a fixed length representation. This representation is decoded using single or multiple decoder LSTMs to perform different tasks, such as reconstructing the input sequence, or predicting the future sequence. We experiment with two kinds of input sequences - patches of image pixels and high-level representations (percepts) of video frames extracted using a pretrained convolutional net. We explore different design choices such as whether the decoder LSTMs should condition on the generated output. We analyze the outputs of the model qualitatively to see how well the model can extrapolate the learned video representation into the future and into the past. We further evaluate the representations by finetuning them for a supervised learning problem - human action recognition on the UCF-101 and HMDB-51 datasets. We show that the representations help improve classification accuracy, especially when there are only few training examples. Even models pretrained on unrelated datasets (300 hours of YouTube videos) can help action recognition performance."
https://openalex.org/W2146444479,Slow Feature Analysis: Unsupervised Learning of Invariances,"{'Invariant': [0], 'features': [1, 26], 'of': [2, 39, 45, 68, 79, 88, 132, 138, 147], 'temporally': [3], 'varying': [4, 25], 'signals': [5, 99], 'are': [6, 83, 127], 'useful': [7], 'for': [8, 20, 168, 196], 'analysis': [9, 14, 48], 'and': [10, 43, 53, 71, 100, 120], 'classification.': [11], 'Slow': [12], 'feature': [13], '(SFA)': [15], 'is': [16, 33, 58, 105, 141, 194, 204], 'a': [17, 28, 36, 66, 76, 135, 144, 163, 179], 'new': [18, 189], 'method': [19], 'learning': [21], 'invariant': [22], 'or': [23], 'slowly': [24], 'from': [27], 'vectorial': [29], 'input': [30, 41, 98], 'signal.': [31], 'It': [32, 57], 'based': [34, 113], 'on': [35, 114, 172], 'nonlinear': [37], 'expansion': [38], 'the': [40, 62, 148, 174, 202], 'signal': [42, 52], 'application': [44, 131], 'principal': [46], 'component': [47], 'to': [49, 60, 74, 95, 108, 162, 184, 188, 206], 'this': [50], 'expanded': [51], 'its': [54], 'time': [55], 'derivative.': [56], 'guaranteed': [59], 'find': [61], 'optimal': [63], 'solution': [64], 'within': [65], 'family': [67], 'functions': [69, 126], 'directly': [70], 'can': [72, 91, 155], 'learn': [73, 156, 207], 'extract': [75, 101], 'large': [77], 'number': [78], 'decorrelated': [80], 'features,': [81], 'which': [82], 'ordered': [84], 'by': [85, 129], 'their': [86], 'degree': [87], 'invariance.': [89], 'SFA': [90, 104, 139], 'be': [92], 'applied': [93, 106], 'hierarchically': [94], 'process': [96], 'high-dimensional': [97], 'complex': [102, 109], 'features.': [103], 'first': [107], 'cell': [110, 116], 'tuning': [111], 'properties': [112], 'simple': [115, 145], 'output,': [117], 'including': [118], 'disparity': [119], 'motion.': [121], 'Then': [122], 'more': [123], 'complicated': [124], 'input-output': [125], 'learned': [128], 'repeated': [130], 'SFA.': [133], 'Finally,': [134], 'hierarchical': [136], 'network': [137, 154, 203], 'modules': [140], 'presented': [142], 'as': [143], 'model': [146], 'visual': [149], 'system.': [150], 'The': [151, 191], 'same': [152], 'unstructured': [153], 'translation,': [157], 'size,': [158], 'rotation,': [159], 'contrast,': [160], 'or,': [161], 'lesser': [164], 'degree,': [165], 'illumination': [166], 'invariance': [167], 'one-dimensional': [169], 'objects,': [170], 'depending': [171], 'only': [173, 178], 'training': [175, 181], 'stimulus.': [176], 'Surprisingly,': [177], 'few': [180], 'objects': [182], 'suffice': [183], 'achieve': [185], 'good': [186], 'generalization': [187], 'objects.': [190], 'generated': [192], 'representation': [193], 'suitable': [195], 'object': [197], 'recognition.': [198], 'Performance': [199], 'degrades': [200], 'if': [201], 'trained': [205], 'multiple': [208], 'invariances': [209], 'simultaneously.': [210]}",2002,"['Pattern recognition (psychology)', 'Invariant (physics)', 'Artificial intelligence', 'Generalization', 'Computer science', 'Representation (politics)', 'Artificial neural network', 'Principal component analysis', 'Feature (linguistics)', 'Simple cell', 'Mathematics', 'Algorithm', 'Mathematical analysis', 'Linguistics', 'Philosophy', 'Politics', 'Political science', 'Law', 'Mathematical physics', 'Receptive field']","Invariant features of temporally varying signals are useful for analysis and classification. Slow feature analysis (SFA) is a new method for learning invariant or slowly varying features from a vectorial input signal. It is based on a nonlinear expansion of the input signal and application of principal component analysis to this expanded signal and its time derivative. It is guaranteed to find the optimal solution within a family of functions directly and can learn to extract a large number of decorrelated features, which are ordered by their degree of invariance. SFA can be applied hierarchically to process high-dimensional input signals and extract complex features. SFA is applied first to complex cell tuning properties based on simple cell output, including disparity and motion. Then more complicated input-output functions are learned by repeated application of SFA. Finally, a hierarchical network of SFA modules is presented as a simple model of the visual system. The same unstructured network can learn translation, size, rotation, contrast, or, to a lesser degree, illumination invariance for one-dimensional objects, depending on only the training stimulus. Surprisingly, only a few training objects suffice to achieve good generalization to new objects. The generated representation is suitable for object recognition. Performance degrades if the network is trained to learn multiple invariances simultaneously."
https://openalex.org/W4385452929,From ChatGPT to ThreatGPT: Impact of Generative AI in Cybersecurity and Privacy,"{'Undoubtedly,': [0], 'the': [1, 10, 16, 20, 51, 58, 68, 81, 91, 100, 116, 120, 137, 148, 157, 224, 234, 252], 'evolution': [2], 'of': [3, 12, 53, 63, 88, 93, 102, 230], 'Generative': [4], 'AI': [5], '(GenAI)': [6], 'models': [7, 23], 'has': [8], 'been': [9], 'highlight': [11], 'digital': [13], 'transformation': [14], 'in': [15, 56, 90, 151], 'year': [17], '2022.': [18], 'As': [19], 'different': [21], 'GenAI': [22, 54, 89, 149, 191, 245], 'like': [24, 128], 'ChatGPT': [25, 160], 'and': [26, 34, 60, 65, 71, 86, 95, 132, 155, 180, 189, 207, 217, 227, 239, 249], 'Google': [27], 'Bard': [28], 'continue': [29], 'to': [30, 38, 111, 166, 193, 242], 'foster': [31], 'their': [32], 'complexity': [33], 'capability,': [35], ""it's"": [36], 'critical': [37], 'understand': [39], 'its': [40, 255], 'consequences': [41], 'from': [42], 'a': [43], 'cybersecurity': [44, 94, 256], 'perspective.': [45], 'Several': [46], 'instances': [47], 'recently': [48], 'have': [49], 'demonstrated': [50], 'use': [52, 147], 'tools': [55, 150, 192], 'both': [57], 'defensive': [59], 'offensive': [61], 'side': [62], 'cybersecurity,': [64], 'focusing': [66], 'on': [67, 119, 136], 'social,': [69, 225], 'ethical': [70, 117, 212, 228, 250], 'privacy': [72], 'implications': [73, 229], 'this': [74, 244], 'technology': [75], 'possesses.': [76], 'This': [77, 122, 183], 'research': [78], 'paper': [79, 123, 140, 184, 235], 'highlights': [80, 236], 'limitations,': [82], 'challenges,': [83], 'potential': [84], 'risks,': [85], 'opportunities': [87], 'domain': [92], 'privacy.': [96], 'The': [97, 139], 'work': [98], 'presents': [99], 'vulnerabilities': [101], 'ChatGPT,': [103], 'which': [104], 'can': [105, 146, 161], 'be': [106, 162], 'exploited': [107], 'by': [108, 164], 'malicious': [109, 113], 'users': [110], 'exfiltrate': [112], 'information': [114], 'bypassing': [115], 'constraints': [118], 'model.': [121], 'demonstrates': [124], 'successful': [125], 'example': [126], 'attacks': [127, 135], 'Jailbreaks,': [129], 'reverse': [130], 'psychology,': [131], 'prompt': [133], 'injection': [134], 'ChatGPT.': [138, 231], 'also': [141, 222], 'investigates': [142], 'how': [143], 'cyber': [144, 153, 198], 'offenders': [145], 'developing': [152, 211], 'attacks,': [154, 170, 172], 'explore': [156], 'scenarios': [158], 'where': [159], 'used': [163], 'adversaries': [165], 'create': [167], 'social': [168], 'engineering': [169], 'phishing': [171], 'automated': [173], 'hacking,': [174], 'attack': [175, 209], 'payload': [176], 'generation,': [177], 'malware': [178, 218], 'creation,': [179], 'polymorphic': [181], 'malware.': [182], 'then': [185], 'examines': [186], 'defense': [187, 199], 'techniques': [188], 'uses': [190], 'improve': [194], 'security': [195], 'measures,': [196], 'including': [197], 'automation,': [200], 'reporting,': [201], 'threat': [202], 'intelligence,': [203], 'secure': [204], 'code': [205], 'generation': [206], 'detection,': [208], 'identification,': [210], 'guidelines,': [213], 'incidence': [214], 'response': [215], 'plans,': [216], 'detection.': [219], 'We': [220], 'will': [221], 'discuss': [223], 'legal,': [226], 'In': [232], 'conclusion,': [233], 'open': [237], 'challenges': [238], 'future': [240], 'directions': [241], 'make': [243], 'secure,': [246], 'safe,': [247], 'trustworthy,': [248], 'as': [251], 'community': [253], 'understands': [254], 'impacts.': [257]}",2023,"['Computer science', 'Computer security', 'Information privacy', 'Generative grammar', 'Internet privacy', 'Artificial intelligence']","Undoubtedly, the evolution of Generative AI (GenAI) models has been the highlight of digital transformation in the year 2022. As the different GenAI models like ChatGPT and Google Bard continue to foster their complexity and capability, it's critical to understand its consequences from a cybersecurity perspective. Several instances recently have demonstrated the use of GenAI tools in both the defensive and offensive side of cybersecurity, and focusing on the social, ethical and privacy implications this technology possesses. This research paper highlights the limitations, challenges, potential risks, and opportunities of GenAI in the domain of cybersecurity and privacy. The work presents the vulnerabilities of ChatGPT, which can be exploited by malicious users to exfiltrate malicious information bypassing the ethical constraints on the model. This paper demonstrates successful example attacks like Jailbreaks, reverse psychology, and prompt injection attacks on the ChatGPT. The paper also investigates how cyber offenders can use the GenAI tools in developing cyber attacks, and explore the scenarios where ChatGPT can be used by adversaries to create social engineering attacks, phishing attacks, automated hacking, attack payload generation, malware creation, and polymorphic malware. This paper then examines defense techniques and uses GenAI tools to improve security measures, including cyber defense automation, reporting, threat intelligence, secure code generation and detection, attack identification, developing ethical guidelines, incidence response plans, and malware detection. We will also discuss the social, legal, and ethical implications of ChatGPT. In conclusion, the paper highlights open challenges and future directions to make this GenAI secure, safe, trustworthy, and ethical as the community understands its cybersecurity impacts."
https://openalex.org/W2810546090,Ethics and Privacy in AI and Big Data: Implementing Responsible Research and Innovation,"{'Emerging': [0], 'combinations': [1], 'of': [2, 20, 50, 73, 117], 'artificial': [3], 'intelligence,': [4], 'big': [5], 'data,': [6], 'and': [7, 16, 25, 47, 76, 98], 'the': [8, 21, 71, 81, 92, 106, 114], 'applications': [9], 'these': [10, 45, 62, 118], 'enable': [11], 'are': [12, 94], 'receiving': [13], 'significant': [14], 'media': [15], 'policy': [17], 'attention.': [18], 'Much': [19], 'attention': [22], 'concerns': [23], 'privacy': [24], 'other': [26], 'ethical': [27], 'issues.': [28], 'In': [29], 'our': [30, 103], 'article,': [31], 'we': [32], 'suggest': [33, 69], 'that': [34, 53, 61, 70, 91], 'what': [35], 'is': [36, 39], 'needed': [37], 'now': [38], 'a': [40, 87], 'way': [41], 'to': [42, 59, 84, 89, 120], 'comprehensively': [43], 'understand': [44], 'issues': [46], 'find': [48], 'mechanisms': [49], 'addressing': [51], 'them': [52], 'involve': [54], 'stakeholders,': [55], 'including': [56], 'civil': [57], 'society,': [58], 'ensure': [60], ""technologies'"": [63], 'benefits': [64], 'outweigh': [65], 'their': [66], 'disadvantages.': [67], 'We': [68, 100], 'concept': [72], 'responsible': [74], 'research': [75], 'innovation': [77], '(RRI)': [78], 'can': [79, 124], 'provide': [80], 'framing': [82], 'required': [83], 'act': [85], 'with': [86], 'view': [88], 'ensuring': [90], 'technologies': [93], 'socially': [95], 'acceptable,': [96], 'desirable,': [97], 'sustainable.': [99], 'draw': [101], 'from': [102], 'work': [104], 'on': [105], 'Human': [107], 'Brain': [108], 'Project,': [109], 'one': [110], 'potential': [111], 'driver': [112], 'for': [113], 'next': [115], 'generation': [116], 'technologies,': [119], 'discuss': [121], 'how': [122], 'RRI': [123], 'be': [125], 'put': [126], 'in': [127], 'practice.': [128]}",2018,"['Responsible Research and Innovation', 'Framing (construction)', 'Big data', 'Ethical issues', 'Emerging technologies', 'Engineering ethics', 'Business', 'Knowledge management', 'Internet privacy', 'Computer science', 'Public relations', 'Political science', 'Engineering', 'Structural engineering', 'Artificial intelligence', 'Operating system']","Emerging combinations of artificial intelligence, big data, and the applications these enable are receiving significant media and policy attention. Much of the attention concerns privacy and other ethical issues. In our article, we suggest that what is needed now is a way to comprehensively understand these issues and find mechanisms of addressing them that involve stakeholders, including civil society, to ensure that these technologies' benefits outweigh their disadvantages. We suggest that the concept of responsible research and innovation (RRI) can provide the framing required to act with a view to ensuring that the technologies are socially acceptable, desirable, and sustainable. We draw from our work on the Human Brain Project, one potential driver for the next generation of these technologies, to discuss how RRI can be put in practice."
https://openalex.org/W4396833271,"Deepfakes, Phrenology, Surveillance, and More! A Taxonomy of AI Privacy Risks","{'Privacy': [0], 'is': [1, 100], 'a': [2, 25, 106, 129], 'key': [3], 'principle': [4], 'for': [5], 'developing': [6], 'ethical': [7], 'AI': [8, 14, 28, 35, 47, 75, 103], 'technologies,': [9], 'but': [10], 'how': [11, 40], 'does': [12], 'including': [13], 'technologies': [15, 48, 76, 104], 'in': [16, 50], 'products': [17], 'and': [18, 44, 139], 'services': [19], 'change': [20], 'privacy': [21, 29, 36, 55, 72, 111, 133], 'risks?': [22], 'We': [23, 38, 68], 'constructed': [24], 'taxonomy': [26], 'of': [27, 46, 97, 131, 142], 'risks': [30, 73, 82, 90, 112, 134], 'by': [31], 'analyzing': [32], '321': [33], 'documented': [34], 'incidents.': [37], 'codified': [39], 'the': [41, 66, 110, 132, 137], 'unique': [42], 'capabilities': [43, 138], 'requirements': [45, 141], 'described': [49], 'those': [51], 'incidents': [52], 'generated': [53], 'new': [54], 'risks,': [56], 'exacerbated': [57, 87], 'known': [58], 'ones,': [59], 'or': [60, 86], 'otherwise': [61], 'did': [62], 'not': [63], 'meaningfully': [64], 'alter': [65, 109], 'risk.': [67], 'present': [69], '12': [70], 'high-level': [71], 'that': [74, 101], 'either': [77], 'newly': [78], 'created': [79], '(e.g.,': [80, 88, 121], 'exposure': [81], 'from': [83, 91, 136], 'deepfake': [84], 'pornography)': [85], 'surveillance': [89], 'collecting': [92], 'training': [93], 'data).': [94], 'One': [95], 'upshot': [96], 'our': [98], 'work': [99], 'incorporating': [102], 'into': [105], 'product': [107], 'can': [108], 'it': [113], 'entails.': [114], 'Yet,': [115], 'current': [116], 'approaches': [117], 'to': [118], 'privacy-preserving': [119], 'AI/ML': [120], 'federated': [122], 'learning,': [123], 'differential': [124], 'privacy,': [125], 'checklists)': [126], 'only': [127], 'address': [128], 'subset': [130], 'arising': [135], 'data': [140], 'AI.': [143]}",2024,"['Differential privacy', 'Computer science', 'Internet privacy', 'Information privacy', 'Privacy by Design', 'Privacy software', 'Computer security', 'Privacy protection', 'Data science', 'Data mining']","Privacy is a key principle for developing ethical AI technologies, but how does including AI technologies in products and services change privacy risks? We constructed a taxonomy of AI privacy risks by analyzing 321 documented AI privacy incidents. We codified how the unique capabilities and requirements of AI technologies described in those incidents generated new privacy risks, exacerbated known ones, or otherwise did not meaningfully alter the risk. We present 12 high-level privacy risks that AI technologies either newly created (e.g., exposure risks from deepfake pornography) or exacerbated (e.g., surveillance risks from collecting training data). One upshot of our work is that incorporating AI technologies into a product can alter the privacy risks it entails. Yet, current approaches to privacy-preserving AI/ML (e.g., federated learning, differential privacy, checklists) only address a subset of the privacy risks arising from the capabilities and data requirements of AI."
https://openalex.org/W3103989228,Federated learning for privacy-preserving AI,"{'Engineering': [0], 'and': [1, 8], 'algorithmic': [2], 'framework': [3], 'to': [4], 'ensure': [5], 'data': [6], 'privacy': [7], 'user': [9], 'confidentiality.': [10]}",2020,"['Computer science', 'Confidentiality', 'Information privacy', 'Internet privacy', 'Computer security', 'Privacy software', 'Privacy by Design']",Engineering and algorithmic framework to ensure data privacy and user confidentiality.
https://openalex.org/W3003326204,"Smart Contract Privacy Protection Using AI in Cyber-Physical Systems: Tools, Techniques and Challenges","{'Applications': [0], 'of': [1, 30, 208], 'Blockchain': [2], '(BC)': [3], 'technology': [4], 'and': [5, 15, 43, 50, 77, 99, 147, 176, 188, 197, 216, 222], 'Cyber-Physical': [6], 'Systems': [7], '(CPS)': [8], 'are': [9, 85, 149, 202], 'increasing': [10], 'exponentially.': [11], 'However,': [12, 83], 'framing': [13], 'resilient': [14], 'correct': [16], 'smart': [17, 22], 'contracts': [18], '(SCs)': [19], 'for': [20, 59, 190, 199], 'these': [21], 'application': [23], 'is': [24, 37, 47, 135, 211], 'a': [25, 109, 125, 169, 205], 'quite': [26], 'challenging': [27], 'task': [28], 'because': [29], 'the': [31, 39, 53, 57, 79, 88, 116, 131, 139, 141, 154, 157, 160], 'complexity': [32], 'associated': [33, 80], 'with': [34], 'them.': [35], 'SC': [36, 112, 145, 191, 201, 217], 'modernizing': [38], 'traditional': [40], 'industrial,': [41], 'technical,': [42], 'business': [44], 'processes.': [45], 'It': [46, 72], 'self-executable,': [48], 'self-verifiable,': [49], 'embedded': [51], 'into': [52], 'BC': [54, 133], 'that': [55, 119, 167], 'eliminates': [56], 'need': [58, 102], 'trusted': [60], 'third-party': [61], 'systems,': [62], 'which': [63, 213], 'ultimately': [64], 'saves': [65], 'administration': [66], 'as': [67, 69], 'well': [68, 86], 'service': [70], 'costs.': [71], 'also': [73], 'improves': [74], 'system': [75], 'efficiency': [76], 'reduces': [78], 'security': [81, 98, 113, 146, 177, 221], 'risks.': [82], 'SCs': [84, 171], 'encouraging': [87], 'new': [89], 'technological': [90], 'reforms': [91], 'in': [92, 115], 'Industry': [93], '4.0,': [94], 'but': [95], 'still,': [96], 'various': [97, 183], 'privacy': [100, 148, 175, 192], 'challenges': [101, 142, 198], 'to': [103, 144, 218], 'be': [104, 121], 'addressed.': [105], 'In': [106], 'this': [107, 180], 'paper,': [108], 'survey': [110], 'on': [111], 'vulnerabilities': [114], 'software': [117], 'code': [118], 'can': [120], 'easily': [122], 'hacked': [123], 'by': [124, 153], 'malicious': [126], 'user': [127], 'or': [128], 'may': [129], 'compromise': [130], 'entire': [132], 'network': [134], 'presented.': [136], 'As': [137], 'per': [138], 'literature,': [140], 'related': [143], 'not': [150], 'explored': [151], 'much': [152], 'authors': [155], 'around': [156], 'world.': [158], 'From': [159], 'existing': [161], 'proposals,': [162], 'it': [163], 'has': [164], 'been': [165], 'observed': [166], 'designing': [168], 'complex': [170], 'cannot': [172], 'mitigate': [173], 'its': [174, 220], 'issues.': [178], 'So,': [179], 'paper': [181], 'investigates': [182], 'Artificial': [184], 'Intelligence': [185], '(AI)': [186], 'techniques': [187], 'tools': [189], 'protection.': [193], 'Then,': [194], 'open': [195], 'issues': [196], 'AI-based': [200], 'analyzed.': [203], 'Finally,': [204], 'case': [206], 'study': [207], 'retail': [209], 'marketing': [210], 'presented,': [212], 'uses': [214], 'AI': [215], 'preserve': [219], 'privacy.': [223]}",2020,"['Computer security', 'Computer science', 'Verifiable secret sharing', 'Compromise', 'Privacy by Design', 'Security service', 'Information privacy', 'Software security assurance', 'Cyber-physical system', 'Information security', 'Social science', 'Set (abstract data type)', 'Sociology', 'Programming language', 'Operating system']","Applications of Blockchain (BC) technology and Cyber-Physical Systems (CPS) are increasing exponentially. However, framing resilient and correct smart contracts (SCs) for these smart application is a quite challenging task because of the complexity associated with them. SC is modernizing the traditional industrial, technical, and business processes. It is self-executable, self-verifiable, and embedded into the BC that eliminates the need for trusted third-party systems, which ultimately saves administration as well as service costs. It also improves system efficiency and reduces the associated security risks. However, SCs are well encouraging the new technological reforms in Industry 4.0, but still, various security and privacy challenges need to be addressed. In this paper, a survey on SC security vulnerabilities in the software code that can be easily hacked by a malicious user or may compromise the entire BC network is presented. As per the literature, the challenges related to SC security and privacy are not explored much by the authors around the world. From the existing proposals, it has been observed that designing a complex SCs cannot mitigate its privacy and security issues. So, this paper investigates various Artificial Intelligence (AI) techniques and tools for SC privacy protection. Then, open issues and challenges for AI-based SC are analyzed. Finally, a case study of retail marketing is presented, which uses AI and SC to preserve its security and privacy."
https://openalex.org/W4281971794,AI Privacy Opinions between US and Chinese People,"{'This': [0, 103], 'paper': [1], 'comparatively': [2], 'studies': [3], ""people's"": [4], 'opinions': [5], 'on': [6, 15, 58, 130], 'AI': [7, 54, 62, 113, 131], 'privacy': [8, 59, 76, 134], 'between': [9, 112], 'the': [10, 35, 87, 90, 98, 110, 120, 128], 'US': [11, 48, 91], 'and': [12, 20, 26, 33, 40, 80, 95, 114, 123], 'China.': [13], 'Based': [14], 'data': [16], 'collected': [17], 'from': [18], 'Twitter': [19], 'Weibo,': [21], 'we': [22], 'perform': [23], 'text': [24], 'clustering': [25], 'content': [27], 'analysis': [28], 'to': [29, 86], 'classify': [30], 'opinion': [31, 38], 'types': [32], 'analyze': [34], 'symptoms': [36], 'of': [37, 89, 100], 'polarization': [39, 88, 99], 'drivers': [41], 'by': [42, 61], 'regression': [43], 'analysis.': [44], 'Results': [45], 'show': [46], 'that': [47], 'people': [49, 67], 'express': [50], 'more': [51, 57, 69], 'concerns': [52], 'about': [53, 71], 'privacy,': [55], 'focusing': [56], 'disclosure': [60], 'applications.': [63], 'In': [64], 'contrast,': [65], 'Chinese': [66, 101], 'are': [68, 82], 'optimistic': [70], ""AI's"": [72], 'role': [73], 'in': [74, 126], 'promoting': [75], 'protection.': [77, 135], 'Security,': [78], 'economics,': [79], 'application': [81], 'driving': [83], 'factors': [84], 'leading': [85], 'people,': [92], 'while': [93], 'technologies': [94], 'algorithms': [96], 'influence': [97], 'people.': [102], 'study': [104], 'offers': [105], 'methodological': [106], 'guidance': [107], 'for': [108, 133], 'examining': [109], 'relationship': [111], 'user': [115], 'privacy.': [116], 'It': [117], 'also': [118], 'guides': [119], 'government': [121], 'agencies': [122], 'other': [124], 'practitioners': [125], 'developing': [127], 'policies': [129], 'regulations': [132]}",2022,"['Internet privacy', 'Privacy policy', 'China', 'Polarization (electrochemistry)', 'Government (linguistics)', 'Information privacy', 'Privacy protection', 'Computer science', 'Political science', 'Law', 'Linguistics', 'Chemistry', 'Philosophy', 'Physical chemistry']","This paper comparatively studies people's opinions on AI privacy between the US and China. Based on data collected from Twitter and Weibo, we perform text clustering and content analysis to classify opinion types and analyze the symptoms of opinion polarization and drivers by regression analysis. Results show that US people express more concerns about AI privacy, focusing more on privacy disclosure by AI applications. In contrast, Chinese people are more optimistic about AI's role in promoting privacy protection. Security, economics, and application are driving factors leading to the polarization of the US people, while technologies and algorithms influence the polarization of Chinese people. This study offers methodological guidance for examining the relationship between AI and user privacy. It also guides the government agencies and other practitioners in developing the policies on AI regulations for privacy protection."
https://openalex.org/W4390829176,"Balancing Privacy and Progress: A Review of Privacy Challenges, Systemic Oversight, and Patient Perceptions in AI-Driven Healthcare","{'Integrating': [0], 'Artificial': [1], 'Intelligence': [2], '(AI)': [3], 'in': [4, 31, 57, 98, 133, 167, 199], 'healthcare': [5, 59, 73, 134, 183], 'represents': [6], 'a': [7, 50], 'transformative': [8], 'shift': [9], 'with': [10, 112, 121, 185], 'substantial': [11], 'potential': [12], 'for': [13, 53, 95, 142, 170], 'enhancing': [14], 'patient': [15, 32, 55, 103, 151, 163, 200], 'care.': [16, 201], 'This': [17], 'paper': [18, 86, 179], 'critically': [19], 'examines': [20], 'this': [21], 'integration,': [22], 'confronting': [23], 'significant': [24], 'ethical,': [25], 'legal,': [26], 'and': [27, 36, 68, 82, 91, 105, 146, 162, 175, 191, 196], 'technological': [28], 'challenges,': [29], 'particularly': [30], 'privacy,': [33], 'decision-making': [34], 'autonomy,': [35], 'data': [37, 158], 'integrity.': [38], 'A': [39], 'structured': [40], 'exploration': [41], 'of': [42, 72, 78, 108, 115, 130, 156], 'these': [43], 'issues': [44], 'focuses': [45], 'on': [46], 'Differential': [47, 80], 'Privacy': [48], 'as': [49], 'critical': [51], 'method': [52], 'preserving': [54], 'confidentiality': [56], 'AI-driven': [58], 'systems.': [60], 'We': [61, 100], 'analyze': [62], 'the': [63, 69, 76, 88, 106, 113, 122, 139, 178], 'balance': [64], 'between': [65], 'privacy': [66], 'preservation': [67], 'practical': [70], 'utility': [71], 'data,': [74], 'emphasizing': [75], 'effectiveness': [77], 'encryption,': [79], 'Privacy,': [81], 'mixed-model': [83], 'approaches.': [84], 'The': [85, 128, 153], 'navigates': [87], 'complex': [89], 'ethical': [90, 186], 'legal': [92], 'frameworks': [93], 'essential': [94], 'AI': [96, 184, 193], 'integration': [97], 'healthcare.': [99], 'comprehensively': [101], 'examine': [102], 'rights': [104], 'nuances': [107], 'informed': [109], 'consent,': [110], 'along': [111], 'challenges': [114], 'harmonizing': [116], 'advanced': [117], 'technologies': [118], 'like': [119], 'blockchain': [120], 'General': [123], 'Data': [124], 'Protection': [125], 'Regulation': [126], '(GDPR).': [127], 'issue': [129], 'algorithmic': [131], 'bias': [132, 144], 'is': [135], 'also': [136], 'explored,': [137], 'underscoring': [138], 'urgent': [140], 'need': [141], 'effective': [143], 'detection': [145], 'mitigation': [147], 'strategies': [148], 'to': [149, 181], 'build': [150], 'trust.': [152], 'evolving': [154], 'roles': [155], 'decentralized': [157], 'sharing,': [159], 'regulatory': [160], 'frameworks,': [161], 'agency': [164], 'are': [165], 'discussed': [166], 'depth.': [168], 'Advocating': [169], 'an': [171], 'interdisciplinary,': [172], 'multi-stakeholder': [173], 'approach': [174], 'responsive': [176], 'governance,': [177], 'aims': [180], 'align': [182], 'principles,': [187], 'prioritize': [188], 'patient-centered': [189], 'outcomes,': [190], 'steer': [192], 'towards': [194], 'responsible': [195], 'equitable': [197], 'enhancements': [198]}",2024,"['Health care', 'Autonomy', 'Confidentiality', 'Transformative learning', 'Information privacy', 'Data sharing', 'Data governance', 'Corporate governance', 'Business', 'Internet privacy', 'Knowledge management', 'Political science', 'Psychology', 'Computer science', 'Medicine', 'Computer security', 'Law', 'Service (business)', 'Finance', 'Pathology', 'Marketing', 'Pedagogy', 'Data quality', 'Alternative medicine']","Integrating Artificial Intelligence (AI) in healthcare represents a transformative shift with substantial potential for enhancing patient care. This paper critically examines this integration, confronting significant ethical, legal, and technological challenges, particularly in patient privacy, decision-making autonomy, and data integrity. A structured exploration of these issues focuses on Differential Privacy as a critical method for preserving patient confidentiality in AI-driven healthcare systems. We analyze the balance between privacy preservation and the practical utility of healthcare data, emphasizing the effectiveness of encryption, Differential Privacy, and mixed-model approaches. The paper navigates the complex ethical and legal frameworks essential for AI integration in healthcare. We comprehensively examine patient rights and the nuances of informed consent, along with the challenges of harmonizing advanced technologies like blockchain with the General Data Protection Regulation (GDPR). The issue of algorithmic bias in healthcare is also explored, underscoring the urgent need for effective bias detection and mitigation strategies to build patient trust. The evolving roles of decentralized data sharing, regulatory frameworks, and patient agency are discussed in depth. Advocating for an interdisciplinary, multi-stakeholder approach and responsive governance, the paper aims to align healthcare AI with ethical principles, prioritize patient-centered outcomes, and steer AI towards responsible and equitable enhancements in patient care."
https://openalex.org/W3112307605,"How Do AI-driven Chatbots Impact User Experience? Examining Gratifications, Perceived Privacy Risk, Satisfaction, Loyalty, and Continued Use","{'This': [0], 'study': [1, 95], 'examined': [2], 'how': [3], 'artificial': [4], 'intelligence': [5], '(AI)-driven': [6], 'chatbots': [7], 'impact': [8], 'user': [9, 71, 77], 'experience.': [10], 'It': [11], 'collected': [12], 'survey': [13], 'data': [14], 'from': [15, 23, 47], '1,064': [16], 'consumers': [17], 'who': [18], 'used': [19], 'any': [20], 'chatbot': [21, 48, 55, 68, 87], 'service': [22], 'the': [24, 29, 82], 'top': [25], '30': [26], 'brands': [27], 'in': [28], 'U.S.': [30], 'Results': [31], 'indicated': [32], 'that': [33, 76], 'utilitarian': [34], '(information),': [35], 'hedonic': [36], '(entertainment),': [37], 'technology': [38], '(media': [39], 'appeal),': [40], 'and': [41, 89], 'social': [42], '(social': [43], 'presence)': [44], 'gratifications': [45], 'obtained': [46], 'use': [49, 69, 84], 'positively': [50, 79], 'predicted': [51], ""users'"": [52], 'satisfaction': [53, 78], 'with': [54, 67], 'services': [56, 88], 'of': [57, 86, 93], 'their': [58], 'selected': [59], 'brand.': [60], 'In': [61], 'contrast,': [62], 'perceived': [63], 'privacy': [64], 'risk': [65], 'associated': [66], 'reduced': [70], 'satisfaction.': [72], 'Data': [73], 'also': [74], 'demonstrated': [75], 'affected': [80], 'both': [81], 'continued': [83], 'intention': [85], 'customer': [90], 'loyalty.': [91], 'Implications': [92], 'this': [94], 'are': [96], 'discussed.': [97]}",2020,"['Chatbot', 'Social media', 'Advertising', 'Psychology', 'Loyalty', 'Internet privacy', 'Service (business)', 'Entertainment', 'Customer satisfaction', 'User satisfaction', 'Business', 'World Wide Web', 'Marketing', 'Computer science', 'Political science', 'Human–computer interaction', 'Law']","This study examined how artificial intelligence (AI)-driven chatbots impact user experience. It collected survey data from 1,064 consumers who used any chatbot service from the top 30 brands in the U.S. Results indicated that utilitarian (information), hedonic (entertainment), technology (media appeal), and social (social presence) gratifications obtained from chatbot use positively predicted users' satisfaction with chatbot services of their selected brand. In contrast, perceived privacy risk associated with chatbot use reduced user satisfaction. Data also demonstrated that user satisfaction positively affected both the continued use intention of chatbot services and customer loyalty. Implications of this study are discussed."
https://openalex.org/W2949750920,AI in Healthcare: Ethical and Privacy Challenges,,2019,"['Panacea (medicine)', 'Software deployment', 'Computer science', 'Health care', 'Information privacy', 'Internet privacy', 'Data science', 'Computer security', 'Engineering ethics', 'Medicine', 'Political science', 'Engineering', 'Alternative medicine', 'Law', 'Software engineering', 'Pathology']",
https://openalex.org/W4223947403,"AI Technologies, Privacy, and Security","{'Privacy': [0], 'remains': [1], 'one': [2], 'of': [3, 16, 19], 'the': [4, 17, 28, 51, 88], 'most': [5], 'recurrent': [6], 'concerns': [7, 29, 86, 102], 'that': [8, 76, 90], 'people': [9, 30, 100], 'have': [10, 31], 'about': [11, 32, 87, 103], 'AI': [12, 55, 108], 'technologies.': [13], 'The': [14], 'meaning': [15], 'concept': [18], '""privacy""': [20], 'has': [21], 'proven': [22], 'to': [23, 44, 49, 63, 72, 107, 114], 'be': [24], 'fairly': [25], 'elusive.': [26], 'Accordingly,': [27], 'privacy': [33, 104, 121], 'are': [34, 77, 111], 'often': [35], 'vague': [36], 'and': [37, 48, 80], 'ill-formed,': [38], 'which': [39, 54], 'makes': [40], 'it': [41], 'correspondingly': [42], 'difficult': [43], 'address': [45], 'these': [46], 'concerns,': [47], 'explain': [50], 'ways': [52], 'in': [53, 105, 120], 'technologies': [56], 'do': [57, 59], 'or': [58], 'not': [60], 'pose': [61], 'threats': [62, 89], ""people's"": [64], 'interests.': [65], 'In': [66], 'this': [67], 'article,': [68], 'we': [69], 'draw': [70], 'attention': [71], 'some': [73], 'important': [74], 'distinctions': [75], 'frequently': [78], 'overlooked,': [79], 'spell': [81], 'out': [82], 'their': [83], 'implications': [84], 'for': [85, 94], 'AI-related': [91], 'technology': [92], 'poses': [93], 'privacy.': [95], 'We': [96], 'argue': [97], 'that,': [98], 'when': [99], 'express': [101], 'relation': [106], 'technologies,': [109], 'they': [110], 'usually': [112], 'referring': [113], 'security': [115], 'interests': [116, 119], 'rather': [117], 'than': [118]}",2022,"['Internet privacy', 'Computer security', 'Computer science']","Privacy remains one of the most recurrent concerns that people have about AI technologies. The meaning of the concept of ""privacy"" has proven to be fairly elusive. Accordingly, the concerns people have about privacy are often vague and ill-formed, which makes it correspondingly difficult to address these concerns, and to explain the ways in which AI technologies do or do not pose threats to people's interests. In this article, we draw attention to some important distinctions that are frequently overlooked, and spell out their implications for concerns about the threats that AI-related technology poses for privacy. We argue that, when people express concerns about privacy in relation to AI technologies, they are usually referring to security interests rather than interests in privacy"
