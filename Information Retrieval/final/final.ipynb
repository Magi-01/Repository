{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "690d7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from time import time\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "230e4953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Setup -----------------\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "base_dir = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "88cc2833",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.path.join(base_dir, \"openalex_papers2.csv\")\n",
    "json_path = os.path.join(base_dir, \"ground_value2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "92f07cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Load CSV -----------------\n",
    "# Get current directory of the script\n",
    "\n",
    "df = pd.read_csv(csv_path).fillna('')\n",
    "df = df.drop_duplicates(subset=['id'], keep='first').reset_index(drop=True)\n",
    "docs = [(str(row['title']) + \" \" + str(row['abstract_text'])) for _, row in df.iterrows()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b7b28284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------- Load Ground Truth --------\n",
    "with open(json_path, \"r\") as f:\n",
    "    ground_data = json.load(f)\n",
    "\n",
    "test_queries = ground_data[\"ground_values\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a55f027e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Preprocessing -----------------\n",
    "def preprocess(text):\n",
    "    if not text:\n",
    "        return []\n",
    "    if isinstance(text, list):\n",
    "        text = \" \".join(text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', ' ', text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "    bigrams = ['_'.join([tokens[i], tokens[i+1]]) for i in range(len(tokens)-1)]\n",
    "    return tokens + bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "0243cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------- Vectorized BM25 -----------------\n",
    "class BM25Vectorized:\n",
    "    def __init__(self, docs, k1=1.2, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.docs_tokens = [preprocess(doc) for doc in docs]\n",
    "        self.N = len(docs)\n",
    "        self.avgdl = np.mean([len(d) for d in self.docs_tokens])\n",
    "        # Build vocabulary and doc-term matrix\n",
    "        self.vocab = {}\n",
    "        self.doc_freq = Counter()\n",
    "        data, rows, cols = [], [], []\n",
    "        for doc_id, tokens in enumerate(self.docs_tokens):\n",
    "            freqs = Counter(tokens)\n",
    "            for term, f in freqs.items():\n",
    "                if term not in self.vocab:\n",
    "                    self.vocab[term] = len(self.vocab)\n",
    "                idx = self.vocab[term]\n",
    "                data.append(f)\n",
    "                rows.append(doc_id)\n",
    "                cols.append(idx)\n",
    "                self.doc_freq[term] += 1\n",
    "        self.DTM = csr_matrix((data, (rows, cols)), shape=(self.N, len(self.vocab)), dtype=float)\n",
    "        self.idf = np.array([np.log((self.N - self.doc_freq[t] + 0.5) / (self.doc_freq[t] + 0.5) + 1) for t in self.vocab])\n",
    "        self.doc_len = np.array([len(tokens) for tokens in self.docs_tokens])\n",
    "\n",
    "    def search(self, query, top_k=10):\n",
    "        q_tokens = preprocess(query)\n",
    "        q_indices = [self.vocab[t] for t in q_tokens if t in self.vocab]\n",
    "        if not q_indices:\n",
    "            return []\n",
    "        # BM25 formula\n",
    "        scores = np.zeros(self.N)\n",
    "        for idx in q_indices:\n",
    "            f = self.DTM[:, idx].toarray().flatten()\n",
    "            scores += self.idf[idx] * ((f * (self.k1 + 1)) / (f + self.k1 * (1 - self.b + self.b * self.doc_len / self.avgdl)))\n",
    "        top_docs = np.argsort(-scores)[:top_k]\n",
    "        return [(int(doc_id), float(scores[doc_id])) for doc_id in top_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "bf2c57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- PRF -----------------\n",
    "def pseudo_relevance_feedback(bm25, query, top_k=5, expansion_terms=5):\n",
    "    results = bm25.search(query, top_k=top_k)\n",
    "    top_doc_ids = [doc_id for doc_id, _ in results]\n",
    "    term_counter = Counter()\n",
    "    for doc_id in top_doc_ids:\n",
    "        term_counter.update(bm25.docs_tokens[doc_id])\n",
    "    # Remove original query tokens\n",
    "    for t in preprocess(query):\n",
    "        term_counter.pop(t, None)\n",
    "    top_terms = [t for t, _ in term_counter.most_common(expansion_terms)]\n",
    "    expanded_query = query + \" \" + \" \".join(top_terms)\n",
    "    return expanded_query, top_terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b6ea5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Metrics -----------------\n",
    "def precision_at_k(relevant, retrieved, k=10):\n",
    "    retrieved_k = [doc for doc, _ in retrieved[:k]]\n",
    "    return len(set(retrieved_k) & set(relevant)) / k\n",
    "\n",
    "def recall_at_k(relevant, retrieved, k=10):\n",
    "    retrieved_k = [doc for doc, _ in retrieved[:k]]\n",
    "    return len(set(retrieved_k) & set(relevant)) / max(len(relevant), 1)\n",
    "\n",
    "def average_precision(relevant, retrieved, k=10):\n",
    "    retrieved_k = [doc for doc, _ in retrieved[:k]]\n",
    "    hits, sum_prec = 0, 0\n",
    "    for i, doc_id in enumerate(retrieved_k, 1):\n",
    "        if doc_id in relevant:\n",
    "            hits += 1\n",
    "            sum_prec += hits / i\n",
    "    return sum_prec / hits if hits > 0 else 0\n",
    "\n",
    "def dcg(rels):\n",
    "    return sum((2**r - 1)/np.log2(i+2) for i, r in enumerate(rels))\n",
    "\n",
    "# -------- Decision Measurement --------\n",
    "\n",
    "def evaluate_query(query, relevant_docs, bm25):\n",
    "    results = bm25.search(query, top_k=10)\n",
    "    rel_list = [1 if doc_id in relevant_docs else 0 for doc_id, _ in results]\n",
    "    prec = precision_at_k(relevant_docs, results)\n",
    "    rec = recall_at_k(relevant_docs, results)\n",
    "    ap = average_precision(relevant_docs, results)\n",
    "    ndcg_score = dcg(rel_list)/dcg(sorted(rel_list, reverse=True)) if dcg(rel_list) > 0 else 0\n",
    "    mrr = next((1/(i+1) for i, r in enumerate(rel_list) if r), 0)\n",
    "    f1 = 2*prec*rec/(prec+rec) if (prec+rec) > 0 else 0\n",
    "    return {\n",
    "        'query': query,\n",
    "        'Precision@10': prec,\n",
    "        'Recall@10': rec,\n",
    "        'AveragePrecision': ap,\n",
    "        'F1@10': f1,\n",
    "        'nDCG@10': ndcg_score,\n",
    "        'MRR@10': mrr\n",
    "    }\n",
    "\n",
    "# -------- Latency Measurement --------\n",
    "def measure_latency(bm25, queries, top_k=5, runs=100):\n",
    "    times = []\n",
    "    for i in range(runs):\n",
    "        query = queries[i % len(queries)]\n",
    "        start = time()\n",
    "        bm25.search(query, top_k=top_k)\n",
    "        end = time()\n",
    "        times.append(end - start)\n",
    "    avg_time = statistics.mean(times)\n",
    "    stdev_time = statistics.stdev(times)\n",
    "    return avg_time, stdev_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "836bbab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bm25 = BM25Vectorized(docs)\n",
    "\n",
    "# ----------------- Query -----------------\n",
    "user_query = input(\"Insert Query: \")\n",
    "result = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "30e85dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_prf = time()\n",
    "expanded_query, learned_terms = pseudo_relevance_feedback(bm25, user_query)\n",
    "prf_time = time() - start_prf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "12be21c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRF expansion time: 0.0055 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"PRF expansion time: {prf_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d3b427d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build a mapping from OpenAlex ID to BM25 doc index\n",
    "id_to_index = {row['id']: idx for idx, row in df.iterrows()}\n",
    "numeric_id_to_url = {i: row['id'] for i, row in df.iterrows()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "05e4c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ----------------- Latency Measure -----------------\n",
    "sample_queries = list(test_queries.keys())[:-1]  # take 10 test queries\n",
    "avg, stdev = measure_latency(bm25, sample_queries, runs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e1391007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average query latency over 100 runs: 0.004514 sec (±0.001529)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Average query latency over 100 runs: {avg:.6f} sec (±{stdev:.6f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3548d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize accumulators\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "ap_list = []\n",
    "f1_list = []\n",
    "ndcg_list = []\n",
    "mrr_list = []\n",
    "\n",
    "# Loop through sample queries and evaluate\n",
    "for query in sample_queries:\n",
    "    relevant_docs_urls = [numeric_id_to_url[num] for num in test_queries[query] if num in numeric_id_to_url]\n",
    "    bm25_relevant = [id_to_index[url] for url in relevant_docs_urls]\n",
    "    res = evaluate_query(query, bm25_relevant, bm25)\n",
    "    \n",
    "    precision_list.append(res['Precision@10'])\n",
    "    recall_list.append(res['Recall@10'])\n",
    "    ap_list.append(res['AveragePrecision'])\n",
    "    f1_list.append(res['F1@10'])\n",
    "    ndcg_list.append(res['nDCG@10'])\n",
    "    mrr_list.append(res['MRR@10'])\n",
    "\n",
    "# Compute averages\n",
    "avg_precision = np.mean(precision_list)\n",
    "avg_recall = np.mean(recall_list)\n",
    "avg_ap = np.mean(ap_list)\n",
    "avg_f1 = np.mean(f1_list)\n",
    "avg_ndcg = np.mean(ndcg_list)\n",
    "avg_mrr = np.mean(mrr_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b79ba102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average metrics over 48 queries:\n",
      "Precision@10: 0.0000\n",
      "Recall@10: 0.0000\n",
      "AveragePrecision: 0.0000\n",
      "F1@10: 0.0000\n",
      "nDCG@10: 0.0000\n",
      "MRR@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Average metrics over {len(test_queries.keys())} queries:\")\n",
    "print(f\"Precision@10: {avg_precision:.4f}\")\n",
    "print(f\"Recall@10: {avg_recall:.4f}\")\n",
    "print(f\"AveragePrecision: {avg_ap:.4f}\")\n",
    "print(f\"F1@10: {avg_f1:.4f}\")\n",
    "print(f\"nDCG@10: {avg_ndcg:.4f}\")\n",
    "print(f\"MRR@10: {avg_mrr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "935288f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Which topics are introduced in AI lecture notes?', 'Precision@10': 0.0, 'Recall@10': 0.0, 'AveragePrecision': 0, 'F1@10': 0, 'nDCG@10': 0, 'MRR@10': 0}\n",
      "\n",
      "query:Which topics are introduced in AI lecture notes?\n",
      "\n",
      "Precision@10:0.0\n",
      "\n",
      "Recall@10:0.0\n",
      "\n",
      "AveragePrecision:0\n",
      "\n",
      "F1@10:0\n",
      "\n",
      "nDCG@10:0\n",
      "\n",
      "MRR@10:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the relevant file index\n",
    "if user_query in test_queries:\n",
    "    relevant_docs = test_queries[user_query]\n",
    "    relevant_docs_urls = [numeric_id_to_url[num] for num in test_queries[query] if num in numeric_id_to_url]\n",
    "    bm25_relevant = [id_to_index[url] for url in relevant_docs_urls]\n",
    "\n",
    "    # Metrics\n",
    "    result = evaluate_query(user_query, relevant_docs, bm25)\n",
    "    print(result)\n",
    "else:\n",
    "    print(\"No ground truth available; skip metrics.\")\n",
    "\n",
    "\n",
    "if result is not None:\n",
    "    for lab, res in result.items():\n",
    "        print(f\"\\n{lab}:{res}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "63cb5ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ID in CSV: 3\n"
     ]
    }
   ],
   "source": [
    "bm25_relevant = []\n",
    "for doc_id in relevant_docs:\n",
    "    if doc_id in id_to_index:\n",
    "        bm25_relevant.append(id_to_index[doc_id])\n",
    "    else:\n",
    "        print(\"Missing ID in CSV:\", doc_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3e9965b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 top docs: [47, 3999, 327, 176, 397, 248, 967, 632, 702, 546]\n",
      "Mapped relevant indices: []\n"
     ]
    }
   ],
   "source": [
    "top_docs = [doc_id for doc_id, _ in bm25.search(query, top_k=10)]\n",
    "print(\"BM25 top docs:\", top_docs)\n",
    "print(\"Mapped relevant indices:\", bm25_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "68197f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original Query: Which topics are introduced in AI lecture notes?\n",
      "Expanded Query: Which topics are introduced in AI lecture notes? artificial intelligence artificial_intelligence note_artificial healthcare\n",
      "Learned Terms: ['artificial', 'intelligence', 'artificial_intelligence', 'note_artificial', 'healthcare']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nOriginal Query:\", user_query)\n",
    "print(\"Expanded Query:\", expanded_query)\n",
    "print(\"Learned Terms:\", learned_terms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d01a4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query:Which topics are introduced in AI lecture notes?, Precision@10:0.0, Recall@10:0.0, AveragePrecision:0, F1@10:0, nDCG@10:0, MRR@10:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if result is not None:\n",
    "    print(\", \".join(f\"{k}:{r}\" for k, r in result.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b7fca174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Results:\n",
      "1. Paper ID: https://openalex.org/W2591744545, Score: 79.7723\n",
      "   Title: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "   Snippet: ...\n",
      "\n",
      "2. Paper ID: https://openalex.org/W3152157423, Score: 79.7723\n",
      "   Title: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "   Snippet: ...\n",
      "\n",
      "3. Paper ID: https://openalex.org/W2626885185, Score: 79.7723\n",
      "   Title: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)\n",
      "   Snippet: ...\n",
      "\n",
      "4. Paper ID: https://openalex.org/W26772505, Score: 75.7437\n",
      "   Title: Lecture Notes in Artificial Intelligence\n",
      "   Snippet: ...\n",
      "\n",
      "5. Paper ID: https://openalex.org/W4290391506, Score: 30.1411\n",
      "   Title: AI in Healthcare: Privacy, Trust and Ethical Challenges\n",
      "   Snippet: Artificial intelligence is an ever-growing technology that has proved its usefulness in the field of healthcare. However, AI has also raised several challenges and ethical concerns in relation to privacy...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nTop Results:\")\n",
    "for rank, (doc_id, score) in enumerate(bm25.search(expanded_query, top_k=5), 1):\n",
    "    snippet = \" \".join(df.loc[doc_id, 'abstract_text'].split()[:30])\n",
    "    print(f\"{rank}. Paper ID: {df.loc[doc_id, 'id']}, Score: {score:.4f}\")\n",
    "    print(f\"   Title: {df.loc[doc_id, 'title']}\")\n",
    "    print(f\"   Snippet: {snippet}...\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "def7312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if expanded query exists from PRF\n",
    "results = bm25.search(expanded_query, top_k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "05c93350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top retrieved doc IDs and scores:\n",
      "1: Doc ID 520, Score 79.7723\n",
      "2: Doc ID 67, Score 79.7723\n",
      "3: Doc ID 687, Score 79.7723\n",
      "4: Doc ID 2, Score 75.7437\n",
      "5: Doc ID 4305, Score 30.1411\n",
      "6: Doc ID 80, Score 29.5302\n",
      "7: Doc ID 955, Score 27.2358\n",
      "8: Doc ID 368, Score 27.2082\n",
      "9: Doc ID 459, Score 25.7037\n",
      "10: Doc ID 899, Score 24.1865\n"
     ]
    }
   ],
   "source": [
    "print(\"Top retrieved doc IDs and scores:\")\n",
    "for rank, (doc_id, score) in enumerate(results, 1):\n",
    "    print(f\"{rank}: Doc ID {doc_id}, Score {score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
