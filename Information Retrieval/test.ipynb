{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de80e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "FEEDBACK_FILE = \"user_feedback10.json\"\n",
    "feedback_dict = {}\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0eb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Preprocessing -----------------\n",
    "def normalize_query(query: str) -> str:\n",
    "    tokens = query.lower().split()\n",
    "    tokens = [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6033c566",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^a-zA-Z0-9 ]', ' ', text.lower())\n",
    "    tokens = [w for w in text.split() if w not in stop_words]  # remove stopwords first\n",
    "    return [stemmer.stem(tok) for tok in tokens]  # then stem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ea107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_feedback():\n",
    "    try:\n",
    "        with open(FEEDBACK_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "######################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d14680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- BM25 Class -----------------\n",
    "class BM25:\n",
    "    def __init__(self, docs, k1=1.5, b=0.75):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.docs = [preprocess(doc) for doc in docs]\n",
    "        self.N = len(docs)\n",
    "        self.avgdl = sum(len(d) for d in self.docs) / self.N\n",
    "        self.doc_len = [len(d) for d in self.docs]\n",
    "        self.index = defaultdict(dict)\n",
    "        self.build_index()\n",
    "    \n",
    "    def build_index(self):\n",
    "        for doc_id, doc in enumerate(self.docs):\n",
    "            freqs = Counter(doc)\n",
    "            for term, f in freqs.items():\n",
    "                self.index[term][doc_id] = f\n",
    "\n",
    "    def idf(self, term):\n",
    "        n_t = len(self.index.get(term, {}))\n",
    "        return math.log((self.N - n_t + 0.5) / (n_t + 0.5) + 1)\n",
    "\n",
    "    def score(self, query_tokens, doc_id):\n",
    "        score = 0.0\n",
    "        doc = self.docs[doc_id]\n",
    "        for term in query_tokens:\n",
    "            f = self.index.get(term, {}).get(doc_id, 0)\n",
    "            if f == 0:\n",
    "                continue\n",
    "            idf = self.idf(term)\n",
    "            denom = f + self.k1 * (1 - self.b + self.b * len(doc)/self.avgdl)\n",
    "            score += idf * (f * (self.k1 + 1)) / denom\n",
    "        return score\n",
    "\n",
    "    def search(self, query, top_k=10):\n",
    "        query_tokens = preprocess(query)\n",
    "        scores = [(doc_id, self.score(query_tokens, doc_id)) for doc_id in range(self.N)]\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        return scores[:top_k]\n",
    "    ##########################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7705c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Search & Feedback -----------------\n",
    "def search_display(bm25, query, df, top_k=10):\n",
    "    results = bm25.search(query, top_k=top_k)\n",
    "    for rank, (doc_id, score) in enumerate(results, start=1):\n",
    "        print(f\"{rank}. Doc ID: {doc_id}, Score: {score:.4f}\")\n",
    "        snippet = \" \".join(df.loc[doc_id, 'abstract_text'].split()[:30])\n",
    "        print(f\"   {snippet}...\\n\")\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4685fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_user_feedback(results, df):\n",
    "    relevant_docs = []\n",
    "    print(\"Mark relevant documents by typing their number (space-separated, e.g., 1 3 5):\")\n",
    "    selected = input(\"Relevant docs: \").strip()\n",
    "    if selected:\n",
    "        try:\n",
    "            indices = [int(x)-1 for x in selected.split()]\n",
    "            for i in indices:\n",
    "                relevant_docs.append(results[i][0])\n",
    "        except:\n",
    "            print(\"Invalid input, no docs marked as relevant.\")\n",
    "    return relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "############################\n",
    "def pseudo_relevance_feedback_iterative(bm25, query, relevant_doc_ids, n_iterations=3, expansion_terms=5):\n",
    "    current_query = query\n",
    "    all_expanded_terms = []\n",
    "    for i in range(n_iterations):\n",
    "        term_counter = Counter()\n",
    "        for doc_id in relevant_doc_ids:\n",
    "            term_counter.update(bm25.docs[doc_id])\n",
    "        original_tokens = preprocess(current_query)\n",
    "        for t in original_tokens:\n",
    "            if t in term_counter:\n",
    "                del term_counter[t]\n",
    "        top_terms = [t for t, _ in term_counter.most_common(expansion_terms)]\n",
    "        if not top_terms:\n",
    "            break\n",
    "        all_expanded_terms.extend(top_terms)\n",
    "        current_query = current_query + \" \" + \" \".join(top_terms)\n",
    "        # Take top 5 docs for next iteration (pseudo-relevance)\n",
    "        search_results = bm25.search(current_query, top_k=10)\n",
    "        relevant_doc_ids = [doc_id for doc_id, _ in search_results[:5]]\n",
    "    return current_query, all_expanded_terms, relevant_doc_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aef6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####\n",
    "def save_feedback(query, expanded_terms, relevant_docs):\n",
    "    try:\n",
    "        with open(FEEDBACK_FILE, \"r\") as f:\n",
    "            feedback_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        feedback_data = {}\n",
    "    if query in feedback_data:\n",
    "        feedback_data[query][\"expanded_terms\"] = list(set(feedback_data[query][\"expanded_terms\"] + expanded_terms))\n",
    "        feedback_data[query][\"relevant_docs\"] = list(set(feedback_data[query][\"relevant_docs\"] + relevant_docs))\n",
    "    else:\n",
    "        feedback_data[query] = {\n",
    "            \"expanded_terms\": expanded_terms,\n",
    "            \"relevant_docs\": relevant_docs\n",
    "        }\n",
    "    with open(FEEDBACK_FILE, \"w\") as f:\n",
    "        json.dump(feedback_data, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b6b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def augment_query_with_feedback(query):\n",
    "    try:\n",
    "        with open(FEEDBACK_FILE, \"r\") as f:\n",
    "            feedback_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return query\n",
    "    if query in feedback_data:\n",
    "        expanded_terms = feedback_data[query][\"expanded_terms\"]\n",
    "        return query + \" \" + \" \".join(expanded_terms)\n",
    "    return query\n",
    "########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de1c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Evaluation -----------------\n",
    "def precision_at_k(relevant_docs, retrieved_docs, k=10):\n",
    "    retrieved_k = [doc for doc, _ in retrieved_docs[:k]]\n",
    "    return len(set(retrieved_k) & set(relevant_docs)) / k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67343ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def recall_at_k(relevant_docs, retrieved_docs, k=10):\n",
    "    retrieved_k = [doc for doc, _ in retrieved_docs[:k]]\n",
    "    return len(set(retrieved_k) & set(relevant_docs)) / len(relevant_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def average_precision(relevant_docs, retrieved_docs, k=10):\n",
    "    retrieved_k = [doc for doc, _ in retrieved_docs[:k]]\n",
    "    hits = 0\n",
    "    sum_prec = 0\n",
    "    for i, doc_id in enumerate(retrieved_k, start=1):\n",
    "        if doc_id in relevant_docs:\n",
    "            hits += 1\n",
    "            sum_prec += hits / i\n",
    "    if hits == 0:\n",
    "        return 0\n",
    "    return sum_prec / hits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96377adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_all_queries(test_queries, bm25, df, top_k=10):\n",
    "    \"\"\"\n",
    "    Evaluate all queries in test_queries and compute aggregate metrics.\n",
    "    \n",
    "    Returns a dictionary with per-query metrics and overall mean metrics.\n",
    "    \"\"\"\n",
    "    all_precisions = []\n",
    "    all_recalls = []\n",
    "    all_average_precisions = []\n",
    "\n",
    "    per_query_metrics = {}\n",
    "\n",
    "    for query_text, relevant_docs in test_queries.items():\n",
    "        # Augment query with previous feedback if available\n",
    "        query_aug = augment_query_with_feedback(query_text)\n",
    "\n",
    "        # Retrieve top_k results\n",
    "        results = bm25.search(query_aug, top_k=top_k)\n",
    "\n",
    "        # Compute metrics for this query\n",
    "        prec = precision_at_k(relevant_docs, results, k=top_k)\n",
    "        rec = recall_at_k(relevant_docs, results, k=top_k)\n",
    "        ap = average_precision(relevant_docs, results, k=top_k)\n",
    "\n",
    "        per_query_metrics[query_text] = {\n",
    "            'Precision@{}'.format(top_k): prec,\n",
    "            'Recall@{}'.format(top_k): rec,\n",
    "            'AveragePrecision': ap\n",
    "        }\n",
    "\n",
    "        all_precisions.append(prec)\n",
    "        all_recalls.append(rec)\n",
    "        all_average_precisions.append(ap)\n",
    "\n",
    "    # Compute mean metrics\n",
    "    mean_metrics = {\n",
    "        'MeanPrecision@{}'.format(top_k): sum(all_precisions)/len(all_precisions),\n",
    "        'MeanRecall@{}'.format(top_k): sum(all_recalls)/len(all_recalls),\n",
    "        'MAP': sum(all_average_precisions)/len(all_average_precisions)\n",
    "    }\n",
    "\n",
    "    return per_query_metrics, mean_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ec035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_feedback(feedback_dict, query, expanded_terms, relevant_docs):\n",
    "    if query in feedback_dict:\n",
    "        feedback_dict[query][\"expanded_terms\"] = list(\n",
    "            set(feedback_dict[query][\"expanded_terms\"] + expanded_terms)\n",
    "        )\n",
    "        feedback_dict[query][\"relevant_docs\"] = list(\n",
    "            set(feedback_dict[query][\"relevant_docs\"] + relevant_docs)\n",
    "        )\n",
    "    else:\n",
    "        feedback_dict[query] = {\n",
    "            \"expanded_terms\": expanded_terms,\n",
    "            \"relevant_docs\": relevant_docs\n",
    "        }\n",
    "\n",
    "########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------- Main Iterative Feedback Workflow -----------------\n",
    "def run_test_queries(df, bm25, test_queries, n_iterations=3, top_k=10, expansion_terms=5):\n",
    "    feedback_dict = load_feedback()  # load existing feedback once\n",
    "\n",
    "    for query_text, relevant_docs in test_queries.items():\n",
    "        print(f\"\\n=== Processing Query: '{query_text}' ===\")\n",
    "        query_aug = augment_query_with_feedback(query_text)\n",
    "        print(f\"Augmented query: {query_aug}\")\n",
    "\n",
    "        results = search_display(bm25, query_aug, df, top_k=top_k)\n",
    "\n",
    "        if not relevant_docs:\n",
    "            relevant_docs = get_user_feedback(results, df)\n",
    "\n",
    "        if not relevant_docs:\n",
    "            print(\"No relevant documents marked. Skipping PRF.\\n\")\n",
    "            continue\n",
    "\n",
    "        final_query, learned_terms, final_relevant_docs = pseudo_relevance_feedback_iterative(\n",
    "            bm25,\n",
    "            query_aug,\n",
    "            relevant_doc_ids=relevant_docs,\n",
    "            n_iterations=n_iterations,\n",
    "            expansion_terms=expansion_terms\n",
    "        )\n",
    "        print(f\"Final query after {n_iterations} iterations: {final_query}\")\n",
    "\n",
    "        new_results = search_display(bm25, final_query, df, top_k=top_k)\n",
    "\n",
    "        # Update feedback **in memory only**\n",
    "        update_feedback(feedback_dict, query_text, learned_terms, final_relevant_docs)\n",
    "\n",
    "    # Write feedback JSON once, at the very end\n",
    "    with open(FEEDBACK_FILE, \"w\") as f:\n",
    "        json.dump(feedback_dict, f, indent=2)\n",
    "\n",
    "    # Optional: evaluate queries after all processing\n",
    "    per_query_metrics, mean_metrics = evaluate_all_queries(test_queries, bm25, df, top_k)\n",
    "    print(\"\\n=== Evaluation Metrics for All Queries ===\")\n",
    "    print(per_query_metrics)\n",
    "    print(\"\\nMean metrics:\")\n",
    "    print(mean_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4da12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load data and build BM25\n",
    "df = pd.read_csv(\"openalex_papers.csv\").fillna('').reset_index(drop=True)\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d397ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abstracts = df['abstract_text'].tolist()\n",
    "bm25 = BM25(abstracts)\n",
    "#####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625142bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example test_queries dict\n",
    "test_queries = {\n",
    "    \"artificial intelligence textbook\": [0, 1, 2, 3],\n",
    "    \"ai in medicine\": [5, 7, 10],\n",
    "    \"distributed multiagent systems\": [8],\n",
    "    \"ai in healthcare\": [7, 9, 44, 45],\n",
    "    \"deep convolutional neural networks\": [10, 12],\n",
    "    \"neural network fundamentals\": [11, 14],\n",
    "    \"prevent overfitting in neural nets\": [13],\n",
    "    \"dimensionality reduction with neural networks\": [16],\n",
    "    \"overview of deep learning\": [17],\n",
    "    \"knowledge distillation in neural networks\": [18],\n",
    "    \"sequence to sequence learning\": [19],\n",
    "    \"semi-supervised learning\": [20, 21, 27, 28, 29],\n",
    "    \"fast supervised learning algorithms\": [22, 23],\n",
    "    \"unsupervised learning methods\": [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n",
    "    \"deep clustering visual features\": [36],\n",
    "    \"lstm video representations\": [37],\n",
    "    \"slow feature analysis\": [38],\n",
    "    \"generative ai cybersecurity\": [39],\n",
    "    \"ai ethics and privacy\": [40, 41, 42, 44, 45],\n",
    "    \"federated learning privacy\": [43],\n",
    "    \"chatbots impact on users\": [44],\n",
    "    \"image classification using deep learning\": [10, 12],\n",
    "    \"convolutional deep belief networks\": [35],\n",
    "    \"linear neural network learning\": [36],\n",
    "    \"jigsaw puzzles visual representations\": [31],\n",
    "    \"unsupervised depth ego-motion\": [34],\n",
    "    \"ai privacy challenges healthcare\": [44],\n",
    "    \"scalable unsupervised learning\": [35],\n",
    "    \"ai in cyber-physical systems\": [43],\n",
    "    \"privacy preserving AI\": [43],\n",
    "    \"lite bert language models\": [25],\n",
    "    \"supervised learning comparison\": [23],\n",
    "    \"gaussian fields semi-supervised learning\": [26],\n",
    "    \"virtual adversarial training\": [30],\n",
    "    \"equality of opportunity supervised learning\": [26],\n",
    "    \"introduction to semi-supervised learning\": [27],\n",
    "    \"deep learning neural networks overview\": [17],\n",
    "    \"reducing data dimensionality neural nets\": [16],\n",
    "    \"sequence models neural networks\": [19],\n",
    "    \"semi-supervised literature survey\": [21],\n",
    "    \"unsupervised visual representations\": [31],\n",
    "    \"slow features unsupervised learning\": [38],\n",
    "    \"chatgpt generative ai impact\": [39],\n",
    "    \"ai privacy research\": [40, 41, 42, 44],\n",
    "    \"federated learning AI\": [43],\n",
    "    \"ethical challenges AI healthcare\": [44],\n",
    "    \"ai technologies privacy security\": [45],\n",
    "    \"dropout neural networks\": [13],\n",
    "    \"pattern recognition neural networks\": [14],\n",
    "    \"physical systems emergent computation\": [15],\n",
    "    \"scaled conjugate gradient learning\": [22],\n",
    "    \"empirical comparison supervised algorithms\": [23]\n",
    "}\n",
    "###############\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf8abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run the workflow\n",
    "run_test_queries(df, bm25, test_queries, n_iterations=3, top_k=10, expansion_terms=5)\n",
    "###########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b211ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def query_feedback(query):\n",
    "    \"\"\"\n",
    "    Returns learned expanded terms and relevant document IDs for a given query.\n",
    "    Does NOT print anything; intended for programmatic access.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(FEEDBACK_FILE, \"r\") as f:\n",
    "            feedback_data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return None  # No feedback data available\n",
    "\n",
    "    if query not in feedback_data:\n",
    "        return None  # No feedback for this query\n",
    "\n",
    "    expanded_terms = feedback_data[query].get(\"expanded_terms\", [])\n",
    "    relevant_docs = feedback_data[query].get(\"relevant_docs\", [])\n",
    "\n",
    "    return {\"expanded_terms\": expanded_terms, \"relevant_docs\": relevant_docs}\n",
    "#####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba848dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_feedback_comparison(query, df, bm25, top_k=10):\n",
    "    \"\"\"\n",
    "    Shows titles from both the original dataset (raw retrieval) and\n",
    "    the learned feedback (JSON) for a given query.\n",
    "    \"\"\"\n",
    "    feedback_info = query_feedback(query)\n",
    "    inferred_doc_ids = feedback_info[\"relevant_docs\"] if feedback_info else []\n",
    "\n",
    "    # Original BM25 retrieval\n",
    "    raw_results = bm25.search(query, top_k=top_k)\n",
    "    raw_doc_ids = [doc_id for doc_id, _ in raw_results]\n",
    "\n",
    "    print(f\"\\nQuery: {query}\\n\")\n",
    "\n",
    "    print(\"Titles from JSON-inferred feedback:\")\n",
    "    if inferred_doc_ids:\n",
    "        for doc_id in inferred_doc_ids:\n",
    "            if doc_id < len(df):\n",
    "                print(f\"- {df.loc[doc_id, 'title']}\")\n",
    "    else:\n",
    "        print(\"- None\")\n",
    "\n",
    "    print(\"\\nTitles from original BM25 retrieval:\")\n",
    "    if raw_doc_ids:\n",
    "        for doc_id in raw_doc_ids:\n",
    "            if doc_id < len(df):\n",
    "                print(f\"- {df.loc[doc_id, 'title']}\")\n",
    "    else:\n",
    "        print(\"- None\")\n",
    "#########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131f181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "show_feedback_comparison(\"healthcare\", df, bm25)\n",
    "####\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
