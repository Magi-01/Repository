\documentclass[a4paper,12pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{hyperref}

\title{Report on BM25 Information Retrieval System on openAlex dataset}
\author{Mutua Fadhla Mohamed}
\author{SM3201434}

\begin{document}
\maketitle

\section{Basic Idea}
This project builds an Information Retrieval (IR) system using the BM25 ranking function. 
We take a collection of research papers, clean the text, turn it into tokens, 
and compute scores to find which papers best match a given query. 
The system adds query expansion using Pseudo-Relevance Feedback (PRF) and evaluates 
how well the system performs using standard IR metrics.
To note is that there were 2 datasets one of 50 academic papers `openalex\_papers.csv` and another of 5000 `openalex\_papers2.csv`. The ones described in this report are of the smaller dataset. The Queries on the other hand are inside the json folder ground\_values.json

\section{Libraries Used}
\begin{itemize}
    \item \textbf{pandas} -- reading CSV files and handling tabular data.
    \item \textbf{re} -- regular expressions for cleaning text.
    \item \textbf{numpy} -- mathematical arrays and fast operations.
    \item \textbf{collections.Counter} -- counting words easily.
    \item \textbf{nltk} -- natural language tools like stopwords and lemmatization.
    \item \textbf{scipy.sparse} -- efficient storage of document-term matrix.
    \item \textbf{time} -- measuring search speed.
    \item \textbf{json} and \textbf{os} -- reading ground truth files and file paths.
    \item \textbf{statistics} -- used for measuring latency.
\end{itemize}

\section{Preprocessing Function}
The function \texttt{preprocess} prepares text for retrieval.

\begin{itemize}
    \item \textbf{Lowercasing:} Treats ``Apple'' and ``apple'' as the same.
    \item \textbf{Regex cleaning:} Keeps only letters and numbers, removes symbols.
    \item \textbf{Tokenization:} Splits text into words (tokens).
    \item \textbf{Stopword removal:} Removes common words like ``the'' that add little meaning.
    \item \textbf{Lemmatization:} Reduces words to base form, e.g., ``running'' $\to$ ``run'', improving matches between queries and documents.
    \item \textbf{Bigrams:} Creates word pairs like ``information\_retrieval'' to capture phrases.
\end{itemize}

\section{BM25 Class}
BM25 scores how relevant a document is to a query.

\subsection{IDF Formula}
\[
idf(t) = \log \frac{N - df(t) + 0.5}{df(t) + 0.5} + 1
\]

\textbf{Explanation:}  
- $N$ is the total number of documents. \\
- $df(t)$ is how many documents contain term $t$.  \\
- This gives higher scores to rare terms (more informative), lower scores to common words.  \\
- Adding 1 ensures the score is positive.  \\

Rare words like ``unsupervised'' in a corpus carry more distinguishing power. 

\subsection{BM25 Score Formula}
\[
score(d, q) = \sum_{t \in q} idf(t) \cdot \frac{f(t,d)(k_1 + 1)}{f(t,d) + k_1 \left(1 - b + b \cdot \frac{|d|}{avgdl}\right)}
\]

\textbf{Explanation:}  
- $f(t,d)$ = frequency of term $t$ in document $d$.  \\
- $|d|$ = number of terms in document $d$.  \\
- $avgdl$ = average document length in the collection.  \\
- $k_1$ controls how term frequency affects score; $b$ controls document length normalization.  \\

Terms that appear more often in a document increase its score, but very long documents are normalized to prevent unfair advantage.

\section{Pseudo-Relevance Feedback (PRF)}
PRF expands a query with terms from top-ranked documents.

Steps:
\begin{enumerate}
    \item Search with the original query.
    \item Take top $k$ documents.
    \item Count terms that appear frequently in them.
    \item Exclude original query terms.
    \item Add the top frequent terms to the query.
\end{enumerate}

PRF helps capture semantic context so documents that don't contain exact query words but are conceptually relevant can be retrieved.

\section{Evaluation Metrics}
\begin{itemize}
    \item \textbf{Precision\_k:} Proportion of top $k$ results that are relevant.
    \[
    P_k = \frac{|Retrieved_k \cap Relevant|}{k}
    \]
    \item \textbf{Recall\_k:} Fraction of relevant documents retrieved in top $k$.
    \[
    R_k = \frac{|Retrieved_k \cap Relevant|}{|Relevant|}
    \]
    \item \textbf{Average Precision (AP):} Average of precision values at ranks of relevant documents.
    \item \textbf{F1 Score:} Harmonic mean of precision and recall, balances both.
    \[
    F1 = 2 \cdot \frac{Precision \cdot Recall}{Precision + Recall}
    \]
    \item \textbf{DCG / nDCG:} Measures ranking quality.
    \[
    DCG = \sum_{i=1}^k \frac{2^{rel_i}-1}{\log_2(i+1)}
    \]
    - $rel_i$ = relevance of document at rank $i$.  \\
    - nDCG normalizes DCG by the ideal ranking DCG.  \\
  Higher-ranked relevant documents contribute more to the score.
  
    \item \textbf{MRR:} Mean Reciprocal Rank, focuses on the position of the first relevant document.
    \[
    MRR = \frac{1}{rank\ of\ first\ relevant\ doc}
    \]
\end{itemize}

\section{Query Latency Measurement}
The system measures the time to answer queries to ensure it is faster than reading the collection from disk.  
For $R$ runs, each query $q_i$ is timed as $t_i$.  

Average latency:
\[
\bar{t} = \frac{1}{R} \sum_{i=1}^{R} t_i
\]

Standard deviation of latency:
\[
\sigma_t = \sqrt{\frac{1}{R} \sum_{i=1}^{R} (t_i - \bar{t})^2}
\]

\textbf{Interpretation:}  \\
- $\bar{t}$ shows typical query speed.  \\
- $\sigma_t$ shows variability; a high $\sigma_t$ means some queries are much slower or faster than average. \\

\section{Evaluation}
Average query latency over 100 runs is about: 
\[ \bar{x} = 0.006265 \text{ sec},
 \quad \sigma = 0.008554 \text{ sec} 
 \] 
The relatively large standard deviation, \(\sigma = 0.008554\), compared to the mean, \(\bar{x} = 0.006265\ \text{sec}\), indicates high variability: some queries may take much longer than the average, while others are faster, reflecting inconsistent query latency across different inputs.


Average metrics over 59 queries:
\[
\text{Precision@10: } 0.1155, \quad
\text{Recall@10: } 0.5948, \quad
\text{AveragePrecision: } 0.2707
\]

\[
\text{F1@10: } 0.1763, \quad
\text{nDCG@10: } 0.3827, \quad
\text{MRR@10: } 0.2879
\]

\textbf{Interpretation:}  
- System retrieves many relevant documents (high recall) but top results include irrelevant docs (low precision).  \\
- Average Precision and nDCG show relevant docs often appear early, but ranking is not perfect.  \\
- F1 and MRR indicate overall retrieval quality and position of first relevant document are moderate. \\


Given the Query
 \begin{quote}
  distance between nlp and ml 
  \end{quote} 
  An example expansion of the query is: 
  \begin{quote} 
  distance between nlp and ml learning data unsupervised dialect based 
  \end{quote} 
  The learned terms added via pseudo-relevance feedback are: 
  \begin{quote} learning, data, unsupervised, dialect, based 
  \end{quote} 
  \subsubsection*{Interpretation}
   \begin{enumerate} 
   \item
   	 \textbf{Effect of Query Expansion:} 
   	 \\ The original query was expanded using terms frequently appearing in top-ranked documents. This captures semantic context, allowing retrieval of documents that are relevant even if they do not contain the exact original words.
 \item 
 	\textbf{Top Results Relevance:} \\
 	 The top five retrieved documents are:
 	 \begin{enumerate} 
 	 \item Arabic Dialect Identification with an Unsupervised Learning (Based on a Lexicon). Application Case
 	  \item Exploring Unsupervised Learning Techniques for the Internet of Things
 	   \item Generalized $K$-Harmonic Means -- Dynamic Weighting of Data in Unsupervised Learning 
 	   \item Supervised and Unsupervised Learning for Data Science 
 	   \item An Overview on Unsupervised Learning from Data Mining Perspective 
 	  \end{enumerate} 

All top documents match the expanded query terms, especially \texttt{unsupervised}, \texttt{learning}, and \texttt{data}, showing that pseudo-relevance feedback effectively improves relevance. 

\item 
	\textbf{Score Distribution:} \\ 
	The highest score is 27.5407, while the fifth score is 13.6614. This indicates that some documents are much more strongly related to the expanded query than others. 
\item 
	\textbf{System Insight:} \\ 
	Pseudo-relevance feedback surfaces documents that the original query might miss due to vocabulary mismatch. Learned terms guide retrieval toward conceptually related papers, not only literal matches.
  \end{enumerate} 

\section{Conclusion}
BM25-based retrieval system, enhanced with pseudo-relevance feedback, thus effectively improves access to relevant documents. Query expansion captures semantic context, allowing retrieval of papers that match the intended meaning even when exact query terms are absent. Evaluation shows high recall but moderate precision, indicating that while most relevant documents are retrieved, top results may include some irrelevant items. Metrics such as Average Precision, nDCG, F1, and MRR confirm that relevant documents generally appear early in the ranking, and the first relevant document is often positioned near the top. Overall, the system highlights the practical benefit of combining BM25 scoring with pseudo-relevance feedback for small-scale academic datasets.

\end{document}
