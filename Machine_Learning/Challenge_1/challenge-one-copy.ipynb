{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis of Supervised and Unsupervised Learning on UCI Dataset (ID: 267)**  \n",
    "\n",
    "## **1. Data Pretreatment**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import numpy as np  \n",
    "import pandas as pd \n",
    "import seaborn as sns  \n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from numpy import linalg as LA  \n",
    "from tqdm import tqdm  \n",
    "from scipy.spatial.distance import cdist, euclidean, minkowski  \n",
    "\n",
    "from sklearn import preprocessing  \n",
    "from sklearn.decomposition import PCA  \n",
    "from sklearn.manifold import TSNE  \n",
    "from sklearn.cluster import KMeans, DBSCAN  \n",
    "from sklearn.neighbors import NearestNeighbors  \n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression  \n",
    "from sklearn.model_selection import KFold, train_test_split  \n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, classification_report, confusion_matrix, \n",
    "    ConfusionMatrixDisplay, normalized_mutual_info_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **1.1 Data Loading and Inspection**  \n",
    "The dataset was obtained from the UCI Machine Learning Repository using `fetch_ucirepo(id=267)`. After loading the data, we examined its structure, including the number of samples , features, and class distribution. From the repository we have:\n",
    "1. The data has no missing value\n",
    "2. The class distribution (the target) is already encoded (either 1 or 0)\n",
    "3. There are 1372 samples and 4 features\n",
    "4. The four features are continuos\n",
    "\n",
    "data: [1372 rows x 5 columns]\n",
    "| variance | skewness | curtosis | entropy  | targets |\n",
    "|----------|----------|----------|----------|---------|\n",
    "| 3.62160  | 8.66610  | -2.8073  | -0.44699 | 0       |\n",
    "| 4.54590  | 8.16740  | -2.4586  | -1.46210 | 0       |\n",
    "| 3.86600  | -2.63830 | 1.9242   | 0.10645  | 0       |\n",
    "| 3.45660  | 9.52280  | -4.0112  | -3.59440 | 0       |\n",
    "| 0.32924  | -4.45520 | 4.5718   | -0.98880 | 0       |\n",
    "| ...      | ...      | ...      | ...      | ...     |\n",
    "| 0.40614  | 1.34920  | -1.4501  | -0.55949 | 1       |\n",
    "| -1.38870 | -4.87730 | 6.4774   | 0.34179  | 1       |\n",
    "| -3.75030 | -13.45860| 17.5932  | -2.77710 | 1       |\n",
    "| -3.56370 | -8.38270 | 12.3930  | -1.28230 | 1       |\n",
    "| -2.54190 | -0.65804 | 2.6842   | 1.19520  | 1       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      variance  skewness  curtosis  entropy  targets\n",
      "0      3.62160   8.66610   -2.8073 -0.44699        0\n",
      "1      4.54590   8.16740   -2.4586 -1.46210        0\n",
      "2      3.86600  -2.63830    1.9242  0.10645        0\n",
      "3      3.45660   9.52280   -4.0112 -3.59440        0\n",
      "4      0.32924  -4.45520    4.5718 -0.98880        0\n",
      "...        ...       ...       ...      ...      ...\n",
      "1367   0.40614   1.34920   -1.4501 -0.55949        1\n",
      "1368  -1.38870  -4.87730    6.4774  0.34179        1\n",
      "1369  -3.75030 -13.45860   17.5932 -2.77710        1\n",
      "1370  -3.56370  -8.38270   12.3930 -1.28230        1\n",
      "1371  -2.54190  -0.65804    2.6842  1.19520        1\n",
      "\n",
      "[1372 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "    # fetch dataset \n",
    "    banknote_authentication = fetch_ucirepo(id=267)\n",
    "    # construct the datafram\n",
    "    data = banknote_authentication[\"data\"][\"features\"]\n",
    "    data[\"targets\"] = banknote_authentication[\"data\"][\"targets\"]\n",
    "    print(data)\n",
    "except:\n",
    "    FFILE = './data_banknote_authentication.txt'\n",
    "    if os.path.isfile(FFILE): \n",
    "        print(\"File already exists\")\n",
    "        if os.access(FFILE, os.R_OK):\n",
    "            print (\"File is readable\")\n",
    "        else:\n",
    "            print (\"File is not readable\")\n",
    "    else:\n",
    "        print(\"Either the file is missing or not readable, download it and extract the zip\")\n",
    "        !wget \"https://archive.ics.uci.edu/static/public/267/banknote+authentication.zip\"\n",
    "    \n",
    "    # Load the data\n",
    "\n",
    "    column_names = [\"variance\", \"skewness\", \"curtosis\", \"entropy\", \"targets\"]\n",
    "    data = pd.read_csv('./data_banknote_authentication.txt', names=column_names)\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(sample, features): (1372, 5)\n",
      "First class count: 762\n",
      "Second class count: 610\n"
     ]
    }
   ],
   "source": [
    "print(\"(sample, features):\", data.shape)\n",
    "print(\"First class count:\", sum(data['targets']==0))\n",
    "print(\"Second class count:\", sum(data['targets']==1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **1.2 Scaling, Normalization and Sorting Issues in the Dataset** \n",
    "The dataset consists of numerical features, but their values are on different scales. To ensure proper model training and clustering, **feature scaling** was applied using standardization (z-score normalization). I.e.\n",
    "- Max of all features = 17.9274 while the Min is -13.7731\n",
    "\n",
    "The data was immediately split into train and test sets, rescaled, and then reassembled for models that require the full dataset.\n",
    "\n",
    "full dataset randomized and scaled: [1372 rows x 5 columns]\n",
    "| variance | skewness | curtosis | entropy  | targets |\n",
    "|----------|----------|----------|----------|---------|\n",
    "| 0.904618 | 1.601126 | -1.265374 | -1.495569 | 0.0     |\n",
    "| 1.532814 | -0.691013 | -0.000450 | 0.973356 | 0.0     |\n",
    "| -0.367168 | -1.662094 | 1.257462 | 0.697353 | 1.0     |\n",
    "| -2.299623 | 1.344148 | -0.419396 | -2.767430 | 1.0     |\n",
    "| -0.539056 | -0.520896 | 0.148416 | 0.520688 | 1.0     |\n",
    "| ...      | ...      | ...      | ...      | ...     |\n",
    "| 0.706408 | 0.908746 | -0.465262 | 0.769656 | 0.0     |\n",
    "| 1.130878 | 0.958700 | -0.751494 | 0.639514 | 0.0     |\n",
    "| -1.804741 | 0.344855 | -0.217882 | -0.196042 | 1.0     |\n",
    "| -0.369069 | -0.631649 | -0.471420 | 0.563440 | 1.0     |\n",
    "| 1.394944 | -1.047881 | 0.753370 | 1.083987 | 0.0     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = data.iloc[:,:-1].values\n",
    "y_scaled = data.iloc[:,4].values\n",
    "N = X.shape[0]\n",
    "nc = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line splits the dataset into training and test sets\n",
    "X_train, X_test, train_y, test_y = train_test_split(X, y_scaled, test_size=0.728863, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line Ensures all features have a mean of 0 and variance of 1\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Scale training data\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contruct the dataframes \n",
    "# A subset of 372 elements was set aside as the test set, \n",
    "# the remaining data was used for training models \n",
    "# all of which to be used in supervised learning. \n",
    "combined_train = np.hstack((X_train_scaled, train_y.reshape(-1, 1)))\n",
    "data_scaled_train = pd.DataFrame(combined_train, columns=data.columns.tolist())\n",
    "\n",
    "combined_test = np.hstack((X_test_scaled, test_y.reshape(-1, 1)))\n",
    "data_scaled_test = pd.DataFrame(combined_test, columns=data.columns.tolist())\n",
    "\n",
    "full_data_scaled = pd.concat([data_scaled_train, data_scaled_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature after randomizing\n",
    "X_scaled = np.array(full_data_scaled.iloc[:, :-1])\n",
    "y_scaled = np.array(full_data_scaled.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trasform from column to row\n",
    "train_y = train_y.ravel()\n",
    "test_y = test_y.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2. Unsupervised Learning**  \n",
    "\n",
    "### **2.1 PCA for Visualization**  \n",
    "**Principal Component Analysis (PCA)** was applied to reduce the dataset to two dimensions for visualization. The first two principal components were plotted, with points colored by their actual class labels.\n",
    "\n",
    "**Observations (from plot):**  \n",
    "- The classes are **not linearly separable** in this reduced space.  \n",
    "- Some overlap between clusters suggests that linear models might struggle with classification.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Principal Component Analysis (PCA) using sklearn\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fitting the PCA model to the scaled data\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Transforming the original data to the principal components\n",
    "projection = pca.transform(X_scaled)\n",
    "\n",
    "# Calculating the cumulative explained variance ratio\n",
    "cumulative_variance = np.zeros(nc)\n",
    "for i in range(nc):\n",
    "    cumulative_variance[i] = np.sum(pca.explained_variance_ratio_[:i+1])\n",
    "\n",
    "# Extracting eigenvalues and component indices\n",
    "eigenvector = pca.components_\n",
    "eigenvalues = pca.explained_variance_\n",
    "components = np.arange(nc) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(projection[:, 0], projection[:, 1], c=y_scaled)\n",
    "ax.set_title('2D PCA Visualization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 K-Means Clustering**  \n",
    "**K-Means clustering** was applied with `k=2` (assuming two clusters).  \n",
    "\n",
    "**Results:**  \n",
    "- When using **only the first two PCA components**, k-means **misclassified several points**, showing that a 2D projection may not contain enough information.  \n",
    "- When using **all features**, clustering improved but misclassifications persisted.   \n",
    "\n",
    "We also obtain the following table:\n",
    "| Metric        | 0.0  | 1.0  | Macro Avg | Weighted Avg | Accuracy |\n",
    "|--------------|------|------|-----------|--------------|----------|\n",
    "| Precision    | 0.50 | 0.38 | 0.44      | 0.45         |   -   |\n",
    "| Recall       | 0.46 | 0.42 | 0.44      | 0.44         |    -      |\n",
    "| F1-Score     | 0.48 | 0.40 | 0.44      | 0.44         |    -      |\n",
    "| Support      | 762  | 610  | 1372      | 1372         |   -       |\n",
    "| Accuracy      | -  | -  | -      | -         |   0.44       |\n",
    "\n",
    "From which:\n",
    "1. Precision\n",
    "    - For class 0.0, the model's precision is 0.50, meaning that when it predicts class 0, it is correct 50% of the time.\n",
    "    - For class 1.0, the precision is 0.38, so when it predicts class 1, it is correct 38% of the time.\n",
    "\n",
    "2. Recall:\n",
    "    - For class 0.0, recall is 0.46, meaning the model correctly identifies 46% of actual class 0 instances.\n",
    "    - For class 1.0, recall is 0.42, meaning it correctly identifies 42% of actual class 1 instances.\n",
    "\n",
    "3. F1-Score:\n",
    "    - For class 0.0, the F1-score is 0.48, indicating a balance between precision and recall.\n",
    "    - For class 1.0, the F1-score is 0.40, showing lower performance in predicting this class.\n",
    "\n",
    "The model correctly classifies 44% of the total samples.\n",
    "\n",
    "comparing it to the table that takes the full data set:\n",
    "| Metric        | 0.0  | 1.0  | Accuracy | Macro Avg | Weighted Avg |\n",
    "|--------------|------|------|----------|-----------|--------------|\n",
    "| Precision    | 0.61 | 0.50 | -     | 0.56      | 0.56         |\n",
    "| Recall       | 0.55 | 0.57 |   -   | 0.56      | 0.56         |\n",
    "| F1-Score     | 0.58 | 0.53 |   -   | 0.56      | 0.56         |\n",
    "| Support      | 762  | 610  |   -   | 1372      | 1372         |\n",
    "| Accuracy      | -  | -  | 0.56      | -         |   -       |\n",
    "\n",
    "From which it is clear that PCA results in information loss as all metrics increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the pairwise Euclidean distance matrix between unique rows of the scaled dataset\n",
    "unique_scaled_data = np.unique(X_scaled, axis=0)\n",
    "distance_matrix = cdist(unique_scaled_data, unique_scaled_data)\n",
    "distance_matrix.sort(axis=1)\n",
    "\n",
    "# Calculate the ratio of the second-nearest neighbor distance to the nearest neighbor distance\n",
    "mu_i = np.divide(distance_matrix[:, 2], distance_matrix[:, 1])\n",
    "\n",
    "# Compute the logarithm of the calculated ratios\n",
    "log_mu_i = np.log(mu_i)\n",
    "\n",
    "# Calculate the inverse of the mean logarithm of the second-nearest neighbor ratios\n",
    "two_nn = 1 / np.mean(log_mu_i)\n",
    "\n",
    "# Print the resulting value\n",
    "print(\"Two-Nearest-Neighbor (2NN) Distance Measure:\", two_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_internal(k, X, init):\n",
    "    '''\n",
    "    Perform k-means clustering.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of clusters.\n",
    "    X : matrix of dimension N x D\n",
    "        Dataset.\n",
    "    init : str, {'++', 'random'}\n",
    "        Type of initialization for k-means algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        z_new : array\n",
    "            Cluster assignments for each data point.\n",
    "        L : float\n",
    "            Final value of the k-means objective function (loss).\n",
    "        niter : int\n",
    "            Number of iterations performed.\n",
    "    '''\n",
    "    N = X.shape[0]  # number of points\n",
    "    nc = X.shape[1]  # number of coordinates\n",
    "    ll = np.arange(k)\n",
    "    z = np.zeros(N, dtype='int')  # cluster number assigned to each data point\n",
    "    cent = np.zeros([k, nc])  # coordinates of the cluster centers\n",
    "\n",
    "    # k-means++\n",
    "    if init == '++':\n",
    "        b = np.random.choice(N, 1, replace=False)  # choose the first cluster center at random\n",
    "        cent[0, :] = X[b, :]\n",
    "        nchosen = 1  # number of cluster centers already set\n",
    "\n",
    "        while nchosen < k:\n",
    "            dist = cdist(cent[:nchosen, :], X)  # distance of each point from the cluster centers\n",
    "            dmin = np.min(dist, axis=0)  # min distance between point and cluster centers\n",
    "            prob = dmin**2\n",
    "            prob = prob / np.sum(prob)\n",
    "\n",
    "            # choose next center according to the computed prob\n",
    "            b = np.random.choice(N, 1, replace=False, p=prob)\n",
    "            cent[nchosen, :] = X[b, :]\n",
    "            nchosen += 1\n",
    "\n",
    "    # random initialization\n",
    "    else:\n",
    "        b = np.random.choice(N, k, replace=False)  # choose the k centers randomly\n",
    "        for i in ll:\n",
    "            cent[i, :] = X[b[i], :]\n",
    "\n",
    "    dist = cdist(cent, X)  # distance of each point from cluster centers\n",
    "    z_new = np.argmin(dist, axis=0)  # assign each point to cluster with the closest center\n",
    "    dmin = np.min(dist, axis=0)\n",
    "    niter = 0\n",
    "    L = np.sum(dmin**2)  # loss function evaluation\n",
    "\n",
    "    while (z_new != z).any():  # until a stable configuration is reached\n",
    "        z = np.copy(z_new)\n",
    "\n",
    "        for i in range(k):\n",
    "            cent[i, :] = np.average(X[z == i, :], axis=0)  # compute cluster centroids\n",
    "\n",
    "        dist = cdist(cent, X)  # update distances from cluster centers\n",
    "        z_new = np.argmin(dist, axis=0)  # find cluster with the minimum centroid distance\n",
    "        dmin = np.min(dist, axis=0)\n",
    "        L = np.sum(dmin**2)  # loss function evaluation\n",
    "        niter += 1\n",
    "\n",
    "    return z_new, L, niter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(k, X, init='++', n_init=20):\n",
    "    '''\n",
    "    Perform k-means clustering with multiple initializations to find the best result.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        Number of clusters.\n",
    "    X : matrix of dimension N x D\n",
    "        Dataset.\n",
    "    init : str, {'++', 'random'}, optional\n",
    "        Type of initialization for k-means algorithm.\n",
    "    n_init : int, optional\n",
    "        Number of runs of the algorithm with different initializations.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        labels_opt : array\n",
    "            Cluster assignments for each data point in the best-performing iteration.\n",
    "        lmin : float\n",
    "            Loss (objective function) for the best-performing iteration.\n",
    "    '''\n",
    "    lmin = float('inf')  # Initialize with a large value\n",
    "    labels_opt = None\n",
    "\n",
    "    for i in range(n_init):\n",
    "        # Run k-means for each initialization\n",
    "        labels, loss, niter = k_means_internal(k, X, init=init)\n",
    "\n",
    "        # Check if the current iteration has a lower loss\n",
    "        if loss < lmin:\n",
    "            lmin = loss\n",
    "            labels_opt = labels\n",
    "\n",
    "    return labels_opt, lmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels, l_kmeans = k_means(2, projection, init='++', n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(projection[:,0], projection[:,1], c=kmeans_labels)\n",
    "plt.show()\n",
    "print(\"clustering comparison: \", normalized_mutual_info_score(kmeans_labels, y_scaled.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k-means + pca with n=2: \\n\", classification_report(y_scaled, kmeans_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_labels, l_kmeans = k_means(2, np.array(full_data_scaled.iloc[:, :-1]), init='++', n_init=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(np.array(full_data_scaled.iloc[:, 0]), np.array(full_data_scaled.iloc[:, 1]), c=kmeans_labels)\n",
    "plt.show()\n",
    "print(\"clustering comparison: \", normalized_mutual_info_score(kmeans_labels, y_scaled.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"k-means with n=2: \\n\", classification_report(y_scaled, kmeans_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 t-SNE for Nonlinear Projection**  \n",
    "We used **t-SNE** for dimensionality reduction and visualized the data in 2D.  \n",
    "\n",
    "**Observations:**  \n",
    "- t-SNE provided a **better separation** than PCA, suggesting some non-linear class structure.  \n",
    "- The class distributions are still somewhat mixed, indicating potential challenges for clustering algorithms.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the data into 2D using t-SNE\n",
    "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', perplexity=15, random_state=42).fit_transform(X_scaled)\n",
    "\n",
    "# Create a scatter plot of the embedded data, colored by ground truth labels\n",
    "fig, ax = plt.subplots(figsize=(7, 7))\n",
    "ax.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y_scaled)\n",
    "\n",
    "# Set plot title\n",
    "ax.set_title('t-SNE Visualization')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 DBSCAN Clustering**  \n",
    "We applied **DBSCAN**, a density-based clustering algorithm.  \n",
    "\n",
    "**Results:**   \n",
    "- It identified core clusters but also **classified some points as noise**.  \n",
    "- The results depended significantly on hyperparameters `eps` and `min_samples`, of which eps is based on `n_neighbors` = 50.\n",
    "\n",
    "### DBSCAN Metrics with noise removed\n",
    "\n",
    "| Cluster | Precision | Recall | F1-Score | Support | **Accuracy** |\n",
    "|---------|-----------|--------|----------|---------|---------|\n",
    "| **0.0** | 1.00     | 0.83   | 0.91     | 521     | -  |\n",
    "| **1.0** | 0.96     | 0.80   | 0.88     | 369     |-     |\n",
    "| **2.0** | 0.00     | 0.00   | 0.00     | 0       |-     |\n",
    "| **3.0** | 0.00     | 0.00   | 0.00     | 0       |-     |\n",
    "| **Accuracy**  | -  | -  | - | - |**0.82**     |\n",
    "| **Macro Avg** | 0.49 | 0.41 | 0.45 | 890 |-     |\n",
    "| **Weighted Avg** | 0.99 | 0.82 | 0.90 | 890 |-     |\n",
    "\n",
    "From which:\n",
    "The table shows three clusters (0.0, 1.0, and 2.0), but clusters 2.0 and 3.0 have zero support, meaning no data points were assigned to them.\n",
    "The majority of the data points are assigned to clusters 0.0 and 1.0.\n",
    "\n",
    "1. Precision:\n",
    "    - Cluster 0.0 has a precision of 1.00, meaning all points assigned to this cluster were correctly grouped (no false positives).\n",
    "    - Cluster 1.0 has a precision of 0.96, indicating that most points were correctly assigned, but a few may have been misclassified.\n",
    "\n",
    "2. Recall:\n",
    "    - Cluster 0.0 has a recall of 0.83, meaning 83% of the actual members of this cluster were successfully identified.\n",
    "    - Cluster 1.0 has a recall of 0.80, meaning 80% of the actual points belonging to this cluster were captured.\n",
    "    - Since DBSCAN removes noise points, recall is slightly lower, as some valid points may have been left unclustered.\n",
    "\n",
    "3. F1-Score:\n",
    "    - Cluster 0.0: 0.91 (high, meaning both precision and recall are strong).\n",
    "    - Cluster 1.0: 0.88 (also high, but slightly lower than cluster 0.0).\n",
    "\n",
    "And given accuracy of 0.82, we have that 82% of points were correctly assigned to their respective clusters.\n",
    "\n",
    "- Macro Average: The unweighted mean of precision, recall, and F1-score across clusters. Since clusters 2.0 and 3.0 have zero support, their presence lowers the macro average.\n",
    "- Noise points were removed, improving the accuracy but slightly lowering recall (since some actual points were left out).\n",
    "- Weighted Average: Averages the scores while considering the number of points in each cluster. The weighted values are high because the meaningful clusters (0.0 and 1.0) have strong performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This piece of code means it will find the 50 nearest neighbors for each data point from which we can infer epsilon\n",
    "neighbors = NearestNeighbors(n_neighbors=50)\n",
    "neighbors_fit = neighbors.fit(X_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X_scaled)\n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# First plot: Full range\n",
    "axs[0].plot(distances)\n",
    "axs[0].set_title(\"Full Range\")\n",
    "axs[0].set_xlabel(\"Index\")\n",
    "axs[0].set_ylabel(\"Distances\")\n",
    "\n",
    "# Second plot: Limited x-axis\n",
    "axs[1].plot(distances)\n",
    "axs[1].set_xlim(1360, 1380)\n",
    "axs[1].set_title(\"Limited X-Range [1360, 1380]\")\n",
    "axs[1].set_xlabel(\"Index\")\n",
    "axs[1].set_ylabel(\"Distances\")\n",
    "\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply DBSCAN clustering algorithm to the scaled data\n",
    "db = DBSCAN(eps=0.6, min_samples=50)\n",
    "dbscan = db.fit(X_scaled)\n",
    "\n",
    "# Visualize the clusters in a 3D scatter plot using the first three Principal Components\n",
    "fig = plt.figure(figsize=(9, 9))\n",
    "ax = fig.add_subplot()\n",
    "ax.scatter(projection[:, 0], projection[:, 1], c=dbscan.labels_)\n",
    "\n",
    "# Display the 3D plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **3. Supervised Learning**  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3.1 Logistic Regression** \n",
    "- From the model we find a high accuracy of 0.9770 and by analyzing the confusion matrix, it is observed that the model makes incorrect predictions for only 23 out of 1372 instances.\n",
    "- Evaluating the effect of **regularization** using cross-validation finds the best parameter as.\n",
    "\n",
    "| Metric      | Best Score | Parameters                          |\n",
    "|------------|-----------|------------------------------------|\n",
    "| Accuracy   | 0.9900    | {'penalty': 'l1', 'C': 2.1544}   |\n",
    "| Precision  | 0.9901    | {'penalty': 'l1', 'C': 10.0}     |\n",
    "| Recall     | 0.9906    | {'penalty': 'l1', 'C': 2.1544}   |\n",
    "| F1-Score   | 0.9898    | {'penalty': 'l1', 'C': 2.1544}   |\n",
    "\n",
    "- So we have that the logistic model performed well using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.977022977022977\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Train (fit) the model\n",
    "log_reg.fit(X_train_scaled, train_y.ravel())\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(test_y.ravel(), y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix that shows how many are predicted correctly\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different regularization types\n",
    "penalties = ['l1', 'l2', 'elasticnet']\n",
    "C_values = np.logspace(-3, 3, 10)  \n",
    "l1_ratios = np.linspace(0.1, 0.9, 5)  \n",
    "\n",
    "# Store best scores\n",
    "best_metrics = {'accuracy': 0, 'precision': 0, 'recall': 0, 'f1': 0}\n",
    "best_params = {}\n",
    "\n",
    "for pen in penalties:\n",
    "    for C in C_values:\n",
    "        if pen == 'elasticnet':\n",
    "            for l1_ratio in l1_ratios:\n",
    "                log_reg = LogisticRegression(C=C, penalty=pen, solver='saga', l1_ratio=l1_ratio, max_iter=15000)\n",
    "                log_reg.fit(X_train_scaled, train_y)\n",
    "                y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "                # Compute metrics\n",
    "                report = classification_report(test_y, y_pred, output_dict=True, zero_division=0)\n",
    "                acc = report['accuracy']\n",
    "                prec = report['macro avg']['precision']\n",
    "                rec = report['macro avg']['recall']\n",
    "                f1 = report['macro avg']['f1-score']\n",
    "\n",
    "                # Update best metrics\n",
    "                for metric, value in zip(['accuracy', 'precision', 'recall', 'f1'], [acc, prec, rec, f1]):\n",
    "                    if value > best_metrics[metric]:\n",
    "                        best_metrics[metric] = value\n",
    "                        best_params[metric] = {'penalty': pen, 'C': C, 'l1_ratio': l1_ratio}\n",
    "\n",
    "        else:\n",
    "            log_reg = LogisticRegression(C=C, penalty=pen, solver='liblinear', max_iter=15000)\n",
    "            log_reg.fit(X_train_scaled, train_y)\n",
    "            y_pred = log_reg.predict(X_test_scaled)\n",
    "\n",
    "            # Compute metrics\n",
    "            report = classification_report(test_y, y_pred, output_dict=True, zero_division=0)\n",
    "            acc = report['accuracy']\n",
    "            prec = report['macro avg']['precision']\n",
    "            rec = report['macro avg']['recall']\n",
    "            f1 = report['macro avg']['f1-score']\n",
    "\n",
    "            # Update best metrics\n",
    "            for metric, value in zip(['accuracy', 'precision', 'recall', 'f1'], [acc, prec, rec, f1]):\n",
    "                if value > best_metrics[metric]:\n",
    "                    best_metrics[metric] = value\n",
    "                    best_params[metric] = {'penalty': pen, 'C': C}\n",
    "\n",
    "# Print the best values and parameters\n",
    "for metric, value in best_metrics.items():\n",
    "    print(f\"Best {metric}: {value:.4f} (Params: {best_params[metric]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **3.2 Decision Tree (ID3 Algorithm)**  \n",
    "- Greedy algorithm for tree construction.  \n",
    "- Hyperparameters (depth, minimum samples per leaf) were optimized via cross-validation. \n",
    "\n",
    "From the algorithim we get an accuracy of 0.9470 for the training data.\n",
    "\n",
    "Performing cross Validation we get:\n",
    "- Mean Accuracy: 0.9410\n",
    "- Best Accuracy: 0.9599\n",
    "- Best Tree:\n",
    "\n",
    "    - variance <= 0.0827  \n",
    "    - skewness <= -0.2305 → **1.0**  \n",
    "    - skewness > -0.2305  \n",
    "        - skewness <= 0.3693 → **1.0**  \n",
    "        - skewness > 0.3693  \n",
    "        - variance <= -0.7789  \n",
    "            - skewness <= 1.0834 → **1.0**  \n",
    "            - skewness > 1.0834 → **0.0**  \n",
    "        - variance > -0.7789 → **0.0**  \n",
    "    - variance > 0.0827  \n",
    "    - variance <= 0.9059  \n",
    "        - curtosis <= -0.3654  \n",
    "        - skewness <= 0.8130 → **1.0**  \n",
    "        - skewness > 0.8130 → **0.0**  \n",
    "        - curtosis > -0.3654 → **0.0**  \n",
    "    - variance > 0.9059 → **0.0** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute H(S)\n",
    "def entropy(train_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_entr : float\n",
    "        The entropy of the dataset.\n",
    "    \"\"\"\n",
    "    # Get the total number of instances in the dataset\n",
    "    total_row = train_data.shape[0]\n",
    "    # Initialize the total entropy variable\n",
    "    total_entr = 0\n",
    "\n",
    "    # Iterate through each possible class in the label\n",
    "    for c in class_list:\n",
    "        # Count the number of points belonging to the current class\n",
    "        total_class_count = train_data[train_data[label] == c].shape[0]\n",
    "\n",
    "        # Check if there are instances of the class to avoid numerical errors\n",
    "        if total_class_count > 0:\n",
    "            # Calculate the entropy of the current class\n",
    "            total_class_entr = - (total_class_count / total_row) * np.log2(total_class_count / total_row)\n",
    "            # Add the entropy of the current class to the total entropy of the dataset\n",
    "            total_entr += total_class_entr\n",
    "\n",
    "    # Return the calculated total entropy of the dataset\n",
    "    return total_entr\n",
    "\n",
    "# compute H(S_j)\n",
    "def feature_entropy(left_data, right_data, label, class_list):\n",
    "    \"\"\"\n",
    "    Calculate the conditional entropy of a dataset split by a specific feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    left_data : DataFrame\n",
    "        Subset of the dataset where the feature has a specific value.\n",
    "    right_data : DataFrame\n",
    "        Subset of the dataset where the feature has another value.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ent : float\n",
    "        The conditional entropy of the dataset split by the feature.\n",
    "    \"\"\"\n",
    "    # Get the total number of points considered after the split\n",
    "    row_count = left_data.shape[0] + right_data.shape[0]\n",
    "\n",
    "    # Calculate the probabilities of the left and right subsets\n",
    "    p_left = left_data.shape[0] / row_count\n",
    "    p_right = right_data.shape[0] / row_count\n",
    "\n",
    "    # Calculate the conditional entropy using the weighted average of entropies for left and right subsets\n",
    "    ent = p_left * entropy(left_data, label, class_list) + p_right * entropy(right_data, label, class_list)\n",
    "\n",
    "    # Return the calculated conditional entropy\n",
    "    return ent\n",
    "\n",
    "def split_dec_tree(feature_column, threshold):\n",
    "    \"\"\"\n",
    "    Split the indices of data points based on a feature and a threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_column : array-like\n",
    "        The values of the feature for each data point.\n",
    "    threshold : float\n",
    "        The threshold value for splitting the data points.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    left_rows : array-like\n",
    "        Indices of data points where the feature value is less than or equal to the threshold.\n",
    "    right_rows : array-like\n",
    "        Indices of data points where the feature value is greater than the threshold.\n",
    "    \"\"\"\n",
    "    # Find the indices of data points where the feature value is less than or equal to the threshold\n",
    "    left_rows = np.argwhere(feature_column <= threshold).flatten()\n",
    "    # Find the indices of data points where the feature value is greater than the threshold\n",
    "    right_rows = np.argwhere(feature_column > threshold).flatten()\n",
    "\n",
    "    # Return the indices for left and right subsets\n",
    "    return left_rows, right_rows\n",
    "\n",
    "def information_gain(data, feature_name, label, class_list, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the information gain after splitting the dataset based on a feature and a threshold.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        The dataset.\n",
    "    feature_name : str\n",
    "        The name of the feature for which information gain is calculated.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "    threshold : float\n",
    "        The threshold value for splitting the dataset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    feat_information_gain : float\n",
    "        The information gain achieved by splitting the dataset based on the specified feature and threshold.\n",
    "    \"\"\"\n",
    "    # Split the dataset into left and right subsets based on the feature and threshold\n",
    "    left_rows, right_rows = split_dec_tree(data[feature_name].values, threshold)\n",
    "\n",
    "    # Check if either subset is empty; if so, information gain is zero\n",
    "    if len(left_rows) == 0 or len(right_rows) == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate the entropy of the split dataset\n",
    "    feat_entropy = feature_entropy(data.iloc[left_rows], data.iloc[right_rows], label, class_list)\n",
    "\n",
    "    return feat_entropy\n",
    "\n",
    "def get_split_thresholds(feature_column, n_thresholds):\n",
    "    \"\"\"\n",
    "    Generate candidate split thresholds for a given feature column.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_column : array-like\n",
    "        The values of the feature for each data point.\n",
    "    n_thresholds : int\n",
    "        The number of thresholds to generate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    thresholds : list of float\n",
    "        List of candidate split thresholds for the feature column.\n",
    "    \"\"\"\n",
    "    # Extract the values of the feature column\n",
    "    feature_column = feature_column.values\n",
    "    # Get the total number of data points\n",
    "    n_data = len(feature_column)\n",
    "\n",
    "    # Sort the feature column in ascending order\n",
    "    sorted_column = np.sort(feature_column)\n",
    "\n",
    "    # Check if there is more than one data point\n",
    "    if len(feature_column) > 1:\n",
    "        # Split the sorted feature column into n_thresholds + 1 partitions\n",
    "        partitioned_array = np.array_split(sorted_column, n_thresholds + 1)\n",
    "\n",
    "        # Calculate the midpoint between consecutive partitions as candidate thresholds\n",
    "        thresholds = [(partitioned_array[i][-1] + partitioned_array[i + 1][0]) / 2 for i in range(len(partitioned_array) - 1)]\n",
    "    else:\n",
    "        # If there is only one data point, use it as the threshold\n",
    "        thresholds = [feature_column[0]]\n",
    "\n",
    "    # Return the list of candidate split thresholds\n",
    "    return thresholds\n",
    "\n",
    "def most_informative_feature(train_data, label, class_list, n_thresholds):\n",
    "    \"\"\"\n",
    "    Find the most informative feature and its corresponding threshold for splitting the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "    n_thresholds : int\n",
    "        The number of thresholds to generate for each feature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    min_entropy_feature : str\n",
    "        The name of the most informative feature.\n",
    "    min_entropy_threshold : float\n",
    "        The corresponding threshold for splitting the dataset based on the most informative feature.\n",
    "    \"\"\"\n",
    "    # Get the list of features excluding the label\n",
    "    feature_list = train_data.columns.drop(label)\n",
    "\n",
    "    # Initialize variables to store the minimum entropy and corresponding feature and threshold\n",
    "    min_entropy = float('inf')\n",
    "    min_entropy_feature = None\n",
    "    min_entropy_threshold = None\n",
    "\n",
    "    # Iterate over each feature in the feature list\n",
    "    for feature in feature_list:\n",
    "        # Generate candidate split thresholds for the current feature\n",
    "        thresholds = get_split_thresholds(train_data[feature], n_thresholds)\n",
    "\n",
    "        # Iterate over each threshold\n",
    "        for t in thresholds:\n",
    "            # Calculate information gain for the current feature and threshold\n",
    "            info_gain = information_gain(train_data, feature, label, class_list, t)\n",
    "\n",
    "            # Check if the calculated information gain is less than the current minimum entropy\n",
    "            if info_gain < min_entropy:\n",
    "                # Update the minimum entropy and corresponding feature and threshold\n",
    "                min_entropy = info_gain\n",
    "                min_entropy_feature = feature\n",
    "                min_entropy_threshold = t\n",
    "\n",
    "    # Return the most informative feature and its corresponding threshold\n",
    "    return min_entropy_feature, min_entropy_threshold\n",
    "\n",
    "def is_leaf(train_data, label):\n",
    "    \"\"\"\n",
    "    Check if a node in a decision tree is a leaf node.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The dataset associated with the current node.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the node is a leaf node (contains only one class), False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the unique classes in the current node\n",
    "    classes_in_node = np.unique(train_data[label])\n",
    "\n",
    "    # Check if there is only one class in the node\n",
    "    if len(classes_in_node) == 1:\n",
    "        # If there is only one class, the node is a leaf node\n",
    "        return True\n",
    "    else:\n",
    "        # If there is more than one class, the node is not a leaf node\n",
    "        return False\n",
    "    \n",
    "def leaf_class(train_data, label):\n",
    "    \"\"\"\n",
    "    Determine the class of a leaf node in a decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The dataset associated with the leaf node.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    leaf_class : str\n",
    "        The class label assigned to the leaf node.\n",
    "    \"\"\"\n",
    "    # Get the unique classes and their counts in the current leaf node\n",
    "    class_list, count_class = np.unique(train_data[label], return_counts=True)\n",
    "\n",
    "    # Find the index of the class with the highest count (most frequent class)\n",
    "    idx = count_class.argmax()\n",
    "\n",
    "    # Return the class label associated with the most frequent class in the leaf node\n",
    "    return class_list[idx]\n",
    "\n",
    "def make_tree(train_data, label, class_list, n_thresholds, cur_depth, min_samples, max_depth):\n",
    "    \"\"\"\n",
    "    Recursively build a decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset associated with the current node.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    class_list : list of str\n",
    "        List of possible values of the class labels.\n",
    "    n_thresholds : int\n",
    "        The number of thresholds to generate for each feature.\n",
    "    cur_depth : int\n",
    "        The current depth of the decision tree.\n",
    "    min_samples : int\n",
    "        The minimum number of samples required to split a node.\n",
    "    max_depth : int\n",
    "        The maximum depth of the decision tree.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tree : dict or str\n",
    "        The constructed decision tree represented as a nested dictionary. If a leaf node, returns the class label.\n",
    "    \"\"\"\n",
    "    # Check stopping conditions for creating a leaf node\n",
    "    if is_leaf(train_data, label) or cur_depth >= max_depth or len(train_data) <= min_samples:\n",
    "        return leaf_class(train_data, label)\n",
    "    else:\n",
    "        # Increment the current depth for the next level of recursion\n",
    "        cur_depth += 1\n",
    "\n",
    "        # Find the most informative feature and its corresponding threshold for splitting\n",
    "        split_feature, split_threshold = most_informative_feature(train_data, label, class_list, n_thresholds)\n",
    "\n",
    "        # Split the dataset into left and right subsets based on the feature and threshold\n",
    "        left_rows, right_rows = split_dec_tree(train_data[split_feature].values, split_threshold)\n",
    "\n",
    "        # Check if either subset is empty; if so, create a leaf node\n",
    "        if len(left_rows) == 0 or len(right_rows) == 0:\n",
    "            return leaf_class(train_data, label)\n",
    "        else:\n",
    "            # Build the subtree\n",
    "            split_condition = \"{} <= {}\".format(split_feature, split_threshold)\n",
    "            sub_tree = {split_condition: []}\n",
    "\n",
    "            # Recursive calls for the left and right branches\n",
    "            left_branch = make_tree(train_data.iloc[left_rows], label, class_list, n_thresholds, cur_depth, min_samples, max_depth)\n",
    "            right_branch = make_tree(train_data.iloc[right_rows], label, class_list, n_thresholds, cur_depth, min_samples, max_depth)\n",
    "\n",
    "            # Check if both branches result in the same leaf class; if so, make the subtree a leaf\n",
    "            if left_branch == right_branch:\n",
    "                sub_tree = left_branch\n",
    "            else:\n",
    "                # Grow the tree by adding left and right branches to the split condition\n",
    "                sub_tree[split_condition].append(left_branch)\n",
    "                sub_tree[split_condition].append(right_branch)\n",
    "\n",
    "            return sub_tree\n",
    "        \n",
    "def id3(train_data_m, label, n_thresholds=1, min_samples=4, max_depth=5):\n",
    "    \"\"\"\n",
    "    Build a decision tree using the ID3 algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data_m : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    n_thresholds : int, optional\n",
    "        The number of thresholds to generate for each feature.\n",
    "    min_samples : int, optional\n",
    "        The minimum number of samples required to split a node.\n",
    "    max_depth : int, optional\n",
    "        The maximum depth of the decision tree.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tree : dict or str\n",
    "        The constructed decision tree represented as a nested dictionary. If a leaf node, returns the class label.\n",
    "    \"\"\"\n",
    "    # Create a copy of the training dataset\n",
    "    train_data = train_data_m.copy()\n",
    "\n",
    "    # Get the unique classes of the label\n",
    "    class_list = train_data[label].unique()\n",
    "\n",
    "    # Start the recursion by calling the make_tree function\n",
    "    tree = make_tree(train_data, label, class_list, n_thresholds, 0, min_samples, max_depth)\n",
    "\n",
    "    # Return the constructed decision tree\n",
    "    return tree\n",
    "\n",
    "def predict_dec_tree(test_point, tree):\n",
    "    \"\"\"\n",
    "    Predict the class label for a given test point using a decision tree.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_point : Series\n",
    "        The test point for which the class label is predicted.\n",
    "    tree : dict or str\n",
    "        The decision tree used for prediction.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    prediction : str\n",
    "        The predicted class label for the test point.\n",
    "    \"\"\"\n",
    "    # Base case: if the tree is a leaf node (a class label)\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "\n",
    "    # Recursive case: traverse the tree based on feature values\n",
    "    question = list(tree.keys())[0]\n",
    "    attribute, value = question.split(\" <= \")\n",
    "\n",
    "    # Check the condition and follow the appropriate branch\n",
    "    if test_point[attribute] <= float(value):\n",
    "        answer = tree[question][0]\n",
    "    else:\n",
    "        answer = tree[question][1]\n",
    "\n",
    "    # Recursive call on the selected branch\n",
    "    return predict_dec_tree(test_point, answer)\n",
    "\n",
    "def evaluate_dec_tree(tree, test_data, label):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of a decision tree on a test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tree : dict or str\n",
    "        The decision tree to be evaluated.\n",
    "    test_data : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        The accuracy of the decision tree on the test dataset.\n",
    "    \"\"\"\n",
    "    correct_predict = 0\n",
    "    wrong_predict = 0\n",
    "\n",
    "    # Iterate over each row in the test dataset\n",
    "    for index in tqdm(range(len(test_data.index))):\n",
    "        # Predict the class label for the current test point\n",
    "        result = predict_dec_tree(test_data.iloc[index], tree)\n",
    "\n",
    "        # Check if the predicted value matches the expected value\n",
    "        if result == test_data[label].iloc[index]:\n",
    "            correct_predict += 1  # Increase correct count\n",
    "        else:\n",
    "            wrong_predict += 1  # Increase incorrect count\n",
    "\n",
    "    # Calculate and return the accuracy\n",
    "    accuracy = correct_predict / (correct_predict + wrong_predict)\n",
    "    return accuracy\n",
    "\n",
    "def cross_validate_id3(full_data, label, k=5, n_thresholds=1, min_samples=4, max_depth=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for the ID3 decision tree algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    full_data : DataFrame\n",
    "        The full dataset containing feature columns and the target label.\n",
    "    label : str\n",
    "        The column name representing the class labels.\n",
    "    k : int, optional\n",
    "        Number of folds for cross-validation (default is 5).\n",
    "    n_thresholds : int, optional\n",
    "        The number of thresholds to generate for each feature.\n",
    "    min_samples : int, optional\n",
    "        The minimum number of samples required to split a node.\n",
    "    max_depth : int, optional\n",
    "        The maximum depth of the decision tree.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_tree : dict or str\n",
    "        The best-performing decision tree model.\n",
    "    best_accuracy : float\n",
    "        The highest accuracy achieved across the k-folds.\n",
    "    \"\"\"\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    best_tree = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(full_data):\n",
    "        train_data = full_data.iloc[train_index]\n",
    "        test_data = full_data.iloc[test_index]\n",
    "\n",
    "        # Train the decision tree using ID3\n",
    "        tree = id3(train_data, label, n_thresholds, min_samples, max_depth)\n",
    "\n",
    "        # Evaluate the decision tree\n",
    "        accuracy = evaluate_dec_tree(tree, test_data, label)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Save the best tree model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_tree = tree\n",
    "\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    print(f\"Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "\n",
    "    return best_tree, best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = id3(data_scaled_train, 'targets')\n",
    "evaluate_dec_tree(tree, data_scaled_test, 'targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_id3(full_data_scaled, 'targets', k=5, n_thresholds=1, min_samples=4, max_depth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **3.3 Naive Bayes Classifier**  \n",
    "- Assumes feature independence.  \n",
    "- Performed well in some cases but had lower accuracy due to its strong assumptions.\n",
    "\n",
    "From the Gaussian Naive Bayes Classifier we get 0.8421 accuracy and performing cross-validation we get:\n",
    "- Mean Accuracy: 0.8382\n",
    "- Best Accuracy: 0.8978\n",
    "\n",
    "- Best parameters:\n",
    " \n",
    "| Parameter       | Value |\n",
    "|----------------|----------------------------------------------------------------------------------|\n",
    "| n_labels       | 2 |\n",
    "| unique_labels  | [0., 1.] |\n",
    "| n_classes      | 2 |\n",
    "| mean           | [[ 0.7461,  0.4655, -0.2144, -0.0412,  0. ], [-0.6915, -0.4813,  0.1274, -0.0640,  1. ]] |\n",
    "| variance       | [[0.5237, 0.8054, 0.6522, 1.0811, 1e-9], [0.4388, 0.9101, 1.7591, 0.9921, 1e-9]] |\n",
    "| prior          | [-0.6161, -0.7767] |\n",
    "| Score          | 0.8978 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_gauss_bayes(train_data, label):\n",
    "    \"\"\"\n",
    "    Calculate the log prior probabilities for each class in the dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    priors : array-like\n",
    "        The log prior probabilities for each class.\n",
    "    \"\"\"\n",
    "    # Calculate the prior probabilities for each class\n",
    "    priors = train_data.groupby(by=label).apply(lambda x: len(x) / len(train_data))\n",
    "\n",
    "    # Return the log of the prior probabilities as an array\n",
    "    return np.log(priors).values\n",
    "\n",
    "\n",
    "def mean_variance(train_data, label):\n",
    "    \"\"\"\n",
    "    Calculate the mean and variance for each feature in the dataset, grouped by class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    mean : array-like\n",
    "        The mean values for each feature and class.\n",
    "    variance : array-like\n",
    "        The variance values for each feature and class.\n",
    "    \"\"\"\n",
    "    # Calculate the mean values for each feature and class\n",
    "    mean = train_data.groupby(by=label).apply(lambda x: x.mean(axis=0))\n",
    "\n",
    "    # Calculate the variance values for each feature and class\n",
    "    variance = train_data.groupby(by=label).apply(lambda x: x.var(axis=0))\n",
    "\n",
    "    # Return the mean and variance as arrays\n",
    "    return (mean.values, variance.values + 1e-9)\n",
    "\n",
    "\n",
    "def gaussian_density(mean, variance, point):\n",
    "    \"\"\"\n",
    "    Calculate the Gaussian probability density for a given point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : array-like\n",
    "        The mean values for each feature and class.\n",
    "    variance : array-like\n",
    "        The variance values for each feature and class.\n",
    "    point : array-like\n",
    "        The values of the features for a given point.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    density : array-like\n",
    "        The Gaussian probability density for the given point.\n",
    "    \"\"\"\n",
    "    # Calculate the Gaussian probability density for each feature\n",
    "    d = (1 / np.sqrt(2*np.pi*variance)) * np.exp((-(point - mean)**2) / (2*variance))\n",
    "\n",
    "    # Return the density as an array\n",
    "    return d\n",
    "\n",
    "\n",
    "def train_gaussian_naive_bayes(train_data, label):\n",
    "    \"\"\"\n",
    "    Train a Gaussian Naive Bayes classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_data : DataFrame\n",
    "        The training dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : dict\n",
    "        A dictionary containing the parameters of the trained Gaussian Naive Bayes model.\n",
    "    \"\"\"\n",
    "    # Calculate the mean and variance for each feature and class\n",
    "    mean, variance = mean_variance(train_data, label)\n",
    "\n",
    "    # Calculate the log prior probabilities for each class\n",
    "    priors = prior_gauss_bayes(train_data, label)\n",
    "\n",
    "    # Get unique class labels and their count\n",
    "    unique_labels = train_data[label].unique()\n",
    "    n_labels = len(unique_labels)\n",
    "\n",
    "    # Construct and return the Gaussian Naive Bayes model\n",
    "    return {'n_labels': n_labels, 'unique_labels': unique_labels, 'n_classes': n_labels, 'mean': mean,\n",
    "            'variance': variance, 'prior': priors}\n",
    "\n",
    "def posterior_gauss_bayes(point, mean, variance, class_list, n_classes, n_feat):\n",
    "    \"\"\"\n",
    "    Calculate the log posterior probabilities for each class given a data point.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point : array-like\n",
    "        The values of the features for a given data point.\n",
    "    mean : array-like\n",
    "        The mean values for each feature and class.\n",
    "    variance : array-like\n",
    "        The variance values for each feature and class.\n",
    "    class_list : array-like\n",
    "        The unique class labels.\n",
    "    n_classes : int\n",
    "        The number of classes.\n",
    "    n_feat : int\n",
    "        The number of features.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    posteriors : array-like\n",
    "        The log posterior probabilities for each class.\n",
    "    \"\"\"\n",
    "    posteriors = []\n",
    "    for i in range(n_classes):\n",
    "        posterior = 0\n",
    "        for j in range(n_feat):\n",
    "            posterior += np.log(gaussian_density(mean[i][j], variance[i][j], point[j]))\n",
    "        posteriors.append(posterior)\n",
    "    return posteriors\n",
    "\n",
    "\n",
    "def predict_gauss_bayes(test_data, label, gaus_bayes):\n",
    "    \"\"\"\n",
    "    Predict the class labels for a given test dataset using a trained Gaussian Naive Bayes model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    gaus_bayes : dict\n",
    "        A dictionary containing the parameters of the trained Gaussian Naive Bayes model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array-like\n",
    "        The predicted class labels for the test dataset.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    n_feat = len(test_data.columns) - 1\n",
    "    for i in range(len(test_data)):\n",
    "        pr = gaus_bayes['prior']\n",
    "        post = posterior_gauss_bayes(test_data.iloc[i, :-1], gaus_bayes['mean'], gaus_bayes['variance'],\n",
    "                         gaus_bayes['unique_labels'], gaus_bayes['n_classes'], n_feat)\n",
    "        prob = pr + post\n",
    "        max_prob_class_idx = np.argmax(prob)\n",
    "        predictions.append(gaus_bayes['unique_labels'][max_prob_class_idx])\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def evaluate_gaus_naive_bayes(test_data, label, gaus_bayes):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of a Gaussian Naive Bayes model on a test dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_data : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    gaus_bayes : dict\n",
    "        A dictionary containing the parameters of the trained Gaussian Naive Bayes model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        The accuracy of the Gaussian Naive Bayes model on the test dataset.\n",
    "    \"\"\"\n",
    "    gaus_pred = predict_gauss_bayes(test_data, label, gaus_bayes)\n",
    "    correct_predict = 0\n",
    "    wrong_predict = 0\n",
    "    for index in tqdm(range(len(test_data.index))):\n",
    "        if gaus_pred[index] == test_data[label].iloc[index]:\n",
    "            correct_predict += 1\n",
    "        else:\n",
    "            wrong_predict += 1\n",
    "    accuracy = correct_predict / (correct_predict + wrong_predict)\n",
    "    return accuracy\n",
    "\n",
    "def cross_validate_naive_bayes(full_data, label, k=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation for Gaussian Naïve Bayes using the existing training and evaluation functions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : DataFrame\n",
    "        The full dataset containing feature columns and the target label.\n",
    "    label : str\n",
    "        The column name representing the class labels.\n",
    "    k : int, optional\n",
    "        Number of folds for cross-validation (default is 5).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_model : dict\n",
    "        The best-performing Naïve Bayes model.\n",
    "    best_accuracy : float\n",
    "        The highest accuracy achieved.\n",
    "    \"\"\"\n",
    "\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    best_model = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for train_index, test_index in kf.split(full_data):\n",
    "        train_data = full_data.iloc[train_index]\n",
    "        test_data = full_data.iloc[test_index]\n",
    "\n",
    "        # Train the Gaussian Naïve Bayes model\n",
    "        gaus_bayes = train_gaussian_naive_bayes(train_data, label)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = evaluate_gaus_naive_bayes(test_data, label, gaus_bayes)\n",
    "        accuracies.append(accuracy)\n",
    "\n",
    "        # Save the best model\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_model = gaus_bayes\n",
    "\n",
    "    print(f\"Mean Accuracy: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"Best Accuracy: {best_accuracy:.4f}\")\n",
    "    return best_model, best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaus_bayes = train_gaussian_naive_bayes(data_scaled_train, 'targets')\n",
    "evaluate_gaus_naive_bayes(data_scaled_test, 'targets', gaus_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.concat([data_scaled_test,data_scaled_train])\n",
    "cross_validate_naive_bayes(full_data, 'targets', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### **3.4 k-Nearest Neighbors (k-NN)**  \n",
    "- Hyperparameter `k` was tuned via cross-validation.  \n",
    "- Performed well but computationally expensive.\n",
    "\n",
    "Assuming p = 2 and 5 clusters then we have an accuracy of 0.9900\n",
    "\n",
    "Performing cross-validation we get:\n",
    "- Best Hyperparameters: k=2, distance=euclidean, p=1\n",
    "- Best Cross-Validation Accuracy: 0.9985\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_knn(point_one, point_two, dist, p):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance between two points.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    point_one : array-like\n",
    "        Coordinates of the first point.\n",
    "    point_two : array-like\n",
    "        Coordinates of the second point.\n",
    "    dist: str\n",
    "        Allow to choose between Euclidean or Minkowski distance.\n",
    "    p: int\n",
    "        Order of the norm, only used with Minkowski distance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Euclidean or Minkowski distance between the two points.\n",
    "    \"\"\"\n",
    "    if dist == 'euclidean':\n",
    "        return euclidean(point_one, point_two)\n",
    "    else:\n",
    "        return minkowski(point_one, point_two, p=p)\n",
    "\n",
    "\n",
    "def get_neighbors_knn(train_set, test_point, label_col, n_neighbors, dist, p):\n",
    "    \"\"\"\n",
    "    Get the nearest neighbors of a test point in the training set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : array-like\n",
    "        The training set containing data points.\n",
    "    test_point : array-like\n",
    "        The test point for which neighbors are to be found.\n",
    "    label_col : array-like\n",
    "        The labels corresponding to the training set.\n",
    "    n_neighbors : int\n",
    "        The number of neighbors to retrieve.\n",
    "    dist: str\n",
    "        Allow to choose between Euclidean or Minkowski distance.\n",
    "    p: int\n",
    "        Order of the norm, only used with Minkowski distance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ordered_train : array-like\n",
    "        The nearest neighbors in the training set.\n",
    "    ordered_label : array-like\n",
    "        The corresponding labels of the nearest neighbors.\n",
    "    \"\"\"\n",
    "    # Calculate distances between the test point and all points in the training set\n",
    "    dist = np.array([distance_knn(train_point, test_point, dist, p) for train_point in train_set])\n",
    "    # Get indices that would sort the distances in ascending order\n",
    "    idx_dist = dist.argsort()\n",
    "    # Order the training set and labels based on the sorted distances\n",
    "    ordered_train = train_set[idx_dist, :]\n",
    "    ordered_label = label_col[idx_dist]\n",
    "    # Return the top n_neighbors neighbors and their labels\n",
    "    return ordered_train[:n_neighbors], ordered_label[:n_neighbors]\n",
    "\n",
    "def predict_knn(train_set, test_point, labels, n_neighbors, dist, p):\n",
    "    \"\"\"\n",
    "    Predict the label of a test point using k-nearest neighbors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : array-like\n",
    "        The training set containing data points.\n",
    "    test_point : array-like\n",
    "        The test point for which the label is to be predicted.\n",
    "    labels : array-like\n",
    "        The labels corresponding to the training set.\n",
    "    n_neighbors : int\n",
    "        The number of neighbors to consider for the prediction.\n",
    "    dist: str\n",
    "        Allow to choose between Euclidean or Minkowski distance.\n",
    "    p: int\n",
    "        Order of the norm, only used with Minkowski distance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predicted_label : array-like\n",
    "        The predicted label for the test point.\n",
    "    \"\"\"\n",
    "    # Get the nearest neighbors and their labels\n",
    "    neigh, neigh_label = get_neighbors_knn(train_set, test_point, labels, n_neighbors, dist, p)\n",
    "    # Count occurrences of each label among the neighbors\n",
    "    values, counts = np.unique(neigh_label, return_counts=True)\n",
    "    # Find the label with the highest count (majority class)\n",
    "    idx = np.argmax(counts)\n",
    "    # Return the predicted label\n",
    "    return values[idx]\n",
    "\n",
    "def evaluate_knn(train_set, test_set, label, n_neighbors=2, dist='Euclidean', p=2):\n",
    "    \"\"\"\n",
    "    Evaluate the accuracy of k-nearest neighbors algorithm on a test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_set : DataFrame\n",
    "        The training dataset.\n",
    "    test_set : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The name of the column representing the class labels.\n",
    "    n_neighbors : int, optional\n",
    "        The number of neighbors to consider for the prediction. Default is 2.\n",
    "    dist: str\n",
    "        Allow to choose between Euclidean or Minkowski distance.\n",
    "    p: int\n",
    "        Order of the norm, only used with Minkowski distance.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    accuracy : float\n",
    "        The accuracy of the k-nearest neighbors algorithm on the test set.\n",
    "    \"\"\"\n",
    "    # Initialize counters for correct and incorrect predictions\n",
    "    correct_predict = 0\n",
    "    wrong_predict = 0\n",
    "    # Extract labels and features from the training and test sets\n",
    "    \n",
    "    train_labels = train_set[label].values\n",
    "    train_set = train_set.drop(label, axis=1)\n",
    "    \n",
    "    test_labels = test_set[label].values\n",
    "    test_set = test_set.drop(label, axis=1)\n",
    "    # Iterate through each row in the test dataset\n",
    "    for index in tqdm(range(len(test_set.index))):\n",
    "        # Predict the class label for the current test row\n",
    "        result = predict_knn(train_set.values, test_set.iloc[index].values, train_labels, n_neighbors, dist, p)\n",
    "        # Check if the predicted value matches the actual value\n",
    "        if result == test_labels[index]:\n",
    "            # Increase the correct prediction count\n",
    "            correct_predict += 1\n",
    "        else:\n",
    "            # Increase the incorrect prediction count\n",
    "            wrong_predict += 1\n",
    "\n",
    "    # Calculate and return the accuracy\n",
    "    accuracy = correct_predict / (correct_predict + wrong_predict)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_accuracy = evaluate_knn(data_scaled_train, data_scaled_test, 'targets', n_neighbors=5)\n",
    "knn_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters to test\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "distance_metrics = ['euclidean', 'minkowski']\n",
    "p_values = [1, 2]\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Perform cross-validation for each combination of hyperparameters\n",
    "for k in k_values:\n",
    "    for dist in distance_metrics:\n",
    "        for p in p_values:\n",
    "            accuracies = [\n",
    "                evaluate_knn(\n",
    "                    full_data.iloc[train_idx],\n",
    "                    full_data.iloc[val_idx],\n",
    "                    label='targets',\n",
    "                    n_neighbors=k,\n",
    "                    dist=dist,\n",
    "                    p=p\n",
    "                )\n",
    "                for train_idx, val_idx in kf.split(full_data)\n",
    "            ]\n",
    "            \n",
    "            # Store average accuracy for the current hyperparameters\n",
    "            results.append((k, dist, p, np.mean(accuracies)))\n",
    "\n",
    "# Find best hyperparameter combination\n",
    "best_params = max(results, key=lambda x: x[3])\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"Best Hyperparameters: k={best_params[0]}, distance={best_params[1]}, p={best_params[2]}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {best_params[3]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_data, label, model_type=\"naive_bayes\"):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the test set using accuracy, precision, recall, and F1-score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : dict or None\n",
    "        The trained model (Decision Tree or Naïve Bayes). Not required for KNN.\n",
    "    test_data : DataFrame\n",
    "        The test dataset.\n",
    "    label : str\n",
    "        The column name representing the class labels.\n",
    "    model_type : str\n",
    "        Type of model (\"naive_bayes\", \"decision_tree\", \"knn\").\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None (Prints the evaluation results).\n",
    "    \"\"\"\n",
    "\n",
    "    if model_type == \"naive_bayes\":\n",
    "        # Predict labels using Gaussian Naïve Bayes\n",
    "        predictions = predict_gauss_bayes(test_data, label, model)\n",
    "\n",
    "    elif model_type == \"decision_tree\":\n",
    "        # Predict labels using Decision Tree\n",
    "        predictions = [predict_dec_tree(row, model) for _, row in test_data.iterrows()]\n",
    "\n",
    "    elif model_type == \"knn\":\n",
    "        # Extract labels and features\n",
    "        train_labels = data_scaled_train[label].values\n",
    "        train_features = data_scaled_train.drop(label, axis=1).values\n",
    "        test_labels = test_data[label].values\n",
    "        test_features = test_data.drop(label, axis=1).values\n",
    "\n",
    "        # Predict labels for test set using KNN\n",
    "        predictions = [\n",
    "            predict_knn(train_features, test_features[i], train_labels, n_neighbors=1, dist='minkowski', p=1)\n",
    "            for i in range(len(test_features))\n",
    "        ]\n",
    "\n",
    "    # True labels\n",
    "    y_true = test_data[label].values\n",
    "\n",
    "    # Print classification report\n",
    "    print(f\"Evaluation for {model_type.upper()}:\")\n",
    "    print(classification_report(y_true, predictions, digits=4))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# Train models\n",
    "nb_model = train_gaussian_naive_bayes(data_scaled_train, 'targets')\n",
    "dt_model = id3(data_scaled_train, 'targets')\n",
    "\n",
    "# Evaluate each model\n",
    "evaluate_model(nb_model, data_scaled_test, 'targets', model_type=\"naive_bayes\")\n",
    "evaluate_model(dt_model, data_scaled_test, 'targets', model_type=\"decision_tree\")\n",
    "evaluate_model(None, data_scaled_test, 'targets', model_type=\"knn\")\n",
    "# Logistic model\n",
    "y_pred = log_reg.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(test_y.ravel(), y_pred)\n",
    "print(\"Model Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3.3 Performance Comparison**  \n",
    "Analyzing **accuracy, precision, recall, and F1-score** on the test set for supervised.  \n",
    "\n",
    "| Model                | Accuracy | Precision | Recall | F1-Score | Support |\n",
    "|----------------------|----------|-----------|--------|----------|----------|\n",
    "| Naive Bayes | 0.8422      | **0**: 0.8486 <br> **1**: 0.8329       | **0**: 0.8785 <br> **1**: 0.7945    | **0**: 0.8633 <br> **1**: 0.8132      |**0**: 568 <br> **1**: 433      |\n",
    "| Decision Tree       | 0.9471      | **0**: 0.9813 <br> **1**: 0.9077       | **0**: 0.9243 <br> **1**: 0.9769    | **0**: 0.9519 <br> **1**: 0.9410      |**0**: 568 <br> **1**: 433      |\n",
    "|    k-NN      | 0.9970      | **0**: 1.0000 <br> **1**: 0.9931       | **0**: 0.9947 <br> **1**: 1.0000    | **0**: 0.9974 <br> **1**: 0.9965      |**0**: 568 <br> **1**: 433      |\n",
    "|  Logistic Regression  | 0.9770      | **0**: 0.9964 <br> **1**: 0.9535       | **0**: 0.9630 <br> **1**: 0.9954    | **0**: 0.9794 <br> **1**: 0.9740      |**0**: 568 <br> **1**: 433      |\n",
    "\n",
    "1. k-NN achieves the highest accuracy (0.9970) with near-perfect precision, recall, and F1-score for both classes, making it the best-performing model. However, k-NN can be sensitive to noisy data and computationally expensive for large datasets.\n",
    "\n",
    "2. Decision Tree also performs well (0.9471 accuracy) but is slightly weaker than k-NN. It has high precision and recall but may be prone to overfitting, depending on the depth of the tree.\n",
    "\n",
    "3. Logistic Regression performs slightly better than Decision Tree, with 0.9770 accuracy. It has high precision and recall but is slightly less effective for class 1, which may indicate some bias toward class 0.\n",
    "\n",
    "4. Naive Bayes has the lowest accuracy (0.8422) among the models, with slightly lower recall for class 1. This suggests it makes more false negatives for class 1, potentially due to its assumption of feature independence.\n",
    "\n",
    "\n",
    "Analyzing **accuracy, precision, recall, and F1-score** on the test set for unsupervised.\n",
    "| Model                | Accuracy | Precision | Recall | F1-Score | Support |\n",
    "|----------------------|----------|-----------|--------|----------|----------|\n",
    "|  k-means  | 0.44      | **0**: 0.50 <br> **1**: 0.38       | **0**: 0.46 <br> **1**: 0.42    | **0**: 0.48 <br> **1**: 0.40     |**0**: 762 <br> **1**: 610     |\n",
    "|  k-means (full data)  | 0.56      | **0**: 0.61 <br> **1**: 0.50       | **0**: 0.55 <br> **1**: 0.57    | **0**: 0.58 <br> **1**: 0.53      |**0**: 762 <br> **1**: 610      |\n",
    "|  DBSCAN (no Noise)  | 0.82      | **0**: 1.00 <br> **1**: 0.96       | **0**: 0.83 <br> **1**: 0.80    | **0**: 0.91 <br> **1**: 0.88      |**0**: 521 <br> **1**: 369      |\n",
    "\n",
    "\n",
    "The first set of models (Naïve Bayes, Decision Tree, k-NN, Logistic Regression) achieves strong accuracy scores, ranging from 84.22% to 99.70%, while the second set (k-means, k-means Full Data, DBSCAN without Noise) performs significantly worse, with accuracy ranging from 44% to 82%.\n",
    "\n",
    "This suggests that the first set of models is well-suited for the classification task, while the second set struggles with distinguishing between classes effectively\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **4.Recommendations**  \n",
    "\n",
    "1. **Feature Engineering:**  \n",
    "   - Use polynomial features to capture non-linear relationships or kernel methods for better class separation.  \n",
    "2. **Ensemble Methods:**  \n",
    "   - Use **Random Forest** or **Gradient Boosting** for better generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
