{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d10fb206",
   "metadata": {},
   "source": [
    "## Fadhla Mohamed Mutua\n",
    "## SM3201434"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cac2cf0",
   "metadata": {},
   "source": [
    "### 1. Introduction\n",
    "This study investigates three machine learning parts with simulated data from random numbers. The random number generation comes from the use of numpy `np.linspace(-5, 5, train_points)` for linear ridge and kernel ridge (where `train_points` is 20 for traning and 1000 for testing) regression and sklearn `sklearn.datasets.make_circles` for PCA. The first part compares linear ridge regression with kernel ridge regression (using both Gaussian and polynomial kernels) to assess regression performance under different model assumptions and kernel formulations. The second part compares traditional Principal Component Analysis (PCA) with Kernel PCA (KPCA) to determine how both methods perform in reducing dimensionality on structured data. A third \"experimental\" section repeats the PCA versus KPCA comparison on a dataset made using `sklearn.datasets.make_classification`, highlighting the sensitivity of kernel-based methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dabfdd",
   "metadata": {},
   "source": [
    "### 2. Ridge Regression vs. Kernel Ridge Regression\n",
    "\n",
    "#### 2.1 Objective and Methodology\n",
    "- **Objective**: The goal was to determine whether kernelizing the ridge regression approach (with either Gaussian or polynomial kernels) could capture non-linear relationships in data better than standard linear ridge regression.\n",
    "\n",
    "- **Methodology**: \n",
    "    - Split of training dataset: The dataset consists of 20 training data values in a range from -5 to 5 and whose y value is gotten from the formula:\n",
    "        - $y_{train} = (X_{train}+4) * (X_{train}+1) * (np.cos(X_{train})-1) * (X_{train}-3) + eps$\n",
    "        - With $eps = $`np.random.normal(0, 1, train_points)` an array of random movements (obtained from the normal distribution) from the true value of y\n",
    "    - Split of testing dataset: The dataset consists of 1000 testing data valus in a range from -5 to 5 and whose y value is gotten from the formula:\n",
    "        - $y_{test} = (X_{test}+4) * (X_{test}+1) * (np.cos(X_{test})-1) * (X_{test}-3)$\n",
    "\n",
    "#### 2.2 Linear Ridge Regression:\n",
    "A regularized linear model that imposes penalties on the coefficient sizes to avoid overfitting.\n",
    "\n",
    "- **Formula used**: Given the formula for linear regression $Y =  X * \\omega + b$,  by including the bias term as a feature (in this case all 1), we can express the model as \n",
    "    - $Y = X_{training\\_bias} * \\omega$ \n",
    "    from which\n",
    "    - $\\omega = (X_{training\\_bias}^T * X_{training\\_bias} + \\lambda*I)^{-1} * X_{training\\_bias}^T * Y$\n",
    "    - The prediction is then formulated as the line $Y_{predict} = X_{training\\_bias} * \\omega$\n",
    "\n",
    "- **Observation**: From the data, we observe that the prediction line $y_{prediction}$ does not accurately represent the data. In fact, the high RMSE suggests significant underfitting.\n",
    "\n",
    "\n",
    "<figure>\n",
    "  <img src=\"LRR.png\" alt=\"Linear Ridge Regression\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 1:</em> Plot for Linear Ridge Regression with RMSE = 26.69</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "#### 2.3 Kernel Ridge Regression (KRR):\n",
    "Extends ridge regression by mapping data to a higher-dimensional space.\n",
    "\n",
    "- **Formula used**: To make our regression non linear, we utilize kernels where kernel $k(x_1,x_2) = \\phi(x_1)^T * \\phi(x_2)$. By having \n",
    "\n",
    "    $\\Phi(X) = \\left( \\begin{array}{c}\n",
    "    \\phi(x_1)  \\\\\n",
    "    .  \\\\\n",
    "    .  \\\\\n",
    "    .  \\\\\n",
    "    \\phi(x_n)  \\end{array} \\right)$\n",
    "\n",
    "- And replacing all X in the previous linear ridge regression with $\\Phi(X)$ we get:\n",
    "\n",
    "    $\\omega = \\Phi(X)^T * (\\Phi(X)^T * \\Phi(X) + \\lambda*I)^{-1} * Y$\n",
    "\n",
    "- Then we get\n",
    "\n",
    "    $Y_{predict} = \\phi(x)^T * \\omega = \\phi(x)^T * \\Phi(X)^T * (\\Phi(X)^T * \\Phi(X) + \\lambda*I)^{-1} * Y$\n",
    "\n",
    "- We can then replace:\n",
    "\n",
    "$\\begin{array}{c}\n",
    "    \\Phi(X)^T * \\Phi(X) = K \\\\\n",
    "    (K)_{i,j} = k(x_i,x_j) \\\\\n",
    "    \\phi(x)^T * \\Phi(X)^T = \\sum_{i=1}^{n}{\\phi(x)^T*\\phi(x_i)} \\\\\n",
    "    \\sum_{i=1}^{n}{\\phi(x)^T*\\phi(x_i)} = \\sum_{i=1}^{n}{k(x,x_i)} \\end{array}$\n",
    "\n",
    "- From which:\n",
    "\n",
    "    $\\begin{array}{c}\n",
    "            \\alpha = (K + \\lambda*I)^{-1} * Y\\\\\n",
    "            Y_{predict} = \\sum_{i=1}^{n}{\\alpha_{i} * k(x,x_i)} \\end{array}$\n",
    "\n",
    "**Two kernels were tested**:\n",
    "\n",
    "##### 2.3.1 Gaussian (RBF) Kernel\n",
    "\n",
    "- **Formula used**: Applying the Gaussian Kernel we have:\n",
    "    - $k(x,x') = e^{−∥x−x'∥^2/(2*\\sigma^2)}$ with $\\sigma>0$\n",
    "\n",
    "- **Observation**: By applying a grid search on sigma_grid = [0.01, 0.1, 1, 5, 10] and lambda_grid = [0.01, 0.1, 1, 5, 10]\n",
    "\n",
    "<figure>\n",
    "  <img src=\"RMSE_Guass.png\" alt=\"RMSE vs Sigma (Guass).png\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 2:</em> Plot for RMSE vs Sigma Gauss</figcaption>\n",
    "</figure>\n",
    "\n",
    "- We find that the best fit curve has $\\sigma = 1$ and $\\lambda = 0.01$\n",
    "\n",
    "<figure>\n",
    "  <img src=\"KRRG.png\" alt=\"Kernel Ridge Rigression (Gauss)\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 3:</em> Plot for Kernel Ridge Rigression (Gauss) with RMSE = 0.80</figcaption>\n",
    "</figure>\n",
    "\n",
    "##### 2.3.2 Polynomial Kernel\n",
    "\n",
    "- **Formula used**: Applying the Polynomial Kernel we have:\n",
    "    - $k(x,x′)=(x^Tx′+1)^\\sigma$\n",
    "\n",
    "- **Observation**: By applying a grid search on sigma_grid = [0.01, 0.1, 1, 5, 10] and lambda_grid = [0.01, 0.1, 1, 5, 10]\n",
    "\n",
    "<figure>\n",
    "  <img src=\"RMSE_Poly.png\" alt=\"RMSE vs Sigma (Poly).png\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 4:</em> Plot for RMSE vs Sigma</figcaption>\n",
    "</figure>\n",
    "\n",
    "- We find that the best fit curve has $\\sigma = 10$ and $\\lambda = 5$\n",
    "\n",
    "<figure>\n",
    "  <img src=\"KRRP.png\" alt=\"Kernel Ridge Rigression (Poly)\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 5:</em> Plot for Kernel Ridge Rigression (Poly) with RMSE = 0.78</figcaption>\n",
    "</figure>\n",
    "\n",
    "#### 2.4 Key Findings\n",
    "\n",
    "When non-linearity was introduced or present in the dataset, both kernel methods outperformed the linear approach, with the Gaussian kernel providing a slight edge over the polynomial kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045bd17",
   "metadata": {},
   "source": [
    "### 3. PCA vs. Kernel PCA\n",
    "#### 3.1 Objective and Methodology\n",
    "- **Objective**: To compare the effectiveness of standard PCA against KPCA in terms of capturing the variance and revealing underlying structure in the data for linear SVM.\n",
    "\n",
    "- **Methodology**:\n",
    "    - Gets the dataset from `sklearn.datasets.make_circles` with 1000 samples and divides it into 20% training and 80% testing with 15% of them being noise. For consistency the sklearn seed is set to 0\n",
    "\n",
    "#### 3.2 PCA\n",
    "\n",
    "A linear dimensionality reduction technique that projects data onto a subspace spanned by the principal components (directions of maximal variance).\n",
    "\n",
    "- PCA Performance:\n",
    "    - `Using sklearn.decomposition.PCA`, we perform dimensionality reduction on `X_train` and visualize the result. Although the dataset is correctly labeled, the projected shape reveals two concentric circles; one enclosed within the other. This circular structure highlights a key limitation: the data is not linearly separable in the reduced space, which significantly reduces the effectiveness of a linear SVM in classifying the points correctly as proven with the classification report on accuracy:\n",
    "    \n",
    "<figure>\n",
    "  <img src=\"PCA.png\" alt=\"PCA projection\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 6:</em> Plot for PCA projection</figcaption>\n",
    "</figure>\n",
    "\n",
    "|               | Precision | Recall | F1-score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| **Class 0**   | 0.90      | 0.30   | 0.45     | 125     |\n",
    "| **Class 1**   | 0.58      | 0.97   | 0.72     | 125     |\n",
    "| **Accuracy**  |           |        | 0.63     | 250     |\n",
    "| **Macro avg** | 0.74      | 0.63   | 0.59     | 250     |\n",
    "| **Weighted avg** | 0.74   | 0.63   | 0.59     | 250     |\n",
    "\n",
    "#### 3.2 KPCA\n",
    "\n",
    "A non-linear extension of PCA that first applies a kernel transformation. The Gaussian or polynomial kernel can help uncover non-linear manifolds hidden in the data.\n",
    "\n",
    "- KPCA Performance:\n",
    "    - Using `sklearn.decomposition.KernelPCA `with `gamma=5`, we perform dimensionality reduction on `X_train` and visualize the result. The projected data is well-separated and nearly linearly separable, which significantly improves the performance of a linear SVM, as confirmed by the classification report accuracy.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"KPCA.png\" alt=\"KPCA projection\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 7:</em> Plot for KPCA projection</figcaption>\n",
    "</figure>\n",
    "\n",
    "|               | Precision | Recall | F1-score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| **Class 0**   | 0.98      |   1.00 | 0.99     | 125     |\n",
    "| **Class 1**   | 1.00      | 0.98   | 0.99     | 125     |\n",
    "| **Accuracy**  |           |        | 0.99     | 250     |\n",
    "| **Macro avg** | 0.99      | 0.99   | 0.99     | 250     |\n",
    "| **Weighted avg** | 0.99   | 0.99   | 0.99     | 250     |\n",
    "\n",
    "#### 3.3 Key Findings\n",
    "\n",
    "In scenarios where the data had a non-linear structure, KPCA delivered more insightful lower-dimensional representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02655614",
   "metadata": {},
   "source": [
    "### 4. PCA vs. Kernel PCA Part 2\n",
    "#### 4.1 Objective and Methodology\n",
    "\n",
    "- **Methodology**:\n",
    "    - We generate a dataset using `sklearn.datasets.make_classification` with 1000 samples and 15% label noise. The data is split into 80% testing and 20% training, with `random_state=0` set for consistency.\n",
    "\n",
    "#### 4.2 PCA\n",
    "\n",
    "- PCA Performance:\n",
    "    - Using `sklearn.decomposition.PCA`, we perform dimensionality reduction on `X_train` and visualize the result. We observe that the dataset becomes almost linearly separable, as confirmed by the SVM classification report.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"PCA_2.png\" alt=\"PCA projection\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 8:</em> Plot for PCA projection</figcaption>\n",
    "</figure>\n",
    "\n",
    "|               | Precision | Recall | F1-score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| **Class 0**   | 0.98      |   1.00 | 0.99     | 124     |\n",
    "| **Class 1**   | 1.00      | 0.98   | 0.99     | 126     |\n",
    "| **Accuracy**  |           |        | 0.99     | 250     |\n",
    "| **Macro avg** | 0.99      | 0.99   | 0.99     | 250     |\n",
    "| **Weighted avg** | 0.99   | 0.99   | 0.99     | 250     |\n",
    "\n",
    "#### 4.3 PCA\n",
    "\n",
    "- PCA Performance:\n",
    "    - Using `sklearn.decomposition.KernelPCA` with gamma=0.01, we perform dimensionality reduction on `X_train` and visualize the result. We observe that the dataset becomes nearly linearly separable. However, the classification accuracy does not improve compared to standard PCA; in fact, it slightly decreases, as confirmed by the SVM classification report.\n",
    "\n",
    "<figure>\n",
    "  <img src=\"KPCA_2.png\" alt=\"KPCA projection\" style=\"width:800px;\">\n",
    "  <figcaption><em>Figure 9:</em> Plot for KPCA projection</figcaption>\n",
    "</figure>\n",
    "\n",
    "|               | Precision | Recall | F1-score | Support |\n",
    "|---------------|-----------|--------|----------|---------|\n",
    "| **Class 0**   | 0.84      |   0.89 | 0.86     | 124     |\n",
    "| **Class 1**   | 0.88      | 0.83   | 0.86     | 126     |\n",
    "| **Accuracy**  |           |        | 0.86     | 250     |\n",
    "| **Macro avg** | 0.86      | 0.86   | 0.86     | 250     |\n",
    "| **Weighted avg** | 0.86   | 0.86   | 0.86     | 250     |\n",
    "\n",
    "#### 4.4 Key Findings\n",
    "\n",
    "These results highlight that while KPCA can be highly effective, its benefits are conditional on the data characteristics.\n",
    "\n",
    "In datasets where the non-linear structure is weak or obscured by noise, traditional PCA’s stability and simplicity can make it a more reliable choice."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
