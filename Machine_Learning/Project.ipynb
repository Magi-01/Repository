{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Respresentation of concepts using Conjunctive Normal Form (CNF)\n",
    "- (Sunny and Normal) or (Overcast) or (Rain and Weak)... This is a decision tree if it's sunny and the humidity in normal then we play oustide\n",
    "- if outlook == Sunny and hudity == normal \\ elif outlook == overcast \\ elif outlook == Rainy and humidity == Severe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees:\n",
    "- Given a dataset, isolate them in classes using for example lines: the things contained shall be the leaf, and the lines that separate shall be the decision nodes\n",
    "- Start with an empty Tree\n",
    "    1. Split the decision attribute A for the next node and Assign A as decision attribute (think attribute A as the x-axis and you are drawing a line to make certain intervals).\n",
    "    2. If the boxes formed are perfect Stop else redo 1. while changing attribute if needed\n",
    "- What you end up with is a greedy approach\n",
    "- We use entropy to select which decision line to use where $entropy(S) = -log_2(p)$\n",
    "- Given that I (in this example of true and False) have x negative and y positives:\n",
    "    - $S = p_x(-log_2(p_x)) + p_y(-log_2(p_y))$\n",
    "    - $S = -p_x*log_2(p_x) + -p_y*log_2(p_y)$\n",
    "- We then use ingÃ¬formation Gain to reduce the weight of the entropy:\n",
    "    - $Gain(S,A) = S - \\sum_{v \\in values(A)}\\frac{|S_v|}{|S|}*S_v$\n",
    "    - Essentially an Error calculation\n",
    "- The result is the one with the biggest entropy Gain\n",
    "- This method is NP-Hard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID3\n",
    "- Restriction bias:\n",
    "    - Comes from incomplete hypothesis bias\n",
    "- Preference bias:\n",
    "    - Comes from incomplete walk\n",
    "- Inductive bias:\n",
    "    - Comes from the selection of the shortest path\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "- Prepruning:\n",
    "    - Stop at a certain criteria (statistical method)\n",
    "- Postpruning:\n",
    "    - Do all of the paths the cut of un-needed ones\n",
    "- Cross-validation is the best way to do pruning:\n",
    "    - Divide our data into Grow, Train, and validation\n",
    "    - Build the grow tree\n",
    "    - Use the validation test to cut off from the top and then replace it by majority vote (you are at a leaf node which continues on, on the left you have a total of 5 negatives and 10 positives on the rights, given 10>5, we replace the 5 negatives with positives)\n",
    "    - And then test it and find the greatest accurancy and then permanently cut that completely\n",
    "    - Repeat the process untill validation holds\n",
    "- Second is Statistical methods which is is self explanatory I.e. Occam's Razor (Search it Up)\n",
    "- A problem occurs when you have alot of values (e.g. birthdays):\n",
    "    - In that case we use $GainRatio(S,A) = \\frac{Gain(S,A)}{SplitInformation(S,A)}$\n",
    "    - $SplitInformation(S,A) = \\sum_{v \\in values} \\frac{|S_v|}{|S|}*log(\\frac{|S_v|}{|S|})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Trees are fast, Flexible, Interpretable\n",
    "- They are not"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
